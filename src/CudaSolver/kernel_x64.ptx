//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	FluffySeed4K
.const .align 8 .b8 recovery[336];
// FluffySeed4K$__cuda_local_var_207176_30_non_const_magazine has been demoted
// FluffyRound_A1$__cuda_local_var_207252_30_non_const_ecounters has been demoted
// FluffyRound_A2$__cuda_local_var_207398_30_non_const_ecounters has been demoted
// FluffyRound_A3$__cuda_local_var_207504_30_non_const_ecounters has been demoted
// FluffySeed4K_C0$__cuda_local_var_207638_30_non_const_magazine has been demoted
// FluffyRound_C1$__cuda_local_var_207714_30_non_const_ecounters has been demoted
// FluffyRound_C2$__cuda_local_var_207858_30_non_const_ecounters has been demoted
// FluffyRound_C3$__cuda_local_var_207964_30_non_const_ecounters has been demoted
// FluffyRound_B1$__cuda_local_var_208097_30_non_const_ecounters has been demoted
// FluffyRound_B2$__cuda_local_var_208230_30_non_const_ecounters has been demoted
// FluffyRound_B3$__cuda_local_var_208291_30_non_const_ecounters has been demoted
// FluffyTail$__cuda_local_var_208417_30_non_const_destIdx has been demoted
// FluffyRecovery$__cuda_local_var_208434_30_non_const_nonces has been demoted

.visible .entry FluffySeed4K(
	.param .u64 FluffySeed4K_param_0,
	.param .u64 FluffySeed4K_param_1,
	.param .u64 FluffySeed4K_param_2,
	.param .u64 FluffySeed4K_param_3,
	.param .u64 FluffySeed4K_param_4,
	.param .u64 FluffySeed4K_param_5,
	.param .u32 FluffySeed4K_param_6
)
{
	.local .align 16 .b8 	__local_depot0[480];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<1214>;
	.reg .b64 	%rd<987>;
	// demoted variable
	.shared .align 8 .b8 FluffySeed4K$__cuda_local_var_207176_30_non_const_magazine[32768];

	mov.u64 	%rd986, __local_depot0;
	cvta.local.u64 	%SP, %rd986;
	ld.param.u64 	%rd69, [FluffySeed4K_param_0];
	ld.param.u64 	%rd70, [FluffySeed4K_param_1];
	ld.param.u64 	%rd71, [FluffySeed4K_param_2];
	ld.param.u64 	%rd72, [FluffySeed4K_param_3];
	ld.param.u64 	%rd73, [FluffySeed4K_param_4];
	ld.param.u64 	%rd74, [FluffySeed4K_param_5];
	ld.param.u32 	%r44, [FluffySeed4K_param_6];
	cvta.to.global.u64 	%rd1, %rd73;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r45, %r1, 3;
	mov.u32 	%r46, FluffySeed4K$__cuda_local_var_207176_30_non_const_magazine;
	add.s32 	%r2, %r46, %r45;
	mov.u64 	%rd75, 0;
	st.shared.u64 	[%r2], %rd75;
	st.shared.u64 	[%r2+4096], %rd75;
	st.shared.u64 	[%r2+8192], %rd75;
	st.shared.u64 	[%r2+12288], %rd75;
	st.shared.u64 	[%r2+16384], %rd75;
	st.shared.u64 	[%r2+20480], %rd75;
	st.shared.u64 	[%r2+24576], %rd75;
	st.shared.u64 	[%r2+28672], %rd75;
	mov.u32 	%r47, %ctaid.x;
	mov.u32 	%r48, %ntid.x;
	mad.lo.s32 	%r10, %r47, %r48, %r1;
	cvta.to.global.u64 	%rd2, %rd74;
	add.u64 	%rd76, %SP, 0;
	cvta.to.local.u64 	%rd3, %rd76;
	bar.sync 	0;
	shl.b32 	%r49, %r10, 8;
	add.s32 	%r11, %r49, %r44;
	add.s64 	%rd4, %rd3, 448;
	mov.u16 	%rs7, 0;
	mov.u16 	%rs12, %rs7;

BB0_1:
	cvt.s32.s16	%r50, %rs12;
	add.s32 	%r51, %r11, %r50;
	cvt.s64.s32	%rd5, %r51;
	mov.u64 	%rd981, %rd69;
	mov.u64 	%rd982, %rd70;
	mov.u64 	%rd983, %rd71;
	mov.u64 	%rd984, %rd72;
	mov.u16 	%rs13, %rs7;

BB0_2:
	cvt.s64.s16	%rd77, %rs13;
	add.s64 	%rd78, %rd77, %rd5;
	xor.b64  	%rd79, %rd78, %rd984;
	add.s64 	%rd80, %rd983, %rd79;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r52}, %rd79;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r53,%dummy}, %rd79;
	}
	shf.l.wrap.b32 	%r54, %r53, %r52, 16;
	shf.l.wrap.b32 	%r55, %r52, %r53, 16;
	mov.b64 	%rd81, {%r55, %r54};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r56}, %rd982;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r57,%dummy}, %rd982;
	}
	shf.l.wrap.b32 	%r58, %r57, %r56, 13;
	shf.l.wrap.b32 	%r59, %r56, %r57, 13;
	mov.b64 	%rd82, {%r59, %r58};
	add.s64 	%rd83, %rd981, %rd982;
	xor.b64  	%rd84, %rd82, %rd83;
	xor.b64  	%rd85, %rd81, %rd80;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd83, 32;
	shr.b64 	%rhs, %rd83, 32;
	add.u64 	%rd86, %lhs, %rhs;
	}
	add.s64 	%rd87, %rd84, %rd80;
	add.s64 	%rd88, %rd85, %rd86;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r60}, %rd84;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r61,%dummy}, %rd84;
	}
	shf.l.wrap.b32 	%r62, %r61, %r60, 17;
	shf.l.wrap.b32 	%r63, %r60, %r61, 17;
	mov.b64 	%rd89, {%r63, %r62};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r64}, %rd85;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r65,%dummy}, %rd85;
	}
	shf.l.wrap.b32 	%r66, %r65, %r64, 25;
	shf.l.wrap.b32 	%r67, %r64, %r65, 25;
	mov.b64 	%rd90, {%r67, %r66};
	xor.b64  	%rd91, %rd89, %rd87;
	xor.b64  	%rd92, %rd90, %rd88;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd87, 32;
	shr.b64 	%rhs, %rd87, 32;
	add.u64 	%rd93, %lhs, %rhs;
	}
	add.s64 	%rd94, %rd88, %rd91;
	add.s64 	%rd95, %rd93, %rd92;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r68}, %rd91;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r69,%dummy}, %rd91;
	}
	shf.l.wrap.b32 	%r70, %r69, %r68, 13;
	shf.l.wrap.b32 	%r71, %r68, %r69, 13;
	mov.b64 	%rd96, {%r71, %r70};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r72}, %rd92;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r73,%dummy}, %rd92;
	}
	shf.l.wrap.b32 	%r74, %r73, %r72, 16;
	shf.l.wrap.b32 	%r75, %r72, %r73, 16;
	mov.b64 	%rd97, {%r75, %r74};
	xor.b64  	%rd98, %rd96, %rd94;
	xor.b64  	%rd99, %rd97, %rd95;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd94, 32;
	shr.b64 	%rhs, %rd94, 32;
	add.u64 	%rd100, %lhs, %rhs;
	}
	add.s64 	%rd101, %rd98, %rd95;
	add.s64 	%rd102, %rd99, %rd100;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r76}, %rd98;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r77,%dummy}, %rd98;
	}
	shf.l.wrap.b32 	%r78, %r77, %r76, 17;
	shf.l.wrap.b32 	%r79, %r76, %r77, 17;
	mov.b64 	%rd103, {%r79, %r78};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r80}, %rd99;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r81,%dummy}, %rd99;
	}
	shf.l.wrap.b32 	%r82, %r81, %r80, 25;
	shf.l.wrap.b32 	%r83, %r80, %r81, 25;
	mov.b64 	%rd104, {%r83, %r82};
	xor.b64  	%rd105, %rd103, %rd101;
	xor.b64  	%rd106, %rd104, %rd102;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd101, 32;
	shr.b64 	%rhs, %rd101, 32;
	add.u64 	%rd107, %lhs, %rhs;
	}
	xor.b64  	%rd108, %rd102, %rd78;
	xor.b64  	%rd109, %rd107, 255;
	add.s64 	%rd110, %rd108, %rd105;
	add.s64 	%rd111, %rd109, %rd106;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r84}, %rd105;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r85,%dummy}, %rd105;
	}
	shf.l.wrap.b32 	%r86, %r85, %r84, 13;
	shf.l.wrap.b32 	%r87, %r84, %r85, 13;
	mov.b64 	%rd112, {%r87, %r86};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r88}, %rd106;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r89,%dummy}, %rd106;
	}
	shf.l.wrap.b32 	%r90, %r89, %r88, 16;
	shf.l.wrap.b32 	%r91, %r88, %r89, 16;
	mov.b64 	%rd113, {%r91, %r90};
	xor.b64  	%rd114, %rd112, %rd110;
	xor.b64  	%rd115, %rd113, %rd111;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd110, 32;
	shr.b64 	%rhs, %rd110, 32;
	add.u64 	%rd116, %lhs, %rhs;
	}
	add.s64 	%rd117, %rd114, %rd111;
	add.s64 	%rd118, %rd115, %rd116;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r92}, %rd114;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r93,%dummy}, %rd114;
	}
	shf.l.wrap.b32 	%r94, %r93, %r92, 17;
	shf.l.wrap.b32 	%r95, %r92, %r93, 17;
	mov.b64 	%rd119, {%r95, %r94};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r96}, %rd115;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r97,%dummy}, %rd115;
	}
	shf.l.wrap.b32 	%r98, %r97, %r96, 25;
	shf.l.wrap.b32 	%r99, %r96, %r97, 25;
	mov.b64 	%rd120, {%r99, %r98};
	xor.b64  	%rd121, %rd119, %rd117;
	xor.b64  	%rd122, %rd120, %rd118;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd117, 32;
	shr.b64 	%rhs, %rd117, 32;
	add.u64 	%rd123, %lhs, %rhs;
	}
	add.s64 	%rd124, %rd118, %rd121;
	add.s64 	%rd125, %rd123, %rd122;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r100}, %rd121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r101,%dummy}, %rd121;
	}
	shf.l.wrap.b32 	%r102, %r101, %r100, 13;
	shf.l.wrap.b32 	%r103, %r100, %r101, 13;
	mov.b64 	%rd126, {%r103, %r102};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r104}, %rd122;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r105,%dummy}, %rd122;
	}
	shf.l.wrap.b32 	%r106, %r105, %r104, 16;
	shf.l.wrap.b32 	%r107, %r104, %r105, 16;
	mov.b64 	%rd127, {%r107, %r106};
	xor.b64  	%rd128, %rd126, %rd124;
	xor.b64  	%rd129, %rd127, %rd125;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd124, 32;
	shr.b64 	%rhs, %rd124, 32;
	add.u64 	%rd130, %lhs, %rhs;
	}
	add.s64 	%rd131, %rd128, %rd125;
	add.s64 	%rd132, %rd129, %rd130;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r108}, %rd128;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r109,%dummy}, %rd128;
	}
	shf.l.wrap.b32 	%r110, %r109, %r108, 17;
	shf.l.wrap.b32 	%r111, %r108, %r109, 17;
	mov.b64 	%rd133, {%r111, %r110};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r112}, %rd129;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r113,%dummy}, %rd129;
	}
	shf.l.wrap.b32 	%r114, %r113, %r112, 25;
	shf.l.wrap.b32 	%r115, %r112, %r113, 25;
	mov.b64 	%rd134, {%r115, %r114};
	xor.b64  	%rd135, %rd133, %rd131;
	xor.b64  	%rd136, %rd134, %rd132;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd131, 32;
	shr.b64 	%rhs, %rd131, 32;
	add.u64 	%rd137, %lhs, %rhs;
	}
	add.s64 	%rd138, %rd132, %rd135;
	add.s64 	%rd139, %rd137, %rd136;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r116}, %rd135;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r117,%dummy}, %rd135;
	}
	shf.l.wrap.b32 	%r118, %r117, %r116, 13;
	shf.l.wrap.b32 	%r119, %r116, %r117, 13;
	mov.b64 	%rd140, {%r119, %r118};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r120}, %rd136;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r121,%dummy}, %rd136;
	}
	shf.l.wrap.b32 	%r122, %r121, %r120, 16;
	shf.l.wrap.b32 	%r123, %r120, %r121, 16;
	mov.b64 	%rd141, {%r123, %r122};
	xor.b64  	%rd142, %rd140, %rd138;
	xor.b64  	%rd143, %rd141, %rd139;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd138, 32;
	shr.b64 	%rhs, %rd138, 32;
	add.u64 	%rd144, %lhs, %rhs;
	}
	add.s64 	%rd145, %rd142, %rd139;
	add.s64 	%rd146, %rd143, %rd144;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r124}, %rd142;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r125,%dummy}, %rd142;
	}
	shf.l.wrap.b32 	%r126, %r125, %r124, 17;
	shf.l.wrap.b32 	%r127, %r124, %r125, 17;
	mov.b64 	%rd147, {%r127, %r126};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r128}, %rd143;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r129,%dummy}, %rd143;
	}
	shf.l.wrap.b32 	%r130, %r129, %r128, 25;
	shf.l.wrap.b32 	%r131, %r128, %r129, 25;
	mov.b64 	%rd148, {%r131, %r130};
	xor.b64  	%rd149, %rd147, %rd145;
	xor.b64  	%rd150, %rd148, %rd146;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd145, 32;
	shr.b64 	%rhs, %rd145, 32;
	add.u64 	%rd151, %lhs, %rhs;
	}
	add.s64 	%rd152, %rd146, %rd149;
	add.s64 	%rd153, %rd151, %rd150;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r132}, %rd149;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r133,%dummy}, %rd149;
	}
	shf.l.wrap.b32 	%r134, %r133, %r132, 13;
	shf.l.wrap.b32 	%r135, %r132, %r133, 13;
	mov.b64 	%rd154, {%r135, %r134};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r136}, %rd150;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r137,%dummy}, %rd150;
	}
	shf.l.wrap.b32 	%r138, %r137, %r136, 16;
	shf.l.wrap.b32 	%r139, %r136, %r137, 16;
	mov.b64 	%rd155, {%r139, %r138};
	xor.b64  	%rd156, %rd154, %rd152;
	xor.b64  	%rd157, %rd155, %rd153;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd152, 32;
	shr.b64 	%rhs, %rd152, 32;
	add.u64 	%rd158, %lhs, %rhs;
	}
	add.s64 	%rd159, %rd156, %rd153;
	add.s64 	%rd160, %rd157, %rd158;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r140}, %rd156;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r141,%dummy}, %rd156;
	}
	shf.l.wrap.b32 	%r142, %r141, %r140, 17;
	shf.l.wrap.b32 	%r143, %r140, %r141, 17;
	mov.b64 	%rd161, {%r143, %r142};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r144}, %rd157;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r145,%dummy}, %rd157;
	}
	shf.l.wrap.b32 	%r146, %r145, %r144, 25;
	shf.l.wrap.b32 	%r147, %r144, %r145, 25;
	mov.b64 	%rd162, {%r147, %r146};
	xor.b64  	%rd163, %rd161, %rd159;
	xor.b64  	%rd164, %rd162, %rd160;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd159, 32;
	shr.b64 	%rhs, %rd159, 32;
	add.u64 	%rd165, %lhs, %rhs;
	}
	xor.b64  	%rd166, %rd165, %rd164;
	xor.b64  	%rd167, %rd166, %rd163;
	add.s64 	%rd168, %rd78, 1;
	xor.b64  	%rd169, %rd164, %rd168;
	add.s64 	%rd170, %rd160, %rd163;
	add.s64 	%rd171, %rd165, %rd169;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r148}, %rd163;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r149,%dummy}, %rd163;
	}
	shf.l.wrap.b32 	%r150, %r149, %r148, 13;
	shf.l.wrap.b32 	%r151, %r148, %r149, 13;
	mov.b64 	%rd172, {%r151, %r150};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r152}, %rd169;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r153,%dummy}, %rd169;
	}
	shf.l.wrap.b32 	%r154, %r153, %r152, 16;
	shf.l.wrap.b32 	%r155, %r152, %r153, 16;
	mov.b64 	%rd173, {%r155, %r154};
	xor.b64  	%rd174, %rd172, %rd170;
	xor.b64  	%rd175, %rd173, %rd171;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd170, 32;
	shr.b64 	%rhs, %rd170, 32;
	add.u64 	%rd176, %lhs, %rhs;
	}
	add.s64 	%rd177, %rd174, %rd171;
	add.s64 	%rd178, %rd175, %rd176;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r156}, %rd174;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r157,%dummy}, %rd174;
	}
	shf.l.wrap.b32 	%r158, %r157, %r156, 17;
	shf.l.wrap.b32 	%r159, %r156, %r157, 17;
	mov.b64 	%rd179, {%r159, %r158};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r160}, %rd175;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r161,%dummy}, %rd175;
	}
	shf.l.wrap.b32 	%r162, %r161, %r160, 25;
	shf.l.wrap.b32 	%r163, %r160, %r161, 25;
	mov.b64 	%rd180, {%r163, %r162};
	xor.b64  	%rd181, %rd179, %rd177;
	xor.b64  	%rd182, %rd180, %rd178;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd177, 32;
	shr.b64 	%rhs, %rd177, 32;
	add.u64 	%rd183, %lhs, %rhs;
	}
	add.s64 	%rd184, %rd178, %rd181;
	add.s64 	%rd185, %rd183, %rd182;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r164}, %rd181;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r165,%dummy}, %rd181;
	}
	shf.l.wrap.b32 	%r166, %r165, %r164, 13;
	shf.l.wrap.b32 	%r167, %r164, %r165, 13;
	mov.b64 	%rd186, {%r167, %r166};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r168}, %rd182;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r169,%dummy}, %rd182;
	}
	shf.l.wrap.b32 	%r170, %r169, %r168, 16;
	shf.l.wrap.b32 	%r171, %r168, %r169, 16;
	mov.b64 	%rd187, {%r171, %r170};
	xor.b64  	%rd188, %rd186, %rd184;
	xor.b64  	%rd189, %rd187, %rd185;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd184, 32;
	shr.b64 	%rhs, %rd184, 32;
	add.u64 	%rd190, %lhs, %rhs;
	}
	add.s64 	%rd191, %rd188, %rd185;
	add.s64 	%rd192, %rd189, %rd190;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r172}, %rd188;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r173,%dummy}, %rd188;
	}
	shf.l.wrap.b32 	%r174, %r173, %r172, 17;
	shf.l.wrap.b32 	%r175, %r172, %r173, 17;
	mov.b64 	%rd193, {%r175, %r174};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r176}, %rd189;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r177,%dummy}, %rd189;
	}
	shf.l.wrap.b32 	%r178, %r177, %r176, 25;
	shf.l.wrap.b32 	%r179, %r176, %r177, 25;
	mov.b64 	%rd194, {%r179, %r178};
	xor.b64  	%rd195, %rd193, %rd191;
	xor.b64  	%rd196, %rd194, %rd192;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd191, 32;
	shr.b64 	%rhs, %rd191, 32;
	add.u64 	%rd197, %lhs, %rhs;
	}
	xor.b64  	%rd198, %rd192, %rd168;
	xor.b64  	%rd199, %rd197, 255;
	add.s64 	%rd200, %rd198, %rd195;
	add.s64 	%rd201, %rd199, %rd196;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r180}, %rd195;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r181,%dummy}, %rd195;
	}
	shf.l.wrap.b32 	%r182, %r181, %r180, 13;
	shf.l.wrap.b32 	%r183, %r180, %r181, 13;
	mov.b64 	%rd202, {%r183, %r182};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r184}, %rd196;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r185,%dummy}, %rd196;
	}
	shf.l.wrap.b32 	%r186, %r185, %r184, 16;
	shf.l.wrap.b32 	%r187, %r184, %r185, 16;
	mov.b64 	%rd203, {%r187, %r186};
	xor.b64  	%rd204, %rd202, %rd200;
	xor.b64  	%rd205, %rd203, %rd201;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd200, 32;
	shr.b64 	%rhs, %rd200, 32;
	add.u64 	%rd206, %lhs, %rhs;
	}
	add.s64 	%rd207, %rd204, %rd201;
	add.s64 	%rd208, %rd205, %rd206;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r188}, %rd204;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r189,%dummy}, %rd204;
	}
	shf.l.wrap.b32 	%r190, %r189, %r188, 17;
	shf.l.wrap.b32 	%r191, %r188, %r189, 17;
	mov.b64 	%rd209, {%r191, %r190};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r192}, %rd205;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r193,%dummy}, %rd205;
	}
	shf.l.wrap.b32 	%r194, %r193, %r192, 25;
	shf.l.wrap.b32 	%r195, %r192, %r193, 25;
	mov.b64 	%rd210, {%r195, %r194};
	xor.b64  	%rd211, %rd209, %rd207;
	xor.b64  	%rd212, %rd210, %rd208;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd207, 32;
	shr.b64 	%rhs, %rd207, 32;
	add.u64 	%rd213, %lhs, %rhs;
	}
	add.s64 	%rd214, %rd208, %rd211;
	add.s64 	%rd215, %rd213, %rd212;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r196}, %rd211;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r197,%dummy}, %rd211;
	}
	shf.l.wrap.b32 	%r198, %r197, %r196, 13;
	shf.l.wrap.b32 	%r199, %r196, %r197, 13;
	mov.b64 	%rd216, {%r199, %r198};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r200}, %rd212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r201,%dummy}, %rd212;
	}
	shf.l.wrap.b32 	%r202, %r201, %r200, 16;
	shf.l.wrap.b32 	%r203, %r200, %r201, 16;
	mov.b64 	%rd217, {%r203, %r202};
	xor.b64  	%rd218, %rd216, %rd214;
	xor.b64  	%rd219, %rd217, %rd215;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd214, 32;
	shr.b64 	%rhs, %rd214, 32;
	add.u64 	%rd220, %lhs, %rhs;
	}
	add.s64 	%rd221, %rd218, %rd215;
	add.s64 	%rd222, %rd219, %rd220;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r204}, %rd218;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r205,%dummy}, %rd218;
	}
	shf.l.wrap.b32 	%r206, %r205, %r204, 17;
	shf.l.wrap.b32 	%r207, %r204, %r205, 17;
	mov.b64 	%rd223, {%r207, %r206};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r208}, %rd219;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r209,%dummy}, %rd219;
	}
	shf.l.wrap.b32 	%r210, %r209, %r208, 25;
	shf.l.wrap.b32 	%r211, %r208, %r209, 25;
	mov.b64 	%rd224, {%r211, %r210};
	xor.b64  	%rd225, %rd223, %rd221;
	xor.b64  	%rd226, %rd224, %rd222;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd221, 32;
	shr.b64 	%rhs, %rd221, 32;
	add.u64 	%rd227, %lhs, %rhs;
	}
	add.s64 	%rd228, %rd222, %rd225;
	add.s64 	%rd229, %rd227, %rd226;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r212}, %rd225;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r213,%dummy}, %rd225;
	}
	shf.l.wrap.b32 	%r214, %r213, %r212, 13;
	shf.l.wrap.b32 	%r215, %r212, %r213, 13;
	mov.b64 	%rd230, {%r215, %r214};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r216}, %rd226;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r217,%dummy}, %rd226;
	}
	shf.l.wrap.b32 	%r218, %r217, %r216, 16;
	shf.l.wrap.b32 	%r219, %r216, %r217, 16;
	mov.b64 	%rd231, {%r219, %r218};
	xor.b64  	%rd232, %rd230, %rd228;
	xor.b64  	%rd233, %rd231, %rd229;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd228, 32;
	shr.b64 	%rhs, %rd228, 32;
	add.u64 	%rd234, %lhs, %rhs;
	}
	add.s64 	%rd235, %rd232, %rd229;
	add.s64 	%rd236, %rd233, %rd234;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r220}, %rd232;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r221,%dummy}, %rd232;
	}
	shf.l.wrap.b32 	%r222, %r221, %r220, 17;
	shf.l.wrap.b32 	%r223, %r220, %r221, 17;
	mov.b64 	%rd237, {%r223, %r222};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r224}, %rd233;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r225,%dummy}, %rd233;
	}
	shf.l.wrap.b32 	%r226, %r225, %r224, 25;
	shf.l.wrap.b32 	%r227, %r224, %r225, 25;
	mov.b64 	%rd238, {%r227, %r226};
	xor.b64  	%rd239, %rd237, %rd235;
	xor.b64  	%rd240, %rd238, %rd236;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd235, 32;
	shr.b64 	%rhs, %rd235, 32;
	add.u64 	%rd241, %lhs, %rhs;
	}
	add.s64 	%rd242, %rd236, %rd239;
	add.s64 	%rd243, %rd241, %rd240;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r228}, %rd239;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r229,%dummy}, %rd239;
	}
	shf.l.wrap.b32 	%r230, %r229, %r228, 13;
	shf.l.wrap.b32 	%r231, %r228, %r229, 13;
	mov.b64 	%rd244, {%r231, %r230};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r232}, %rd240;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r233,%dummy}, %rd240;
	}
	shf.l.wrap.b32 	%r234, %r233, %r232, 16;
	shf.l.wrap.b32 	%r235, %r232, %r233, 16;
	mov.b64 	%rd245, {%r235, %r234};
	xor.b64  	%rd246, %rd244, %rd242;
	xor.b64  	%rd247, %rd245, %rd243;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd242, 32;
	shr.b64 	%rhs, %rd242, 32;
	add.u64 	%rd248, %lhs, %rhs;
	}
	add.s64 	%rd249, %rd246, %rd243;
	add.s64 	%rd250, %rd247, %rd248;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r236}, %rd246;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r237,%dummy}, %rd246;
	}
	shf.l.wrap.b32 	%r238, %r237, %r236, 17;
	shf.l.wrap.b32 	%r239, %r236, %r237, 17;
	mov.b64 	%rd251, {%r239, %r238};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r240}, %rd247;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r241,%dummy}, %rd247;
	}
	shf.l.wrap.b32 	%r242, %r241, %r240, 25;
	shf.l.wrap.b32 	%r243, %r240, %r241, 25;
	mov.b64 	%rd252, {%r243, %r242};
	xor.b64  	%rd253, %rd251, %rd249;
	xor.b64  	%rd254, %rd252, %rd250;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd249, 32;
	shr.b64 	%rhs, %rd249, 32;
	add.u64 	%rd255, %lhs, %rhs;
	}
	xor.b64  	%rd256, %rd255, %rd254;
	xor.b64  	%rd257, %rd256, %rd253;
	add.s64 	%rd258, %rd78, 2;
	xor.b64  	%rd259, %rd254, %rd258;
	add.s64 	%rd260, %rd250, %rd253;
	add.s64 	%rd261, %rd255, %rd259;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r244}, %rd253;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r245,%dummy}, %rd253;
	}
	shf.l.wrap.b32 	%r246, %r245, %r244, 13;
	shf.l.wrap.b32 	%r247, %r244, %r245, 13;
	mov.b64 	%rd262, {%r247, %r246};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r248}, %rd259;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r249,%dummy}, %rd259;
	}
	shf.l.wrap.b32 	%r250, %r249, %r248, 16;
	shf.l.wrap.b32 	%r251, %r248, %r249, 16;
	mov.b64 	%rd263, {%r251, %r250};
	xor.b64  	%rd264, %rd262, %rd260;
	xor.b64  	%rd265, %rd263, %rd261;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd260, 32;
	shr.b64 	%rhs, %rd260, 32;
	add.u64 	%rd266, %lhs, %rhs;
	}
	add.s64 	%rd267, %rd264, %rd261;
	add.s64 	%rd268, %rd265, %rd266;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r252}, %rd264;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r253,%dummy}, %rd264;
	}
	shf.l.wrap.b32 	%r254, %r253, %r252, 17;
	shf.l.wrap.b32 	%r255, %r252, %r253, 17;
	mov.b64 	%rd269, {%r255, %r254};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r256}, %rd265;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r257,%dummy}, %rd265;
	}
	shf.l.wrap.b32 	%r258, %r257, %r256, 25;
	shf.l.wrap.b32 	%r259, %r256, %r257, 25;
	mov.b64 	%rd270, {%r259, %r258};
	xor.b64  	%rd271, %rd269, %rd267;
	xor.b64  	%rd272, %rd270, %rd268;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd267, 32;
	shr.b64 	%rhs, %rd267, 32;
	add.u64 	%rd273, %lhs, %rhs;
	}
	add.s64 	%rd274, %rd268, %rd271;
	add.s64 	%rd275, %rd273, %rd272;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r260}, %rd271;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r261,%dummy}, %rd271;
	}
	shf.l.wrap.b32 	%r262, %r261, %r260, 13;
	shf.l.wrap.b32 	%r263, %r260, %r261, 13;
	mov.b64 	%rd276, {%r263, %r262};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r264}, %rd272;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r265,%dummy}, %rd272;
	}
	shf.l.wrap.b32 	%r266, %r265, %r264, 16;
	shf.l.wrap.b32 	%r267, %r264, %r265, 16;
	mov.b64 	%rd277, {%r267, %r266};
	xor.b64  	%rd278, %rd276, %rd274;
	xor.b64  	%rd279, %rd277, %rd275;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd274, 32;
	shr.b64 	%rhs, %rd274, 32;
	add.u64 	%rd280, %lhs, %rhs;
	}
	add.s64 	%rd281, %rd278, %rd275;
	add.s64 	%rd282, %rd279, %rd280;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r268}, %rd278;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r269,%dummy}, %rd278;
	}
	shf.l.wrap.b32 	%r270, %r269, %r268, 17;
	shf.l.wrap.b32 	%r271, %r268, %r269, 17;
	mov.b64 	%rd283, {%r271, %r270};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r272}, %rd279;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r273,%dummy}, %rd279;
	}
	shf.l.wrap.b32 	%r274, %r273, %r272, 25;
	shf.l.wrap.b32 	%r275, %r272, %r273, 25;
	mov.b64 	%rd284, {%r275, %r274};
	xor.b64  	%rd285, %rd283, %rd281;
	xor.b64  	%rd286, %rd284, %rd282;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd281, 32;
	shr.b64 	%rhs, %rd281, 32;
	add.u64 	%rd287, %lhs, %rhs;
	}
	xor.b64  	%rd288, %rd282, %rd258;
	xor.b64  	%rd289, %rd287, 255;
	add.s64 	%rd290, %rd288, %rd285;
	add.s64 	%rd291, %rd289, %rd286;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r276}, %rd285;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r277,%dummy}, %rd285;
	}
	shf.l.wrap.b32 	%r278, %r277, %r276, 13;
	shf.l.wrap.b32 	%r279, %r276, %r277, 13;
	mov.b64 	%rd292, {%r279, %r278};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r280}, %rd286;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r281,%dummy}, %rd286;
	}
	shf.l.wrap.b32 	%r282, %r281, %r280, 16;
	shf.l.wrap.b32 	%r283, %r280, %r281, 16;
	mov.b64 	%rd293, {%r283, %r282};
	xor.b64  	%rd294, %rd292, %rd290;
	xor.b64  	%rd295, %rd293, %rd291;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd290, 32;
	shr.b64 	%rhs, %rd290, 32;
	add.u64 	%rd296, %lhs, %rhs;
	}
	add.s64 	%rd297, %rd294, %rd291;
	add.s64 	%rd298, %rd295, %rd296;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r284}, %rd294;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r285,%dummy}, %rd294;
	}
	shf.l.wrap.b32 	%r286, %r285, %r284, 17;
	shf.l.wrap.b32 	%r287, %r284, %r285, 17;
	mov.b64 	%rd299, {%r287, %r286};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r288}, %rd295;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r289,%dummy}, %rd295;
	}
	shf.l.wrap.b32 	%r290, %r289, %r288, 25;
	shf.l.wrap.b32 	%r291, %r288, %r289, 25;
	mov.b64 	%rd300, {%r291, %r290};
	xor.b64  	%rd301, %rd299, %rd297;
	xor.b64  	%rd302, %rd300, %rd298;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd297, 32;
	shr.b64 	%rhs, %rd297, 32;
	add.u64 	%rd303, %lhs, %rhs;
	}
	add.s64 	%rd304, %rd298, %rd301;
	add.s64 	%rd305, %rd303, %rd302;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r292}, %rd301;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r293,%dummy}, %rd301;
	}
	shf.l.wrap.b32 	%r294, %r293, %r292, 13;
	shf.l.wrap.b32 	%r295, %r292, %r293, 13;
	mov.b64 	%rd306, {%r295, %r294};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r296}, %rd302;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r297,%dummy}, %rd302;
	}
	shf.l.wrap.b32 	%r298, %r297, %r296, 16;
	shf.l.wrap.b32 	%r299, %r296, %r297, 16;
	mov.b64 	%rd307, {%r299, %r298};
	xor.b64  	%rd308, %rd306, %rd304;
	xor.b64  	%rd309, %rd307, %rd305;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd304, 32;
	shr.b64 	%rhs, %rd304, 32;
	add.u64 	%rd310, %lhs, %rhs;
	}
	add.s64 	%rd311, %rd308, %rd305;
	add.s64 	%rd312, %rd309, %rd310;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r300}, %rd308;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r301,%dummy}, %rd308;
	}
	shf.l.wrap.b32 	%r302, %r301, %r300, 17;
	shf.l.wrap.b32 	%r303, %r300, %r301, 17;
	mov.b64 	%rd313, {%r303, %r302};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r304}, %rd309;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r305,%dummy}, %rd309;
	}
	shf.l.wrap.b32 	%r306, %r305, %r304, 25;
	shf.l.wrap.b32 	%r307, %r304, %r305, 25;
	mov.b64 	%rd314, {%r307, %r306};
	xor.b64  	%rd315, %rd313, %rd311;
	xor.b64  	%rd316, %rd314, %rd312;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd311, 32;
	shr.b64 	%rhs, %rd311, 32;
	add.u64 	%rd317, %lhs, %rhs;
	}
	add.s64 	%rd318, %rd312, %rd315;
	add.s64 	%rd319, %rd317, %rd316;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r308}, %rd315;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r309,%dummy}, %rd315;
	}
	shf.l.wrap.b32 	%r310, %r309, %r308, 13;
	shf.l.wrap.b32 	%r311, %r308, %r309, 13;
	mov.b64 	%rd320, {%r311, %r310};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r312}, %rd316;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r313,%dummy}, %rd316;
	}
	shf.l.wrap.b32 	%r314, %r313, %r312, 16;
	shf.l.wrap.b32 	%r315, %r312, %r313, 16;
	mov.b64 	%rd321, {%r315, %r314};
	xor.b64  	%rd322, %rd320, %rd318;
	xor.b64  	%rd323, %rd321, %rd319;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd318, 32;
	shr.b64 	%rhs, %rd318, 32;
	add.u64 	%rd324, %lhs, %rhs;
	}
	add.s64 	%rd325, %rd322, %rd319;
	add.s64 	%rd326, %rd323, %rd324;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r316}, %rd322;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r317,%dummy}, %rd322;
	}
	shf.l.wrap.b32 	%r318, %r317, %r316, 17;
	shf.l.wrap.b32 	%r319, %r316, %r317, 17;
	mov.b64 	%rd327, {%r319, %r318};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r320}, %rd323;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r321,%dummy}, %rd323;
	}
	shf.l.wrap.b32 	%r322, %r321, %r320, 25;
	shf.l.wrap.b32 	%r323, %r320, %r321, 25;
	mov.b64 	%rd328, {%r323, %r322};
	xor.b64  	%rd329, %rd327, %rd325;
	xor.b64  	%rd330, %rd328, %rd326;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd325, 32;
	shr.b64 	%rhs, %rd325, 32;
	add.u64 	%rd331, %lhs, %rhs;
	}
	add.s64 	%rd332, %rd326, %rd329;
	add.s64 	%rd333, %rd331, %rd330;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r324}, %rd329;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r325,%dummy}, %rd329;
	}
	shf.l.wrap.b32 	%r326, %r325, %r324, 13;
	shf.l.wrap.b32 	%r327, %r324, %r325, 13;
	mov.b64 	%rd334, {%r327, %r326};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r328}, %rd330;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r329,%dummy}, %rd330;
	}
	shf.l.wrap.b32 	%r330, %r329, %r328, 16;
	shf.l.wrap.b32 	%r331, %r328, %r329, 16;
	mov.b64 	%rd335, {%r331, %r330};
	xor.b64  	%rd336, %rd334, %rd332;
	xor.b64  	%rd337, %rd335, %rd333;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd332, 32;
	shr.b64 	%rhs, %rd332, 32;
	add.u64 	%rd338, %lhs, %rhs;
	}
	add.s64 	%rd339, %rd336, %rd333;
	add.s64 	%rd340, %rd337, %rd338;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r332}, %rd336;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r333,%dummy}, %rd336;
	}
	shf.l.wrap.b32 	%r334, %r333, %r332, 17;
	shf.l.wrap.b32 	%r335, %r332, %r333, 17;
	mov.b64 	%rd341, {%r335, %r334};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r336}, %rd337;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r337,%dummy}, %rd337;
	}
	shf.l.wrap.b32 	%r338, %r337, %r336, 25;
	shf.l.wrap.b32 	%r339, %r336, %r337, 25;
	mov.b64 	%rd342, {%r339, %r338};
	xor.b64  	%rd343, %rd341, %rd339;
	xor.b64  	%rd344, %rd342, %rd340;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd339, 32;
	shr.b64 	%rhs, %rd339, 32;
	add.u64 	%rd345, %lhs, %rhs;
	}
	xor.b64  	%rd346, %rd345, %rd344;
	xor.b64  	%rd347, %rd346, %rd343;
	add.s64 	%rd348, %rd78, 3;
	xor.b64  	%rd349, %rd344, %rd348;
	add.s64 	%rd350, %rd340, %rd343;
	add.s64 	%rd351, %rd345, %rd349;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r340}, %rd343;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r341,%dummy}, %rd343;
	}
	shf.l.wrap.b32 	%r342, %r341, %r340, 13;
	shf.l.wrap.b32 	%r343, %r340, %r341, 13;
	mov.b64 	%rd352, {%r343, %r342};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r344}, %rd349;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r345,%dummy}, %rd349;
	}
	shf.l.wrap.b32 	%r346, %r345, %r344, 16;
	shf.l.wrap.b32 	%r347, %r344, %r345, 16;
	mov.b64 	%rd353, {%r347, %r346};
	xor.b64  	%rd354, %rd352, %rd350;
	xor.b64  	%rd355, %rd353, %rd351;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd350, 32;
	shr.b64 	%rhs, %rd350, 32;
	add.u64 	%rd356, %lhs, %rhs;
	}
	add.s64 	%rd357, %rd354, %rd351;
	add.s64 	%rd358, %rd355, %rd356;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r348}, %rd354;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r349,%dummy}, %rd354;
	}
	shf.l.wrap.b32 	%r350, %r349, %r348, 17;
	shf.l.wrap.b32 	%r351, %r348, %r349, 17;
	mov.b64 	%rd359, {%r351, %r350};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r352}, %rd355;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r353,%dummy}, %rd355;
	}
	shf.l.wrap.b32 	%r354, %r353, %r352, 25;
	shf.l.wrap.b32 	%r355, %r352, %r353, 25;
	mov.b64 	%rd360, {%r355, %r354};
	xor.b64  	%rd361, %rd359, %rd357;
	xor.b64  	%rd362, %rd360, %rd358;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd357, 32;
	shr.b64 	%rhs, %rd357, 32;
	add.u64 	%rd363, %lhs, %rhs;
	}
	add.s64 	%rd364, %rd358, %rd361;
	add.s64 	%rd365, %rd363, %rd362;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r356}, %rd361;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r357,%dummy}, %rd361;
	}
	shf.l.wrap.b32 	%r358, %r357, %r356, 13;
	shf.l.wrap.b32 	%r359, %r356, %r357, 13;
	mov.b64 	%rd366, {%r359, %r358};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r360}, %rd362;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r361,%dummy}, %rd362;
	}
	shf.l.wrap.b32 	%r362, %r361, %r360, 16;
	shf.l.wrap.b32 	%r363, %r360, %r361, 16;
	mov.b64 	%rd367, {%r363, %r362};
	xor.b64  	%rd368, %rd366, %rd364;
	xor.b64  	%rd369, %rd367, %rd365;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd364, 32;
	shr.b64 	%rhs, %rd364, 32;
	add.u64 	%rd370, %lhs, %rhs;
	}
	add.s64 	%rd371, %rd368, %rd365;
	add.s64 	%rd372, %rd369, %rd370;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r364}, %rd368;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r365,%dummy}, %rd368;
	}
	shf.l.wrap.b32 	%r366, %r365, %r364, 17;
	shf.l.wrap.b32 	%r367, %r364, %r365, 17;
	mov.b64 	%rd373, {%r367, %r366};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r368}, %rd369;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r369,%dummy}, %rd369;
	}
	shf.l.wrap.b32 	%r370, %r369, %r368, 25;
	shf.l.wrap.b32 	%r371, %r368, %r369, 25;
	mov.b64 	%rd374, {%r371, %r370};
	xor.b64  	%rd375, %rd373, %rd371;
	xor.b64  	%rd376, %rd374, %rd372;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd371, 32;
	shr.b64 	%rhs, %rd371, 32;
	add.u64 	%rd377, %lhs, %rhs;
	}
	xor.b64  	%rd378, %rd372, %rd348;
	xor.b64  	%rd379, %rd377, 255;
	add.s64 	%rd380, %rd378, %rd375;
	add.s64 	%rd381, %rd379, %rd376;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r372}, %rd375;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r373,%dummy}, %rd375;
	}
	shf.l.wrap.b32 	%r374, %r373, %r372, 13;
	shf.l.wrap.b32 	%r375, %r372, %r373, 13;
	mov.b64 	%rd382, {%r375, %r374};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r376}, %rd376;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r377,%dummy}, %rd376;
	}
	shf.l.wrap.b32 	%r378, %r377, %r376, 16;
	shf.l.wrap.b32 	%r379, %r376, %r377, 16;
	mov.b64 	%rd383, {%r379, %r378};
	xor.b64  	%rd384, %rd382, %rd380;
	xor.b64  	%rd385, %rd383, %rd381;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd380, 32;
	shr.b64 	%rhs, %rd380, 32;
	add.u64 	%rd386, %lhs, %rhs;
	}
	add.s64 	%rd387, %rd384, %rd381;
	add.s64 	%rd388, %rd385, %rd386;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r380}, %rd384;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r381,%dummy}, %rd384;
	}
	shf.l.wrap.b32 	%r382, %r381, %r380, 17;
	shf.l.wrap.b32 	%r383, %r380, %r381, 17;
	mov.b64 	%rd389, {%r383, %r382};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r384}, %rd385;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r385,%dummy}, %rd385;
	}
	shf.l.wrap.b32 	%r386, %r385, %r384, 25;
	shf.l.wrap.b32 	%r387, %r384, %r385, 25;
	mov.b64 	%rd390, {%r387, %r386};
	xor.b64  	%rd391, %rd389, %rd387;
	xor.b64  	%rd392, %rd390, %rd388;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd387, 32;
	shr.b64 	%rhs, %rd387, 32;
	add.u64 	%rd393, %lhs, %rhs;
	}
	add.s64 	%rd394, %rd388, %rd391;
	add.s64 	%rd395, %rd393, %rd392;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r388}, %rd391;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r389,%dummy}, %rd391;
	}
	shf.l.wrap.b32 	%r390, %r389, %r388, 13;
	shf.l.wrap.b32 	%r391, %r388, %r389, 13;
	mov.b64 	%rd396, {%r391, %r390};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r392}, %rd392;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r393,%dummy}, %rd392;
	}
	shf.l.wrap.b32 	%r394, %r393, %r392, 16;
	shf.l.wrap.b32 	%r395, %r392, %r393, 16;
	mov.b64 	%rd397, {%r395, %r394};
	xor.b64  	%rd398, %rd396, %rd394;
	xor.b64  	%rd399, %rd397, %rd395;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd394, 32;
	shr.b64 	%rhs, %rd394, 32;
	add.u64 	%rd400, %lhs, %rhs;
	}
	add.s64 	%rd401, %rd398, %rd395;
	add.s64 	%rd402, %rd399, %rd400;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r396}, %rd398;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r397,%dummy}, %rd398;
	}
	shf.l.wrap.b32 	%r398, %r397, %r396, 17;
	shf.l.wrap.b32 	%r399, %r396, %r397, 17;
	mov.b64 	%rd403, {%r399, %r398};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r400}, %rd399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r401,%dummy}, %rd399;
	}
	shf.l.wrap.b32 	%r402, %r401, %r400, 25;
	shf.l.wrap.b32 	%r403, %r400, %r401, 25;
	mov.b64 	%rd404, {%r403, %r402};
	xor.b64  	%rd405, %rd403, %rd401;
	xor.b64  	%rd406, %rd404, %rd402;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd401, 32;
	shr.b64 	%rhs, %rd401, 32;
	add.u64 	%rd407, %lhs, %rhs;
	}
	add.s64 	%rd408, %rd402, %rd405;
	add.s64 	%rd409, %rd407, %rd406;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r404}, %rd405;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r405,%dummy}, %rd405;
	}
	shf.l.wrap.b32 	%r406, %r405, %r404, 13;
	shf.l.wrap.b32 	%r407, %r404, %r405, 13;
	mov.b64 	%rd410, {%r407, %r406};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r408}, %rd406;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r409,%dummy}, %rd406;
	}
	shf.l.wrap.b32 	%r410, %r409, %r408, 16;
	shf.l.wrap.b32 	%r411, %r408, %r409, 16;
	mov.b64 	%rd411, {%r411, %r410};
	xor.b64  	%rd412, %rd410, %rd408;
	xor.b64  	%rd413, %rd411, %rd409;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd408, 32;
	shr.b64 	%rhs, %rd408, 32;
	add.u64 	%rd414, %lhs, %rhs;
	}
	add.s64 	%rd415, %rd412, %rd409;
	add.s64 	%rd416, %rd413, %rd414;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r412}, %rd412;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r413,%dummy}, %rd412;
	}
	shf.l.wrap.b32 	%r414, %r413, %r412, 17;
	shf.l.wrap.b32 	%r415, %r412, %r413, 17;
	mov.b64 	%rd417, {%r415, %r414};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r416}, %rd413;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r417,%dummy}, %rd413;
	}
	shf.l.wrap.b32 	%r418, %r417, %r416, 25;
	shf.l.wrap.b32 	%r419, %r416, %r417, 25;
	mov.b64 	%rd418, {%r419, %r418};
	xor.b64  	%rd419, %rd417, %rd415;
	xor.b64  	%rd420, %rd418, %rd416;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd415, 32;
	shr.b64 	%rhs, %rd415, 32;
	add.u64 	%rd421, %lhs, %rhs;
	}
	add.s64 	%rd422, %rd416, %rd419;
	add.s64 	%rd423, %rd421, %rd420;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r420}, %rd419;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r421,%dummy}, %rd419;
	}
	shf.l.wrap.b32 	%r422, %r421, %r420, 13;
	shf.l.wrap.b32 	%r423, %r420, %r421, 13;
	mov.b64 	%rd424, {%r423, %r422};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r424}, %rd420;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r425,%dummy}, %rd420;
	}
	shf.l.wrap.b32 	%r426, %r425, %r424, 16;
	shf.l.wrap.b32 	%r427, %r424, %r425, 16;
	mov.b64 	%rd425, {%r427, %r426};
	xor.b64  	%rd426, %rd424, %rd422;
	xor.b64  	%rd427, %rd425, %rd423;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd422, 32;
	shr.b64 	%rhs, %rd422, 32;
	add.u64 	%rd428, %lhs, %rhs;
	}
	add.s64 	%rd429, %rd426, %rd423;
	add.s64 	%rd981, %rd427, %rd428;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r428}, %rd426;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r429,%dummy}, %rd426;
	}
	shf.l.wrap.b32 	%r430, %r429, %r428, 17;
	shf.l.wrap.b32 	%r431, %r428, %r429, 17;
	mov.b64 	%rd430, {%r431, %r430};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r432}, %rd427;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r433,%dummy}, %rd427;
	}
	shf.l.wrap.b32 	%r434, %r433, %r432, 25;
	shf.l.wrap.b32 	%r435, %r432, %r433, 25;
	mov.b64 	%rd431, {%r435, %r434};
	xor.b64  	%rd982, %rd430, %rd429;
	xor.b64  	%rd984, %rd431, %rd981;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd429, 32;
	shr.b64 	%rhs, %rd429, 32;
	add.u64 	%rd983, %lhs, %rhs;
	}
	xor.b64  	%rd432, %rd983, %rd984;
	xor.b64  	%rd433, %rd432, %rd982;
	cvt.s32.s16	%r436, %rs13;
	shr.s32 	%r437, %r436, 31;
	shr.u32 	%r438, %r437, 30;
	add.s32 	%r439, %r436, %r438;
	shr.s32 	%r440, %r439, 2;
	mul.wide.s32 	%rd434, %r440, 32;
	add.s64 	%rd435, %rd3, %rd434;
	xor.b64  	%rd436, %rd257, %rd250;
	xor.b64  	%rd437, %rd167, %rd160;
	st.local.v2.u64 	[%rd435], {%rd437, %rd436};
	xor.b64  	%rd438, %rd433, %rd981;
	xor.b64  	%rd439, %rd347, %rd340;
	st.local.v2.u64 	[%rd435+16], {%rd439, %rd438};
	cvt.u32.u16	%r441, %rs13;
	add.s32 	%r442, %r441, 4;
	cvt.u16.u32	%rs13, %r442;
	setp.lt.s16	%p1, %rs13, 60;
	@%p1 bra 	BB0_2;

	add.s64 	%rd440, %rd5, 60;
	xor.b64  	%rd441, %rd984, %rd440;
	add.s64 	%rd442, %rd983, %rd441;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r443}, %rd441;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r444,%dummy}, %rd441;
	}
	shf.l.wrap.b32 	%r445, %r444, %r443, 16;
	shf.l.wrap.b32 	%r446, %r443, %r444, 16;
	mov.b64 	%rd443, {%r446, %r445};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r447}, %rd982;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r448,%dummy}, %rd982;
	}
	shf.l.wrap.b32 	%r449, %r448, %r447, 13;
	shf.l.wrap.b32 	%r450, %r447, %r448, 13;
	mov.b64 	%rd444, {%r450, %r449};
	add.s64 	%rd445, %rd981, %rd982;
	xor.b64  	%rd446, %rd444, %rd445;
	xor.b64  	%rd447, %rd443, %rd442;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd445, 32;
	shr.b64 	%rhs, %rd445, 32;
	add.u64 	%rd448, %lhs, %rhs;
	}
	add.s64 	%rd449, %rd446, %rd442;
	add.s64 	%rd450, %rd447, %rd448;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r451}, %rd446;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r452,%dummy}, %rd446;
	}
	shf.l.wrap.b32 	%r453, %r452, %r451, 17;
	shf.l.wrap.b32 	%r454, %r451, %r452, 17;
	mov.b64 	%rd451, {%r454, %r453};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r455}, %rd447;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r456,%dummy}, %rd447;
	}
	shf.l.wrap.b32 	%r457, %r456, %r455, 25;
	shf.l.wrap.b32 	%r458, %r455, %r456, 25;
	mov.b64 	%rd452, {%r458, %r457};
	xor.b64  	%rd453, %rd451, %rd449;
	xor.b64  	%rd454, %rd452, %rd450;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd449, 32;
	shr.b64 	%rhs, %rd449, 32;
	add.u64 	%rd455, %lhs, %rhs;
	}
	add.s64 	%rd456, %rd450, %rd453;
	add.s64 	%rd457, %rd455, %rd454;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r459}, %rd453;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r460,%dummy}, %rd453;
	}
	shf.l.wrap.b32 	%r461, %r460, %r459, 13;
	shf.l.wrap.b32 	%r462, %r459, %r460, 13;
	mov.b64 	%rd458, {%r462, %r461};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r463}, %rd454;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r464,%dummy}, %rd454;
	}
	shf.l.wrap.b32 	%r465, %r464, %r463, 16;
	shf.l.wrap.b32 	%r466, %r463, %r464, 16;
	mov.b64 	%rd459, {%r466, %r465};
	xor.b64  	%rd460, %rd458, %rd456;
	xor.b64  	%rd461, %rd459, %rd457;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd456, 32;
	shr.b64 	%rhs, %rd456, 32;
	add.u64 	%rd462, %lhs, %rhs;
	}
	add.s64 	%rd463, %rd460, %rd457;
	add.s64 	%rd464, %rd461, %rd462;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r467}, %rd460;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r468,%dummy}, %rd460;
	}
	shf.l.wrap.b32 	%r469, %r468, %r467, 17;
	shf.l.wrap.b32 	%r470, %r467, %r468, 17;
	mov.b64 	%rd465, {%r470, %r469};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r471}, %rd461;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r472,%dummy}, %rd461;
	}
	shf.l.wrap.b32 	%r473, %r472, %r471, 25;
	shf.l.wrap.b32 	%r474, %r471, %r472, 25;
	mov.b64 	%rd466, {%r474, %r473};
	xor.b64  	%rd467, %rd465, %rd463;
	xor.b64  	%rd468, %rd466, %rd464;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd463, 32;
	shr.b64 	%rhs, %rd463, 32;
	add.u64 	%rd469, %lhs, %rhs;
	}
	xor.b64  	%rd470, %rd464, %rd440;
	xor.b64  	%rd471, %rd469, 255;
	add.s64 	%rd472, %rd470, %rd467;
	add.s64 	%rd473, %rd471, %rd468;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r475}, %rd467;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r476,%dummy}, %rd467;
	}
	shf.l.wrap.b32 	%r477, %r476, %r475, 13;
	shf.l.wrap.b32 	%r478, %r475, %r476, 13;
	mov.b64 	%rd474, {%r478, %r477};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r479}, %rd468;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r480,%dummy}, %rd468;
	}
	shf.l.wrap.b32 	%r481, %r480, %r479, 16;
	shf.l.wrap.b32 	%r482, %r479, %r480, 16;
	mov.b64 	%rd475, {%r482, %r481};
	xor.b64  	%rd476, %rd474, %rd472;
	xor.b64  	%rd477, %rd475, %rd473;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd472, 32;
	shr.b64 	%rhs, %rd472, 32;
	add.u64 	%rd478, %lhs, %rhs;
	}
	add.s64 	%rd479, %rd476, %rd473;
	add.s64 	%rd480, %rd477, %rd478;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r483}, %rd476;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r484,%dummy}, %rd476;
	}
	shf.l.wrap.b32 	%r485, %r484, %r483, 17;
	shf.l.wrap.b32 	%r486, %r483, %r484, 17;
	mov.b64 	%rd481, {%r486, %r485};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r487}, %rd477;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r488,%dummy}, %rd477;
	}
	shf.l.wrap.b32 	%r489, %r488, %r487, 25;
	shf.l.wrap.b32 	%r490, %r487, %r488, 25;
	mov.b64 	%rd482, {%r490, %r489};
	xor.b64  	%rd483, %rd481, %rd479;
	xor.b64  	%rd484, %rd482, %rd480;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd479, 32;
	shr.b64 	%rhs, %rd479, 32;
	add.u64 	%rd485, %lhs, %rhs;
	}
	add.s64 	%rd486, %rd480, %rd483;
	add.s64 	%rd487, %rd485, %rd484;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r491}, %rd483;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r492,%dummy}, %rd483;
	}
	shf.l.wrap.b32 	%r493, %r492, %r491, 13;
	shf.l.wrap.b32 	%r494, %r491, %r492, 13;
	mov.b64 	%rd488, {%r494, %r493};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r495}, %rd484;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r496,%dummy}, %rd484;
	}
	shf.l.wrap.b32 	%r497, %r496, %r495, 16;
	shf.l.wrap.b32 	%r498, %r495, %r496, 16;
	mov.b64 	%rd489, {%r498, %r497};
	xor.b64  	%rd490, %rd488, %rd486;
	xor.b64  	%rd491, %rd489, %rd487;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd486, 32;
	shr.b64 	%rhs, %rd486, 32;
	add.u64 	%rd492, %lhs, %rhs;
	}
	add.s64 	%rd493, %rd490, %rd487;
	add.s64 	%rd494, %rd491, %rd492;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r499}, %rd490;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r500,%dummy}, %rd490;
	}
	shf.l.wrap.b32 	%r501, %r500, %r499, 17;
	shf.l.wrap.b32 	%r502, %r499, %r500, 17;
	mov.b64 	%rd495, {%r502, %r501};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r503}, %rd491;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r504,%dummy}, %rd491;
	}
	shf.l.wrap.b32 	%r505, %r504, %r503, 25;
	shf.l.wrap.b32 	%r506, %r503, %r504, 25;
	mov.b64 	%rd496, {%r506, %r505};
	xor.b64  	%rd497, %rd495, %rd493;
	xor.b64  	%rd498, %rd496, %rd494;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd493, 32;
	shr.b64 	%rhs, %rd493, 32;
	add.u64 	%rd499, %lhs, %rhs;
	}
	add.s64 	%rd500, %rd494, %rd497;
	add.s64 	%rd501, %rd499, %rd498;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r507}, %rd497;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r508,%dummy}, %rd497;
	}
	shf.l.wrap.b32 	%r509, %r508, %r507, 13;
	shf.l.wrap.b32 	%r510, %r507, %r508, 13;
	mov.b64 	%rd502, {%r510, %r509};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r511}, %rd498;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r512,%dummy}, %rd498;
	}
	shf.l.wrap.b32 	%r513, %r512, %r511, 16;
	shf.l.wrap.b32 	%r514, %r511, %r512, 16;
	mov.b64 	%rd503, {%r514, %r513};
	xor.b64  	%rd504, %rd502, %rd500;
	xor.b64  	%rd505, %rd503, %rd501;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd500, 32;
	shr.b64 	%rhs, %rd500, 32;
	add.u64 	%rd506, %lhs, %rhs;
	}
	add.s64 	%rd507, %rd504, %rd501;
	add.s64 	%rd508, %rd505, %rd506;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r515}, %rd504;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r516,%dummy}, %rd504;
	}
	shf.l.wrap.b32 	%r517, %r516, %r515, 17;
	shf.l.wrap.b32 	%r518, %r515, %r516, 17;
	mov.b64 	%rd509, {%r518, %r517};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r519}, %rd505;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r520,%dummy}, %rd505;
	}
	shf.l.wrap.b32 	%r521, %r520, %r519, 25;
	shf.l.wrap.b32 	%r522, %r519, %r520, 25;
	mov.b64 	%rd510, {%r522, %r521};
	xor.b64  	%rd511, %rd509, %rd507;
	xor.b64  	%rd512, %rd510, %rd508;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd507, 32;
	shr.b64 	%rhs, %rd507, 32;
	add.u64 	%rd513, %lhs, %rhs;
	}
	add.s64 	%rd514, %rd508, %rd511;
	add.s64 	%rd515, %rd513, %rd512;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r523}, %rd511;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r524,%dummy}, %rd511;
	}
	shf.l.wrap.b32 	%r525, %r524, %r523, 13;
	shf.l.wrap.b32 	%r526, %r523, %r524, 13;
	mov.b64 	%rd516, {%r526, %r525};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r527}, %rd512;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r528,%dummy}, %rd512;
	}
	shf.l.wrap.b32 	%r529, %r528, %r527, 16;
	shf.l.wrap.b32 	%r530, %r527, %r528, 16;
	mov.b64 	%rd517, {%r530, %r529};
	xor.b64  	%rd518, %rd516, %rd514;
	xor.b64  	%rd519, %rd517, %rd515;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd514, 32;
	shr.b64 	%rhs, %rd514, 32;
	add.u64 	%rd520, %lhs, %rhs;
	}
	add.s64 	%rd521, %rd518, %rd515;
	add.s64 	%rd14, %rd519, %rd520;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r531}, %rd518;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r532,%dummy}, %rd518;
	}
	shf.l.wrap.b32 	%r533, %r532, %r531, 17;
	shf.l.wrap.b32 	%r534, %r531, %r532, 17;
	mov.b64 	%rd522, {%r534, %r533};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r535}, %rd519;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r536,%dummy}, %rd519;
	}
	shf.l.wrap.b32 	%r537, %r536, %r535, 25;
	shf.l.wrap.b32 	%r538, %r535, %r536, 25;
	mov.b64 	%rd523, {%r538, %r537};
	xor.b64  	%rd15, %rd522, %rd521;
	xor.b64  	%rd524, %rd523, %rd14;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd521, 32;
	shr.b64 	%rhs, %rd521, 32;
	add.u64 	%rd525, %lhs, %rhs;
	}
	xor.b64  	%rd16, %rd525, %rd524;
	add.s64 	%rd526, %rd5, 61;
	xor.b64  	%rd527, %rd524, %rd526;
	add.s64 	%rd528, %rd14, %rd15;
	add.s64 	%rd529, %rd525, %rd527;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r539}, %rd15;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r540,%dummy}, %rd15;
	}
	shf.l.wrap.b32 	%r541, %r540, %r539, 13;
	shf.l.wrap.b32 	%r542, %r539, %r540, 13;
	mov.b64 	%rd530, {%r542, %r541};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r543}, %rd527;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r544,%dummy}, %rd527;
	}
	shf.l.wrap.b32 	%r545, %r544, %r543, 16;
	shf.l.wrap.b32 	%r546, %r543, %r544, 16;
	mov.b64 	%rd531, {%r546, %r545};
	xor.b64  	%rd532, %rd530, %rd528;
	xor.b64  	%rd533, %rd531, %rd529;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd528, 32;
	shr.b64 	%rhs, %rd528, 32;
	add.u64 	%rd534, %lhs, %rhs;
	}
	add.s64 	%rd535, %rd532, %rd529;
	add.s64 	%rd536, %rd533, %rd534;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r547}, %rd532;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r548,%dummy}, %rd532;
	}
	shf.l.wrap.b32 	%r549, %r548, %r547, 17;
	shf.l.wrap.b32 	%r550, %r547, %r548, 17;
	mov.b64 	%rd537, {%r550, %r549};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r551}, %rd533;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r552,%dummy}, %rd533;
	}
	shf.l.wrap.b32 	%r553, %r552, %r551, 25;
	shf.l.wrap.b32 	%r554, %r551, %r552, 25;
	mov.b64 	%rd538, {%r554, %r553};
	xor.b64  	%rd539, %rd537, %rd535;
	xor.b64  	%rd540, %rd538, %rd536;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd535, 32;
	shr.b64 	%rhs, %rd535, 32;
	add.u64 	%rd541, %lhs, %rhs;
	}
	add.s64 	%rd542, %rd536, %rd539;
	add.s64 	%rd543, %rd541, %rd540;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r555}, %rd539;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r556,%dummy}, %rd539;
	}
	shf.l.wrap.b32 	%r557, %r556, %r555, 13;
	shf.l.wrap.b32 	%r558, %r555, %r556, 13;
	mov.b64 	%rd544, {%r558, %r557};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r559}, %rd540;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r560,%dummy}, %rd540;
	}
	shf.l.wrap.b32 	%r561, %r560, %r559, 16;
	shf.l.wrap.b32 	%r562, %r559, %r560, 16;
	mov.b64 	%rd545, {%r562, %r561};
	xor.b64  	%rd546, %rd544, %rd542;
	xor.b64  	%rd547, %rd545, %rd543;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd542, 32;
	shr.b64 	%rhs, %rd542, 32;
	add.u64 	%rd548, %lhs, %rhs;
	}
	add.s64 	%rd549, %rd546, %rd543;
	add.s64 	%rd550, %rd547, %rd548;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r563}, %rd546;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r564,%dummy}, %rd546;
	}
	shf.l.wrap.b32 	%r565, %r564, %r563, 17;
	shf.l.wrap.b32 	%r566, %r563, %r564, 17;
	mov.b64 	%rd551, {%r566, %r565};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r567}, %rd547;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r568,%dummy}, %rd547;
	}
	shf.l.wrap.b32 	%r569, %r568, %r567, 25;
	shf.l.wrap.b32 	%r570, %r567, %r568, 25;
	mov.b64 	%rd552, {%r570, %r569};
	xor.b64  	%rd553, %rd551, %rd549;
	xor.b64  	%rd554, %rd552, %rd550;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd549, 32;
	shr.b64 	%rhs, %rd549, 32;
	add.u64 	%rd555, %lhs, %rhs;
	}
	xor.b64  	%rd556, %rd550, %rd526;
	xor.b64  	%rd557, %rd555, 255;
	add.s64 	%rd558, %rd556, %rd553;
	add.s64 	%rd559, %rd557, %rd554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r571}, %rd553;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r572,%dummy}, %rd553;
	}
	shf.l.wrap.b32 	%r573, %r572, %r571, 13;
	shf.l.wrap.b32 	%r574, %r571, %r572, 13;
	mov.b64 	%rd560, {%r574, %r573};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r575}, %rd554;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r576,%dummy}, %rd554;
	}
	shf.l.wrap.b32 	%r577, %r576, %r575, 16;
	shf.l.wrap.b32 	%r578, %r575, %r576, 16;
	mov.b64 	%rd561, {%r578, %r577};
	xor.b64  	%rd562, %rd560, %rd558;
	xor.b64  	%rd563, %rd561, %rd559;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd558, 32;
	shr.b64 	%rhs, %rd558, 32;
	add.u64 	%rd564, %lhs, %rhs;
	}
	add.s64 	%rd565, %rd562, %rd559;
	add.s64 	%rd566, %rd563, %rd564;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r579}, %rd562;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r580,%dummy}, %rd562;
	}
	shf.l.wrap.b32 	%r581, %r580, %r579, 17;
	shf.l.wrap.b32 	%r582, %r579, %r580, 17;
	mov.b64 	%rd567, {%r582, %r581};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r583}, %rd563;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r584,%dummy}, %rd563;
	}
	shf.l.wrap.b32 	%r585, %r584, %r583, 25;
	shf.l.wrap.b32 	%r586, %r583, %r584, 25;
	mov.b64 	%rd568, {%r586, %r585};
	xor.b64  	%rd569, %rd567, %rd565;
	xor.b64  	%rd570, %rd568, %rd566;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd565, 32;
	shr.b64 	%rhs, %rd565, 32;
	add.u64 	%rd571, %lhs, %rhs;
	}
	add.s64 	%rd572, %rd566, %rd569;
	add.s64 	%rd573, %rd571, %rd570;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r587}, %rd569;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r588,%dummy}, %rd569;
	}
	shf.l.wrap.b32 	%r589, %r588, %r587, 13;
	shf.l.wrap.b32 	%r590, %r587, %r588, 13;
	mov.b64 	%rd574, {%r590, %r589};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r591}, %rd570;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r592,%dummy}, %rd570;
	}
	shf.l.wrap.b32 	%r593, %r592, %r591, 16;
	shf.l.wrap.b32 	%r594, %r591, %r592, 16;
	mov.b64 	%rd575, {%r594, %r593};
	xor.b64  	%rd576, %rd574, %rd572;
	xor.b64  	%rd577, %rd575, %rd573;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd572, 32;
	shr.b64 	%rhs, %rd572, 32;
	add.u64 	%rd578, %lhs, %rhs;
	}
	add.s64 	%rd579, %rd576, %rd573;
	add.s64 	%rd580, %rd577, %rd578;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r595}, %rd576;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r596,%dummy}, %rd576;
	}
	shf.l.wrap.b32 	%r597, %r596, %r595, 17;
	shf.l.wrap.b32 	%r598, %r595, %r596, 17;
	mov.b64 	%rd581, {%r598, %r597};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r599}, %rd577;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r600,%dummy}, %rd577;
	}
	shf.l.wrap.b32 	%r601, %r600, %r599, 25;
	shf.l.wrap.b32 	%r602, %r599, %r600, 25;
	mov.b64 	%rd582, {%r602, %r601};
	xor.b64  	%rd583, %rd581, %rd579;
	xor.b64  	%rd584, %rd582, %rd580;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd579, 32;
	shr.b64 	%rhs, %rd579, 32;
	add.u64 	%rd585, %lhs, %rhs;
	}
	add.s64 	%rd586, %rd580, %rd583;
	add.s64 	%rd587, %rd585, %rd584;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r603}, %rd583;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r604,%dummy}, %rd583;
	}
	shf.l.wrap.b32 	%r605, %r604, %r603, 13;
	shf.l.wrap.b32 	%r606, %r603, %r604, 13;
	mov.b64 	%rd588, {%r606, %r605};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r607}, %rd584;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r608,%dummy}, %rd584;
	}
	shf.l.wrap.b32 	%r609, %r608, %r607, 16;
	shf.l.wrap.b32 	%r610, %r607, %r608, 16;
	mov.b64 	%rd589, {%r610, %r609};
	xor.b64  	%rd590, %rd588, %rd586;
	xor.b64  	%rd591, %rd589, %rd587;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd586, 32;
	shr.b64 	%rhs, %rd586, 32;
	add.u64 	%rd592, %lhs, %rhs;
	}
	add.s64 	%rd593, %rd590, %rd587;
	add.s64 	%rd594, %rd591, %rd592;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r611}, %rd590;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r612,%dummy}, %rd590;
	}
	shf.l.wrap.b32 	%r613, %r612, %r611, 17;
	shf.l.wrap.b32 	%r614, %r611, %r612, 17;
	mov.b64 	%rd595, {%r614, %r613};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r615}, %rd591;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r616,%dummy}, %rd591;
	}
	shf.l.wrap.b32 	%r617, %r616, %r615, 25;
	shf.l.wrap.b32 	%r618, %r615, %r616, 25;
	mov.b64 	%rd596, {%r618, %r617};
	xor.b64  	%rd597, %rd595, %rd593;
	xor.b64  	%rd598, %rd596, %rd594;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd593, 32;
	shr.b64 	%rhs, %rd593, 32;
	add.u64 	%rd599, %lhs, %rhs;
	}
	add.s64 	%rd600, %rd594, %rd597;
	add.s64 	%rd601, %rd599, %rd598;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r619}, %rd597;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r620,%dummy}, %rd597;
	}
	shf.l.wrap.b32 	%r621, %r620, %r619, 13;
	shf.l.wrap.b32 	%r622, %r619, %r620, 13;
	mov.b64 	%rd602, {%r622, %r621};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r623}, %rd598;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r624,%dummy}, %rd598;
	}
	shf.l.wrap.b32 	%r625, %r624, %r623, 16;
	shf.l.wrap.b32 	%r626, %r623, %r624, 16;
	mov.b64 	%rd603, {%r626, %r625};
	xor.b64  	%rd604, %rd602, %rd600;
	xor.b64  	%rd605, %rd603, %rd601;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd600, 32;
	shr.b64 	%rhs, %rd600, 32;
	add.u64 	%rd606, %lhs, %rhs;
	}
	add.s64 	%rd607, %rd604, %rd601;
	add.s64 	%rd17, %rd605, %rd606;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r627}, %rd604;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r628,%dummy}, %rd604;
	}
	shf.l.wrap.b32 	%r629, %r628, %r627, 17;
	shf.l.wrap.b32 	%r630, %r627, %r628, 17;
	mov.b64 	%rd608, {%r630, %r629};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r631}, %rd605;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r632,%dummy}, %rd605;
	}
	shf.l.wrap.b32 	%r633, %r632, %r631, 25;
	shf.l.wrap.b32 	%r634, %r631, %r632, 25;
	mov.b64 	%rd609, {%r634, %r633};
	xor.b64  	%rd18, %rd608, %rd607;
	xor.b64  	%rd610, %rd609, %rd17;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd607, 32;
	shr.b64 	%rhs, %rd607, 32;
	add.u64 	%rd611, %lhs, %rhs;
	}
	xor.b64  	%rd19, %rd611, %rd610;
	add.s64 	%rd612, %rd5, 62;
	xor.b64  	%rd613, %rd610, %rd612;
	add.s64 	%rd614, %rd17, %rd18;
	add.s64 	%rd615, %rd611, %rd613;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r635}, %rd18;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r636,%dummy}, %rd18;
	}
	shf.l.wrap.b32 	%r637, %r636, %r635, 13;
	shf.l.wrap.b32 	%r638, %r635, %r636, 13;
	mov.b64 	%rd616, {%r638, %r637};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r639}, %rd613;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r640,%dummy}, %rd613;
	}
	shf.l.wrap.b32 	%r641, %r640, %r639, 16;
	shf.l.wrap.b32 	%r642, %r639, %r640, 16;
	mov.b64 	%rd617, {%r642, %r641};
	xor.b64  	%rd618, %rd616, %rd614;
	xor.b64  	%rd619, %rd617, %rd615;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd614, 32;
	shr.b64 	%rhs, %rd614, 32;
	add.u64 	%rd620, %lhs, %rhs;
	}
	add.s64 	%rd621, %rd618, %rd615;
	add.s64 	%rd622, %rd619, %rd620;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r643}, %rd618;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r644,%dummy}, %rd618;
	}
	shf.l.wrap.b32 	%r645, %r644, %r643, 17;
	shf.l.wrap.b32 	%r646, %r643, %r644, 17;
	mov.b64 	%rd623, {%r646, %r645};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r647}, %rd619;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r648,%dummy}, %rd619;
	}
	shf.l.wrap.b32 	%r649, %r648, %r647, 25;
	shf.l.wrap.b32 	%r650, %r647, %r648, 25;
	mov.b64 	%rd624, {%r650, %r649};
	xor.b64  	%rd625, %rd623, %rd621;
	xor.b64  	%rd626, %rd624, %rd622;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd621, 32;
	shr.b64 	%rhs, %rd621, 32;
	add.u64 	%rd627, %lhs, %rhs;
	}
	add.s64 	%rd628, %rd622, %rd625;
	add.s64 	%rd629, %rd627, %rd626;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r651}, %rd625;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r652,%dummy}, %rd625;
	}
	shf.l.wrap.b32 	%r653, %r652, %r651, 13;
	shf.l.wrap.b32 	%r654, %r651, %r652, 13;
	mov.b64 	%rd630, {%r654, %r653};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r655}, %rd626;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r656,%dummy}, %rd626;
	}
	shf.l.wrap.b32 	%r657, %r656, %r655, 16;
	shf.l.wrap.b32 	%r658, %r655, %r656, 16;
	mov.b64 	%rd631, {%r658, %r657};
	xor.b64  	%rd632, %rd630, %rd628;
	xor.b64  	%rd633, %rd631, %rd629;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd628, 32;
	shr.b64 	%rhs, %rd628, 32;
	add.u64 	%rd634, %lhs, %rhs;
	}
	add.s64 	%rd635, %rd632, %rd629;
	add.s64 	%rd636, %rd633, %rd634;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r659}, %rd632;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r660,%dummy}, %rd632;
	}
	shf.l.wrap.b32 	%r661, %r660, %r659, 17;
	shf.l.wrap.b32 	%r662, %r659, %r660, 17;
	mov.b64 	%rd637, {%r662, %r661};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r663}, %rd633;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r664,%dummy}, %rd633;
	}
	shf.l.wrap.b32 	%r665, %r664, %r663, 25;
	shf.l.wrap.b32 	%r666, %r663, %r664, 25;
	mov.b64 	%rd638, {%r666, %r665};
	xor.b64  	%rd639, %rd637, %rd635;
	xor.b64  	%rd640, %rd638, %rd636;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd635, 32;
	shr.b64 	%rhs, %rd635, 32;
	add.u64 	%rd641, %lhs, %rhs;
	}
	xor.b64  	%rd642, %rd636, %rd612;
	xor.b64  	%rd643, %rd641, 255;
	add.s64 	%rd644, %rd642, %rd639;
	add.s64 	%rd645, %rd643, %rd640;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r667}, %rd639;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r668,%dummy}, %rd639;
	}
	shf.l.wrap.b32 	%r669, %r668, %r667, 13;
	shf.l.wrap.b32 	%r670, %r667, %r668, 13;
	mov.b64 	%rd646, {%r670, %r669};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r671}, %rd640;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r672,%dummy}, %rd640;
	}
	shf.l.wrap.b32 	%r673, %r672, %r671, 16;
	shf.l.wrap.b32 	%r674, %r671, %r672, 16;
	mov.b64 	%rd647, {%r674, %r673};
	xor.b64  	%rd648, %rd646, %rd644;
	xor.b64  	%rd649, %rd647, %rd645;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd644, 32;
	shr.b64 	%rhs, %rd644, 32;
	add.u64 	%rd650, %lhs, %rhs;
	}
	add.s64 	%rd651, %rd648, %rd645;
	add.s64 	%rd652, %rd649, %rd650;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r675}, %rd648;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r676,%dummy}, %rd648;
	}
	shf.l.wrap.b32 	%r677, %r676, %r675, 17;
	shf.l.wrap.b32 	%r678, %r675, %r676, 17;
	mov.b64 	%rd653, {%r678, %r677};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r679}, %rd649;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r680,%dummy}, %rd649;
	}
	shf.l.wrap.b32 	%r681, %r680, %r679, 25;
	shf.l.wrap.b32 	%r682, %r679, %r680, 25;
	mov.b64 	%rd654, {%r682, %r681};
	xor.b64  	%rd655, %rd653, %rd651;
	xor.b64  	%rd656, %rd654, %rd652;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd651, 32;
	shr.b64 	%rhs, %rd651, 32;
	add.u64 	%rd657, %lhs, %rhs;
	}
	add.s64 	%rd658, %rd652, %rd655;
	add.s64 	%rd659, %rd657, %rd656;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r683}, %rd655;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r684,%dummy}, %rd655;
	}
	shf.l.wrap.b32 	%r685, %r684, %r683, 13;
	shf.l.wrap.b32 	%r686, %r683, %r684, 13;
	mov.b64 	%rd660, {%r686, %r685};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r687}, %rd656;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r688,%dummy}, %rd656;
	}
	shf.l.wrap.b32 	%r689, %r688, %r687, 16;
	shf.l.wrap.b32 	%r690, %r687, %r688, 16;
	mov.b64 	%rd661, {%r690, %r689};
	xor.b64  	%rd662, %rd660, %rd658;
	xor.b64  	%rd663, %rd661, %rd659;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd658, 32;
	shr.b64 	%rhs, %rd658, 32;
	add.u64 	%rd664, %lhs, %rhs;
	}
	add.s64 	%rd665, %rd662, %rd659;
	add.s64 	%rd666, %rd663, %rd664;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r691}, %rd662;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r692,%dummy}, %rd662;
	}
	shf.l.wrap.b32 	%r693, %r692, %r691, 17;
	shf.l.wrap.b32 	%r694, %r691, %r692, 17;
	mov.b64 	%rd667, {%r694, %r693};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r695}, %rd663;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r696,%dummy}, %rd663;
	}
	shf.l.wrap.b32 	%r697, %r696, %r695, 25;
	shf.l.wrap.b32 	%r698, %r695, %r696, 25;
	mov.b64 	%rd668, {%r698, %r697};
	xor.b64  	%rd669, %rd667, %rd665;
	xor.b64  	%rd670, %rd668, %rd666;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd665, 32;
	shr.b64 	%rhs, %rd665, 32;
	add.u64 	%rd671, %lhs, %rhs;
	}
	add.s64 	%rd672, %rd666, %rd669;
	add.s64 	%rd673, %rd671, %rd670;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r699}, %rd669;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r700,%dummy}, %rd669;
	}
	shf.l.wrap.b32 	%r701, %r700, %r699, 13;
	shf.l.wrap.b32 	%r702, %r699, %r700, 13;
	mov.b64 	%rd674, {%r702, %r701};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r703}, %rd670;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r704,%dummy}, %rd670;
	}
	shf.l.wrap.b32 	%r705, %r704, %r703, 16;
	shf.l.wrap.b32 	%r706, %r703, %r704, 16;
	mov.b64 	%rd675, {%r706, %r705};
	xor.b64  	%rd676, %rd674, %rd672;
	xor.b64  	%rd677, %rd675, %rd673;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd672, 32;
	shr.b64 	%rhs, %rd672, 32;
	add.u64 	%rd678, %lhs, %rhs;
	}
	add.s64 	%rd679, %rd676, %rd673;
	add.s64 	%rd680, %rd677, %rd678;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r707}, %rd676;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r708,%dummy}, %rd676;
	}
	shf.l.wrap.b32 	%r709, %r708, %r707, 17;
	shf.l.wrap.b32 	%r710, %r707, %r708, 17;
	mov.b64 	%rd681, {%r710, %r709};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r711}, %rd677;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r712,%dummy}, %rd677;
	}
	shf.l.wrap.b32 	%r713, %r712, %r711, 25;
	shf.l.wrap.b32 	%r714, %r711, %r712, 25;
	mov.b64 	%rd682, {%r714, %r713};
	xor.b64  	%rd683, %rd681, %rd679;
	xor.b64  	%rd684, %rd682, %rd680;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd679, 32;
	shr.b64 	%rhs, %rd679, 32;
	add.u64 	%rd685, %lhs, %rhs;
	}
	add.s64 	%rd686, %rd680, %rd683;
	add.s64 	%rd687, %rd685, %rd684;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r715}, %rd683;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r716,%dummy}, %rd683;
	}
	shf.l.wrap.b32 	%r717, %r716, %r715, 13;
	shf.l.wrap.b32 	%r718, %r715, %r716, 13;
	mov.b64 	%rd688, {%r718, %r717};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r719}, %rd684;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r720,%dummy}, %rd684;
	}
	shf.l.wrap.b32 	%r721, %r720, %r719, 16;
	shf.l.wrap.b32 	%r722, %r719, %r720, 16;
	mov.b64 	%rd689, {%r722, %r721};
	xor.b64  	%rd690, %rd688, %rd686;
	xor.b64  	%rd691, %rd689, %rd687;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd686, 32;
	shr.b64 	%rhs, %rd686, 32;
	add.u64 	%rd692, %lhs, %rhs;
	}
	add.s64 	%rd693, %rd690, %rd687;
	add.s64 	%rd20, %rd691, %rd692;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r723}, %rd690;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r724,%dummy}, %rd690;
	}
	shf.l.wrap.b32 	%r725, %r724, %r723, 17;
	shf.l.wrap.b32 	%r726, %r723, %r724, 17;
	mov.b64 	%rd694, {%r726, %r725};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r727}, %rd691;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r728,%dummy}, %rd691;
	}
	shf.l.wrap.b32 	%r729, %r728, %r727, 25;
	shf.l.wrap.b32 	%r730, %r727, %r728, 25;
	mov.b64 	%rd695, {%r730, %r729};
	xor.b64  	%rd21, %rd694, %rd693;
	xor.b64  	%rd696, %rd695, %rd20;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd693, 32;
	shr.b64 	%rhs, %rd693, 32;
	add.u64 	%rd697, %lhs, %rhs;
	}
	xor.b64  	%rd22, %rd697, %rd696;
	add.s64 	%rd698, %rd5, 63;
	xor.b64  	%rd699, %rd696, %rd698;
	add.s64 	%rd700, %rd20, %rd21;
	add.s64 	%rd701, %rd697, %rd699;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r731}, %rd21;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r732,%dummy}, %rd21;
	}
	shf.l.wrap.b32 	%r733, %r732, %r731, 13;
	shf.l.wrap.b32 	%r734, %r731, %r732, 13;
	mov.b64 	%rd702, {%r734, %r733};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r735}, %rd699;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r736,%dummy}, %rd699;
	}
	shf.l.wrap.b32 	%r737, %r736, %r735, 16;
	shf.l.wrap.b32 	%r738, %r735, %r736, 16;
	mov.b64 	%rd703, {%r738, %r737};
	xor.b64  	%rd704, %rd702, %rd700;
	xor.b64  	%rd705, %rd703, %rd701;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd700, 32;
	shr.b64 	%rhs, %rd700, 32;
	add.u64 	%rd706, %lhs, %rhs;
	}
	add.s64 	%rd707, %rd704, %rd701;
	add.s64 	%rd708, %rd705, %rd706;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r739}, %rd704;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r740,%dummy}, %rd704;
	}
	shf.l.wrap.b32 	%r741, %r740, %r739, 17;
	shf.l.wrap.b32 	%r742, %r739, %r740, 17;
	mov.b64 	%rd709, {%r742, %r741};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r743}, %rd705;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r744,%dummy}, %rd705;
	}
	shf.l.wrap.b32 	%r745, %r744, %r743, 25;
	shf.l.wrap.b32 	%r746, %r743, %r744, 25;
	mov.b64 	%rd710, {%r746, %r745};
	xor.b64  	%rd711, %rd709, %rd707;
	xor.b64  	%rd712, %rd710, %rd708;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd707, 32;
	shr.b64 	%rhs, %rd707, 32;
	add.u64 	%rd713, %lhs, %rhs;
	}
	add.s64 	%rd714, %rd708, %rd711;
	add.s64 	%rd715, %rd713, %rd712;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r747}, %rd711;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r748,%dummy}, %rd711;
	}
	shf.l.wrap.b32 	%r749, %r748, %r747, 13;
	shf.l.wrap.b32 	%r750, %r747, %r748, 13;
	mov.b64 	%rd716, {%r750, %r749};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r751}, %rd712;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r752,%dummy}, %rd712;
	}
	shf.l.wrap.b32 	%r753, %r752, %r751, 16;
	shf.l.wrap.b32 	%r754, %r751, %r752, 16;
	mov.b64 	%rd717, {%r754, %r753};
	xor.b64  	%rd718, %rd716, %rd714;
	xor.b64  	%rd719, %rd717, %rd715;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd714, 32;
	shr.b64 	%rhs, %rd714, 32;
	add.u64 	%rd720, %lhs, %rhs;
	}
	add.s64 	%rd721, %rd718, %rd715;
	add.s64 	%rd722, %rd719, %rd720;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r755}, %rd718;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r756,%dummy}, %rd718;
	}
	shf.l.wrap.b32 	%r757, %r756, %r755, 17;
	shf.l.wrap.b32 	%r758, %r755, %r756, 17;
	mov.b64 	%rd723, {%r758, %r757};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r759}, %rd719;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r760,%dummy}, %rd719;
	}
	shf.l.wrap.b32 	%r761, %r760, %r759, 25;
	shf.l.wrap.b32 	%r762, %r759, %r760, 25;
	mov.b64 	%rd724, {%r762, %r761};
	xor.b64  	%rd725, %rd723, %rd721;
	xor.b64  	%rd726, %rd724, %rd722;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd721, 32;
	shr.b64 	%rhs, %rd721, 32;
	add.u64 	%rd727, %lhs, %rhs;
	}
	xor.b64  	%rd728, %rd722, %rd698;
	xor.b64  	%rd729, %rd727, 255;
	add.s64 	%rd730, %rd728, %rd725;
	add.s64 	%rd731, %rd729, %rd726;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r763}, %rd725;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r764,%dummy}, %rd725;
	}
	shf.l.wrap.b32 	%r765, %r764, %r763, 13;
	shf.l.wrap.b32 	%r766, %r763, %r764, 13;
	mov.b64 	%rd732, {%r766, %r765};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r767}, %rd726;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r768,%dummy}, %rd726;
	}
	shf.l.wrap.b32 	%r769, %r768, %r767, 16;
	shf.l.wrap.b32 	%r770, %r767, %r768, 16;
	mov.b64 	%rd733, {%r770, %r769};
	xor.b64  	%rd734, %rd732, %rd730;
	xor.b64  	%rd735, %rd733, %rd731;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd730, 32;
	shr.b64 	%rhs, %rd730, 32;
	add.u64 	%rd736, %lhs, %rhs;
	}
	add.s64 	%rd737, %rd734, %rd731;
	add.s64 	%rd738, %rd735, %rd736;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r771}, %rd734;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r772,%dummy}, %rd734;
	}
	shf.l.wrap.b32 	%r773, %r772, %r771, 17;
	shf.l.wrap.b32 	%r774, %r771, %r772, 17;
	mov.b64 	%rd739, {%r774, %r773};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r775}, %rd735;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r776,%dummy}, %rd735;
	}
	shf.l.wrap.b32 	%r777, %r776, %r775, 25;
	shf.l.wrap.b32 	%r778, %r775, %r776, 25;
	mov.b64 	%rd740, {%r778, %r777};
	xor.b64  	%rd741, %rd739, %rd737;
	xor.b64  	%rd742, %rd740, %rd738;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd737, 32;
	shr.b64 	%rhs, %rd737, 32;
	add.u64 	%rd743, %lhs, %rhs;
	}
	add.s64 	%rd744, %rd738, %rd741;
	add.s64 	%rd745, %rd743, %rd742;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r779}, %rd741;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r780,%dummy}, %rd741;
	}
	shf.l.wrap.b32 	%r781, %r780, %r779, 13;
	shf.l.wrap.b32 	%r782, %r779, %r780, 13;
	mov.b64 	%rd746, {%r782, %r781};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r783}, %rd742;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r784,%dummy}, %rd742;
	}
	shf.l.wrap.b32 	%r785, %r784, %r783, 16;
	shf.l.wrap.b32 	%r786, %r783, %r784, 16;
	mov.b64 	%rd747, {%r786, %r785};
	xor.b64  	%rd748, %rd746, %rd744;
	xor.b64  	%rd749, %rd747, %rd745;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd744, 32;
	shr.b64 	%rhs, %rd744, 32;
	add.u64 	%rd750, %lhs, %rhs;
	}
	add.s64 	%rd751, %rd748, %rd745;
	add.s64 	%rd752, %rd749, %rd750;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r787}, %rd748;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r788,%dummy}, %rd748;
	}
	shf.l.wrap.b32 	%r789, %r788, %r787, 17;
	shf.l.wrap.b32 	%r790, %r787, %r788, 17;
	mov.b64 	%rd753, {%r790, %r789};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r791}, %rd749;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r792,%dummy}, %rd749;
	}
	shf.l.wrap.b32 	%r793, %r792, %r791, 25;
	shf.l.wrap.b32 	%r794, %r791, %r792, 25;
	mov.b64 	%rd754, {%r794, %r793};
	xor.b64  	%rd755, %rd753, %rd751;
	xor.b64  	%rd756, %rd754, %rd752;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd751, 32;
	shr.b64 	%rhs, %rd751, 32;
	add.u64 	%rd757, %lhs, %rhs;
	}
	add.s64 	%rd758, %rd752, %rd755;
	add.s64 	%rd759, %rd757, %rd756;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r795}, %rd755;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r796,%dummy}, %rd755;
	}
	shf.l.wrap.b32 	%r797, %r796, %r795, 13;
	shf.l.wrap.b32 	%r798, %r795, %r796, 13;
	mov.b64 	%rd760, {%r798, %r797};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r799}, %rd756;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r800,%dummy}, %rd756;
	}
	shf.l.wrap.b32 	%r801, %r800, %r799, 16;
	shf.l.wrap.b32 	%r802, %r799, %r800, 16;
	mov.b64 	%rd761, {%r802, %r801};
	xor.b64  	%rd762, %rd760, %rd758;
	xor.b64  	%rd763, %rd761, %rd759;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd758, 32;
	shr.b64 	%rhs, %rd758, 32;
	add.u64 	%rd764, %lhs, %rhs;
	}
	add.s64 	%rd765, %rd762, %rd759;
	add.s64 	%rd766, %rd763, %rd764;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r803}, %rd762;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r804,%dummy}, %rd762;
	}
	shf.l.wrap.b32 	%r805, %r804, %r803, 17;
	shf.l.wrap.b32 	%r806, %r803, %r804, 17;
	mov.b64 	%rd767, {%r806, %r805};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r807}, %rd763;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r808,%dummy}, %rd763;
	}
	shf.l.wrap.b32 	%r809, %r808, %r807, 25;
	shf.l.wrap.b32 	%r810, %r807, %r808, 25;
	mov.b64 	%rd768, {%r810, %r809};
	xor.b64  	%rd769, %rd767, %rd765;
	xor.b64  	%rd770, %rd768, %rd766;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd765, 32;
	shr.b64 	%rhs, %rd765, 32;
	add.u64 	%rd771, %lhs, %rhs;
	}
	add.s64 	%rd772, %rd766, %rd769;
	add.s64 	%rd773, %rd771, %rd770;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r811}, %rd769;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r812,%dummy}, %rd769;
	}
	shf.l.wrap.b32 	%r813, %r812, %r811, 13;
	shf.l.wrap.b32 	%r814, %r811, %r812, 13;
	mov.b64 	%rd774, {%r814, %r813};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r815}, %rd770;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r816,%dummy}, %rd770;
	}
	shf.l.wrap.b32 	%r817, %r816, %r815, 16;
	shf.l.wrap.b32 	%r818, %r815, %r816, 16;
	mov.b64 	%rd775, {%r818, %r817};
	xor.b64  	%rd776, %rd774, %rd772;
	xor.b64  	%rd777, %rd775, %rd773;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd772, 32;
	shr.b64 	%rhs, %rd772, 32;
	add.u64 	%rd778, %lhs, %rhs;
	}
	add.s64 	%rd779, %rd776, %rd773;
	add.s64 	%rd780, %rd777, %rd778;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r819}, %rd776;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r820,%dummy}, %rd776;
	}
	shf.l.wrap.b32 	%r821, %r820, %r819, 17;
	shf.l.wrap.b32 	%r822, %r819, %r820, 17;
	mov.b64 	%rd781, {%r822, %r821};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r823}, %rd777;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r824,%dummy}, %rd777;
	}
	shf.l.wrap.b32 	%r825, %r824, %r823, 25;
	shf.l.wrap.b32 	%r826, %r823, %r824, 25;
	mov.b64 	%rd782, {%r826, %r825};
	xor.b64  	%rd783, %rd781, %rd779;
	xor.b64  	%rd784, %rd782, %rd780;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd779, 32;
	shr.b64 	%rhs, %rd779, 32;
	add.u64 	%rd785, %lhs, %rhs;
	}
	xor.b64  	%rd786, %rd785, %rd784;
	xor.b64  	%rd787, %rd786, %rd783;
	xor.b64  	%rd23, %rd787, %rd780;
	and.b64  	%rd788, %rd23, 268435455;
	shl.b64 	%rd789, %rd788, 1;
	or.b64  	%rd24, %rd789, 1;
	cvt.u32.u64	%r827, %rd789;
	or.b32  	%r12, %r827, 1;
	shr.u64 	%rd790, %rd23, 31;
	and.b64  	%rd791, %rd790, 536870910;
	or.b64  	%rd25, %rd791, 1;
	cvt.u32.u64	%r828, %rd790;
	and.b32  	%r829, %r828, 536870910;
	or.b32  	%r13, %r829, 1;
	shr.u32 	%r14, %r12, 17;
	shl.b32 	%r830, %r14, 3;
	add.s32 	%r15, %r46, %r830;
	atom.shared.exch.b64 	%rd26, [%r15], 0;
	setp.eq.s64	%p2, %rd26, 0;
	@%p2 bra 	BB0_5;
	bra.uni 	BB0_4;

BB0_5:
	shl.b64 	%rd797, %rd25, 32;
	or.b64  	%rd798, %rd797, %rd24;
	atom.shared.cas.b64 	%rd800, [%r15], %rd75, %rd798;
	setp.eq.s64	%p3, %rd800, 0;
	@%p3 bra 	BB0_7;

	shr.u32 	%r1213, %r12, 17;
	shr.u32 	%r845, %r12, 27;
	mul.lo.s32 	%r846, %r845, 140509184;
	mul.wide.s32 	%rd801, %r1213, 4;
	add.s64 	%rd802, %rd2, %rd801;
	atom.global.add.u32 	%r847, [%rd802], 2;
	mov.u32 	%r848, 34300;
	min.s32 	%r849, %r848, %r847;
	and.b32  	%r850, %r1213, 1023;
	mad.lo.s32 	%r851, %r850, 34304, %r846;
	add.s32 	%r852, %r851, %r849;
	shr.u32 	%r853, %r852, 31;
	add.s32 	%r854, %r852, %r853;
	shr.s32 	%r855, %r854, 1;
	mul.wide.s32 	%rd803, %r855, 16;
	add.s64 	%rd804, %rd1, %rd803;
	mov.u32 	%r856, 0;
	st.global.v4.u32 	[%rd804], {%r12, %r13, %r856, %r856};
	bra.uni 	BB0_7;

BB0_4:
	shr.u32 	%r1197, %r12, 17;
	shr.u32 	%r832, %r12, 27;
	mul.lo.s32 	%r833, %r832, 140509184;
	mul.wide.s32 	%rd792, %r1197, 4;
	add.s64 	%rd793, %rd2, %rd792;
	atom.global.add.u32 	%r834, [%rd793], 2;
	mov.u32 	%r835, 34300;
	min.s32 	%r836, %r835, %r834;
	and.b32  	%r837, %r1197, 1023;
	mad.lo.s32 	%r838, %r837, 34304, %r833;
	add.s32 	%r839, %r838, %r836;
	shr.u32 	%r840, %r839, 31;
	add.s32 	%r841, %r839, %r840;
	shr.s32 	%r842, %r841, 1;
	shr.u64 	%rd794, %rd26, 32;
	mul.wide.s32 	%rd795, %r842, 16;
	add.s64 	%rd796, %rd1, %rd795;
	cvt.u32.u64	%r843, %rd26;
	cvt.u32.u64	%r844, %rd794;
	st.global.v4.u32 	[%rd796], {%r843, %r844, %r12, %r13};

BB0_7:
	xor.b64  	%rd805, %rd16, %rd15;
	xor.b64  	%rd806, %rd805, %rd14;
	xor.b64  	%rd807, %rd806, %rd23;
	and.b64  	%rd27, %rd807, 268435455;
	shl.b64 	%rd28, %rd27, 1;
	cvt.u32.u64	%r16, %rd28;
	shr.u64 	%rd808, %rd807, 31;
	cvt.u32.u64	%r857, %rd808;
	and.b32  	%r17, %r857, 536870910;
	bfe.u64 	%rd29, %rd807, 16, 12;
	cvt.u32.u64	%r18, %rd29;
	shl.b32 	%r858, %r18, 3;
	add.s32 	%r19, %r46, %r858;
	atom.shared.exch.b64 	%rd30, [%r19], 0;
	setp.eq.s64	%p4, %rd30, 0;
	@%p4 bra 	BB0_9;
	bra.uni 	BB0_8;

BB0_9:
	cvt.u64.u32	%rd815, %r17;
	shl.b64 	%rd816, %rd815, 32;
	or.b64  	%rd817, %rd816, %rd28;
	atom.shared.cas.b64 	%rd819, [%r19], %rd75, %rd817;
	setp.eq.s64	%p5, %rd819, 0;
	@%p5 bra 	BB0_11;

	shr.u64 	%rd820, %rd27, 26;
	cvt.u32.u64	%r873, %rd820;
	mul.lo.s32 	%r874, %r873, 140509184;
	shl.b64 	%rd821, %rd29, 2;
	add.s64 	%rd822, %rd2, %rd821;
	atom.global.add.u32 	%r875, [%rd822], 2;
	mov.u32 	%r876, 34300;
	min.s32 	%r877, %r876, %r875;
	and.b32  	%r878, %r18, 1023;
	mad.lo.s32 	%r879, %r878, 34304, %r874;
	add.s32 	%r880, %r879, %r877;
	shr.u32 	%r881, %r880, 31;
	add.s32 	%r882, %r880, %r881;
	shr.s32 	%r883, %r882, 1;
	mul.wide.s32 	%rd823, %r883, 16;
	add.s64 	%rd824, %rd1, %rd823;
	mov.u32 	%r884, 0;
	st.global.v4.u32 	[%rd824], {%r16, %r17, %r884, %r884};
	bra.uni 	BB0_11;

BB0_8:
	shr.u64 	%rd809, %rd27, 26;
	cvt.u32.u64	%r860, %rd809;
	mul.lo.s32 	%r861, %r860, 140509184;
	shl.b64 	%rd810, %rd29, 2;
	add.s64 	%rd811, %rd2, %rd810;
	atom.global.add.u32 	%r862, [%rd811], 2;
	mov.u32 	%r863, 34300;
	min.s32 	%r864, %r863, %r862;
	and.b32  	%r865, %r18, 1023;
	mad.lo.s32 	%r866, %r865, 34304, %r861;
	add.s32 	%r867, %r866, %r864;
	shr.u32 	%r868, %r867, 31;
	add.s32 	%r869, %r867, %r868;
	shr.s32 	%r870, %r869, 1;
	shr.u64 	%rd812, %rd30, 32;
	mul.wide.s32 	%rd813, %r870, 16;
	add.s64 	%rd814, %rd1, %rd813;
	cvt.u32.u64	%r871, %rd30;
	cvt.u32.u64	%r872, %rd812;
	st.global.v4.u32 	[%rd814], {%r871, %r872, %r16, %r17};

BB0_11:
	xor.b64  	%rd825, %rd19, %rd18;
	xor.b64  	%rd826, %rd825, %rd17;
	xor.b64  	%rd827, %rd826, %rd23;
	and.b64  	%rd828, %rd827, 268435455;
	shl.b64 	%rd829, %rd828, 1;
	or.b64  	%rd31, %rd829, 1;
	cvt.u32.u64	%r885, %rd829;
	or.b32  	%r20, %r885, 1;
	shr.u64 	%rd830, %rd827, 31;
	and.b64  	%rd831, %rd830, 536870910;
	or.b64  	%rd32, %rd831, 1;
	cvt.u32.u64	%r886, %rd830;
	and.b32  	%r887, %r886, 536870910;
	or.b32  	%r21, %r887, 1;
	shr.u32 	%r22, %r20, 17;
	shl.b32 	%r888, %r22, 3;
	add.s32 	%r23, %r46, %r888;
	atom.shared.exch.b64 	%rd33, [%r23], 0;
	setp.eq.s64	%p6, %rd33, 0;
	@%p6 bra 	BB0_13;
	bra.uni 	BB0_12;

BB0_13:
	shl.b64 	%rd837, %rd32, 32;
	or.b64  	%rd838, %rd837, %rd31;
	atom.shared.cas.b64 	%rd840, [%r23], %rd75, %rd838;
	setp.eq.s64	%p7, %rd840, 0;
	@%p7 bra 	BB0_15;

	shr.u32 	%r903, %r20, 27;
	mul.lo.s32 	%r904, %r903, 140509184;
	mul.wide.s32 	%rd841, %r22, 4;
	add.s64 	%rd842, %rd2, %rd841;
	atom.global.add.u32 	%r905, [%rd842], 2;
	mov.u32 	%r906, 34300;
	min.s32 	%r907, %r906, %r905;
	and.b32  	%r908, %r22, 1023;
	mad.lo.s32 	%r909, %r908, 34304, %r904;
	add.s32 	%r910, %r909, %r907;
	shr.u32 	%r911, %r910, 31;
	add.s32 	%r912, %r910, %r911;
	shr.s32 	%r913, %r912, 1;
	mul.wide.s32 	%rd843, %r913, 16;
	add.s64 	%rd844, %rd1, %rd843;
	mov.u32 	%r914, 0;
	st.global.v4.u32 	[%rd844], {%r20, %r21, %r914, %r914};
	bra.uni 	BB0_15;

BB0_12:
	shr.u32 	%r890, %r20, 27;
	mul.lo.s32 	%r891, %r890, 140509184;
	mul.wide.s32 	%rd832, %r22, 4;
	add.s64 	%rd833, %rd2, %rd832;
	atom.global.add.u32 	%r892, [%rd833], 2;
	mov.u32 	%r893, 34300;
	min.s32 	%r894, %r893, %r892;
	and.b32  	%r895, %r22, 1023;
	mad.lo.s32 	%r896, %r895, 34304, %r891;
	add.s32 	%r897, %r896, %r894;
	shr.u32 	%r898, %r897, 31;
	add.s32 	%r899, %r897, %r898;
	shr.s32 	%r900, %r899, 1;
	shr.u64 	%rd834, %rd33, 32;
	mul.wide.s32 	%rd835, %r900, 16;
	add.s64 	%rd836, %rd1, %rd835;
	cvt.u32.u64	%r901, %rd33;
	cvt.u32.u64	%r902, %rd834;
	st.global.v4.u32 	[%rd836], {%r901, %r902, %r20, %r21};

BB0_15:
	xor.b64  	%rd845, %rd22, %rd21;
	xor.b64  	%rd846, %rd845, %rd20;
	xor.b64  	%rd847, %rd846, %rd23;
	and.b64  	%rd34, %rd847, 268435455;
	shl.b64 	%rd35, %rd34, 1;
	cvt.u32.u64	%r24, %rd35;
	shr.u64 	%rd848, %rd847, 31;
	cvt.u32.u64	%r915, %rd848;
	and.b32  	%r25, %r915, 536870910;
	bfe.u64 	%rd36, %rd847, 16, 12;
	cvt.u32.u64	%r26, %rd36;
	shl.b32 	%r916, %r26, 3;
	add.s32 	%r27, %r46, %r916;
	atom.shared.exch.b64 	%rd37, [%r27], 0;
	setp.eq.s64	%p8, %rd37, 0;
	@%p8 bra 	BB0_17;
	bra.uni 	BB0_16;

BB0_17:
	cvt.u64.u32	%rd855, %r25;
	shl.b64 	%rd856, %rd855, 32;
	or.b64  	%rd857, %rd856, %rd35;
	atom.shared.cas.b64 	%rd859, [%r27], %rd75, %rd857;
	setp.eq.s64	%p9, %rd859, 0;
	mov.u16 	%rs14, 15;
	@%p9 bra 	BB0_19;

	shr.u64 	%rd860, %rd34, 26;
	cvt.u32.u64	%r931, %rd860;
	mul.lo.s32 	%r932, %r931, 140509184;
	shl.b64 	%rd861, %rd36, 2;
	add.s64 	%rd862, %rd2, %rd861;
	atom.global.add.u32 	%r933, [%rd862], 2;
	mov.u32 	%r934, 34300;
	min.s32 	%r935, %r934, %r933;
	and.b32  	%r936, %r26, 1023;
	mad.lo.s32 	%r937, %r936, 34304, %r932;
	add.s32 	%r938, %r937, %r935;
	shr.u32 	%r939, %r938, 31;
	add.s32 	%r940, %r938, %r939;
	shr.s32 	%r941, %r940, 1;
	mul.wide.s32 	%rd863, %r941, 16;
	add.s64 	%rd864, %rd1, %rd863;
	mov.u32 	%r942, 0;
	st.global.v4.u32 	[%rd864], {%r24, %r25, %r942, %r942};
	bra.uni 	BB0_19;

BB0_16:
	shr.u64 	%rd849, %rd34, 26;
	cvt.u32.u64	%r918, %rd849;
	mul.lo.s32 	%r919, %r918, 140509184;
	shl.b64 	%rd850, %rd36, 2;
	add.s64 	%rd851, %rd2, %rd850;
	atom.global.add.u32 	%r920, [%rd851], 2;
	mov.u32 	%r921, 34300;
	min.s32 	%r922, %r921, %r920;
	and.b32  	%r923, %r26, 1023;
	mad.lo.s32 	%r924, %r923, 34304, %r919;
	add.s32 	%r925, %r924, %r922;
	shr.u32 	%r926, %r925, 31;
	add.s32 	%r927, %r925, %r926;
	shr.s32 	%r928, %r927, 1;
	shr.u64 	%rd852, %rd37, 32;
	mul.wide.s32 	%rd853, %r928, 16;
	add.s64 	%rd854, %rd1, %rd853;
	cvt.u32.u64	%r929, %rd37;
	cvt.u32.u64	%r930, %rd852;
	st.global.v4.u32 	[%rd854], {%r929, %r930, %r24, %r25};
	mov.u16 	%rs14, 15;

BB0_19:
	mov.u64 	%rd985, %rd4;

BB0_20:
	ld.local.v2.u64 	{%rd865, %rd866}, [%rd985+16];
	ld.local.v2.u64 	{%rd867, %rd868}, [%rd985];
	xor.b64  	%rd869, %rd867, %rd23;
	and.b64  	%rd46, %rd869, 268435455;
	shl.b64 	%rd47, %rd46, 1;
	cvt.u32.u64	%r28, %rd47;
	shr.u64 	%rd870, %rd869, 31;
	cvt.u32.u64	%r943, %rd870;
	and.b32  	%r29, %r943, 536870910;
	bfe.u64 	%rd48, %rd869, 16, 12;
	cvt.u32.u64	%r30, %rd48;
	shl.b32 	%r944, %r30, 3;
	add.s32 	%r31, %r46, %r944;
	atom.shared.exch.b64 	%rd49, [%r31], 0;
	setp.eq.s64	%p10, %rd49, 0;
	@%p10 bra 	BB0_22;
	bra.uni 	BB0_21;

BB0_22:
	cvt.u64.u32	%rd877, %r29;
	shl.b64 	%rd878, %rd877, 32;
	or.b64  	%rd879, %rd878, %rd47;
	atom.shared.cas.b64 	%rd881, [%r31], %rd75, %rd879;
	setp.eq.s64	%p11, %rd881, 0;
	@%p11 bra 	BB0_24;

	shr.u64 	%rd882, %rd46, 26;
	cvt.u32.u64	%r959, %rd882;
	mul.lo.s32 	%r960, %r959, 140509184;
	shl.b64 	%rd883, %rd48, 2;
	add.s64 	%rd884, %rd2, %rd883;
	atom.global.add.u32 	%r961, [%rd884], 2;
	mov.u32 	%r962, 34300;
	min.s32 	%r963, %r962, %r961;
	and.b32  	%r964, %r30, 1023;
	mad.lo.s32 	%r965, %r964, 34304, %r960;
	add.s32 	%r966, %r965, %r963;
	shr.u32 	%r967, %r966, 31;
	add.s32 	%r968, %r966, %r967;
	shr.s32 	%r969, %r968, 1;
	mul.wide.s32 	%rd885, %r969, 16;
	add.s64 	%rd886, %rd1, %rd885;
	mov.u32 	%r970, 0;
	st.global.v4.u32 	[%rd886], {%r28, %r29, %r970, %r970};
	bra.uni 	BB0_24;

BB0_21:
	shr.u64 	%rd871, %rd46, 26;
	cvt.u32.u64	%r946, %rd871;
	mul.lo.s32 	%r947, %r946, 140509184;
	shl.b64 	%rd872, %rd48, 2;
	add.s64 	%rd873, %rd2, %rd872;
	atom.global.add.u32 	%r948, [%rd873], 2;
	mov.u32 	%r949, 34300;
	min.s32 	%r950, %r949, %r948;
	and.b32  	%r951, %r30, 1023;
	mad.lo.s32 	%r952, %r951, 34304, %r947;
	add.s32 	%r953, %r952, %r950;
	shr.u32 	%r954, %r953, 31;
	add.s32 	%r955, %r953, %r954;
	shr.s32 	%r956, %r955, 1;
	shr.u64 	%rd874, %rd49, 32;
	mul.wide.s32 	%rd875, %r956, 16;
	add.s64 	%rd876, %rd1, %rd875;
	cvt.u32.u64	%r957, %rd49;
	cvt.u32.u64	%r958, %rd874;
	st.global.v4.u32 	[%rd876], {%r957, %r958, %r28, %r29};

BB0_24:
	xor.b64  	%rd887, %rd868, %rd23;
	and.b64  	%rd888, %rd887, 268435455;
	shl.b64 	%rd889, %rd888, 1;
	or.b64  	%rd50, %rd889, 1;
	cvt.u32.u64	%r971, %rd889;
	or.b32  	%r32, %r971, 1;
	shr.u64 	%rd890, %rd887, 31;
	and.b64  	%rd891, %rd890, 536870910;
	or.b64  	%rd51, %rd891, 1;
	cvt.u32.u64	%r972, %rd890;
	and.b32  	%r973, %r972, 536870910;
	or.b32  	%r33, %r973, 1;
	shr.u32 	%r34, %r32, 17;
	shl.b32 	%r974, %r34, 3;
	add.s32 	%r35, %r46, %r974;
	atom.shared.exch.b64 	%rd52, [%r35], 0;
	setp.eq.s64	%p12, %rd52, 0;
	@%p12 bra 	BB0_26;
	bra.uni 	BB0_25;

BB0_26:
	shl.b64 	%rd897, %rd51, 32;
	or.b64  	%rd898, %rd897, %rd50;
	atom.shared.cas.b64 	%rd900, [%r35], %rd75, %rd898;
	setp.eq.s64	%p13, %rd900, 0;
	@%p13 bra 	BB0_28;

	shr.u32 	%r989, %r32, 27;
	mul.lo.s32 	%r990, %r989, 140509184;
	mul.wide.s32 	%rd901, %r34, 4;
	add.s64 	%rd902, %rd2, %rd901;
	atom.global.add.u32 	%r991, [%rd902], 2;
	mov.u32 	%r992, 34300;
	min.s32 	%r993, %r992, %r991;
	and.b32  	%r994, %r34, 1023;
	mad.lo.s32 	%r995, %r994, 34304, %r990;
	add.s32 	%r996, %r995, %r993;
	shr.u32 	%r997, %r996, 31;
	add.s32 	%r998, %r996, %r997;
	shr.s32 	%r999, %r998, 1;
	mul.wide.s32 	%rd903, %r999, 16;
	add.s64 	%rd904, %rd1, %rd903;
	mov.u32 	%r1000, 0;
	st.global.v4.u32 	[%rd904], {%r32, %r33, %r1000, %r1000};
	bra.uni 	BB0_28;

BB0_25:
	shr.u32 	%r976, %r32, 27;
	mul.lo.s32 	%r977, %r976, 140509184;
	mul.wide.s32 	%rd892, %r34, 4;
	add.s64 	%rd893, %rd2, %rd892;
	atom.global.add.u32 	%r978, [%rd893], 2;
	mov.u32 	%r979, 34300;
	min.s32 	%r980, %r979, %r978;
	and.b32  	%r981, %r34, 1023;
	mad.lo.s32 	%r982, %r981, 34304, %r977;
	add.s32 	%r983, %r982, %r980;
	shr.u32 	%r984, %r983, 31;
	add.s32 	%r985, %r983, %r984;
	shr.s32 	%r986, %r985, 1;
	shr.u64 	%rd894, %rd52, 32;
	mul.wide.s32 	%rd895, %r986, 16;
	add.s64 	%rd896, %rd1, %rd895;
	cvt.u32.u64	%r987, %rd52;
	cvt.u32.u64	%r988, %rd894;
	st.global.v4.u32 	[%rd896], {%r987, %r988, %r32, %r33};

BB0_28:
	xor.b64  	%rd905, %rd865, %rd23;
	and.b64  	%rd53, %rd905, 268435455;
	shl.b64 	%rd54, %rd53, 1;
	cvt.u32.u64	%r36, %rd54;
	shr.u64 	%rd906, %rd905, 31;
	cvt.u32.u64	%r1001, %rd906;
	and.b32  	%r37, %r1001, 536870910;
	bfe.u64 	%rd55, %rd905, 16, 12;
	cvt.u32.u64	%r38, %rd55;
	shl.b32 	%r1002, %r38, 3;
	add.s32 	%r39, %r46, %r1002;
	atom.shared.exch.b64 	%rd56, [%r39], 0;
	setp.eq.s64	%p14, %rd56, 0;
	@%p14 bra 	BB0_30;
	bra.uni 	BB0_29;

BB0_30:
	cvt.u64.u32	%rd913, %r37;
	shl.b64 	%rd914, %rd913, 32;
	or.b64  	%rd915, %rd914, %rd54;
	atom.shared.cas.b64 	%rd917, [%r39], %rd75, %rd915;
	setp.eq.s64	%p15, %rd917, 0;
	@%p15 bra 	BB0_32;

	shr.u64 	%rd918, %rd53, 26;
	cvt.u32.u64	%r1017, %rd918;
	mul.lo.s32 	%r1018, %r1017, 140509184;
	shl.b64 	%rd919, %rd55, 2;
	add.s64 	%rd920, %rd2, %rd919;
	atom.global.add.u32 	%r1019, [%rd920], 2;
	mov.u32 	%r1020, 34300;
	min.s32 	%r1021, %r1020, %r1019;
	and.b32  	%r1022, %r38, 1023;
	mad.lo.s32 	%r1023, %r1022, 34304, %r1018;
	add.s32 	%r1024, %r1023, %r1021;
	shr.u32 	%r1025, %r1024, 31;
	add.s32 	%r1026, %r1024, %r1025;
	shr.s32 	%r1027, %r1026, 1;
	mul.wide.s32 	%rd921, %r1027, 16;
	add.s64 	%rd922, %rd1, %rd921;
	mov.u32 	%r1028, 0;
	st.global.v4.u32 	[%rd922], {%r36, %r37, %r1028, %r1028};
	bra.uni 	BB0_32;

BB0_29:
	shr.u64 	%rd907, %rd53, 26;
	cvt.u32.u64	%r1004, %rd907;
	mul.lo.s32 	%r1005, %r1004, 140509184;
	shl.b64 	%rd908, %rd55, 2;
	add.s64 	%rd909, %rd2, %rd908;
	atom.global.add.u32 	%r1006, [%rd909], 2;
	mov.u32 	%r1007, 34300;
	min.s32 	%r1008, %r1007, %r1006;
	and.b32  	%r1009, %r38, 1023;
	mad.lo.s32 	%r1010, %r1009, 34304, %r1005;
	add.s32 	%r1011, %r1010, %r1008;
	shr.u32 	%r1012, %r1011, 31;
	add.s32 	%r1013, %r1011, %r1012;
	shr.s32 	%r1014, %r1013, 1;
	shr.u64 	%rd910, %rd56, 32;
	mul.wide.s32 	%rd911, %r1014, 16;
	add.s64 	%rd912, %rd1, %rd911;
	cvt.u32.u64	%r1015, %rd56;
	cvt.u32.u64	%r1016, %rd910;
	st.global.v4.u32 	[%rd912], {%r1015, %r1016, %r36, %r37};

BB0_32:
	xor.b64  	%rd923, %rd866, %rd23;
	and.b64  	%rd924, %rd923, 268435455;
	shl.b64 	%rd925, %rd924, 1;
	or.b64  	%rd57, %rd925, 1;
	cvt.u32.u64	%r1029, %rd925;
	or.b32  	%r40, %r1029, 1;
	shr.u64 	%rd926, %rd923, 31;
	and.b64  	%rd927, %rd926, 536870910;
	or.b64  	%rd58, %rd927, 1;
	cvt.u32.u64	%r1030, %rd926;
	and.b32  	%r1031, %r1030, 536870910;
	or.b32  	%r41, %r1031, 1;
	shr.u32 	%r42, %r40, 17;
	shl.b32 	%r1032, %r42, 3;
	add.s32 	%r43, %r46, %r1032;
	atom.shared.exch.b64 	%rd59, [%r43], 0;
	setp.eq.s64	%p16, %rd59, 0;
	@%p16 bra 	BB0_34;
	bra.uni 	BB0_33;

BB0_34:
	shl.b64 	%rd933, %rd58, 32;
	or.b64  	%rd934, %rd933, %rd57;
	atom.shared.cas.b64 	%rd936, [%r43], %rd75, %rd934;
	setp.eq.s64	%p17, %rd936, 0;
	@%p17 bra 	BB0_36;

	shr.u32 	%r1047, %r40, 27;
	mul.lo.s32 	%r1048, %r1047, 140509184;
	mul.wide.s32 	%rd937, %r42, 4;
	add.s64 	%rd938, %rd2, %rd937;
	atom.global.add.u32 	%r1049, [%rd938], 2;
	mov.u32 	%r1050, 34300;
	min.s32 	%r1051, %r1050, %r1049;
	and.b32  	%r1052, %r42, 1023;
	mad.lo.s32 	%r1053, %r1052, 34304, %r1048;
	add.s32 	%r1054, %r1053, %r1051;
	shr.u32 	%r1055, %r1054, 31;
	add.s32 	%r1056, %r1054, %r1055;
	shr.s32 	%r1057, %r1056, 1;
	mul.wide.s32 	%rd939, %r1057, 16;
	add.s64 	%rd940, %rd1, %rd939;
	mov.u32 	%r1058, 0;
	st.global.v4.u32 	[%rd940], {%r40, %r41, %r1058, %r1058};
	bra.uni 	BB0_36;

BB0_33:
	shr.u32 	%r1034, %r40, 27;
	mul.lo.s32 	%r1035, %r1034, 140509184;
	mul.wide.s32 	%rd928, %r42, 4;
	add.s64 	%rd929, %rd2, %rd928;
	atom.global.add.u32 	%r1036, [%rd929], 2;
	mov.u32 	%r1037, 34300;
	min.s32 	%r1038, %r1037, %r1036;
	and.b32  	%r1039, %r42, 1023;
	mad.lo.s32 	%r1040, %r1039, 34304, %r1035;
	add.s32 	%r1041, %r1040, %r1038;
	shr.u32 	%r1042, %r1041, 31;
	add.s32 	%r1043, %r1041, %r1042;
	shr.s32 	%r1044, %r1043, 1;
	shr.u64 	%rd930, %rd59, 32;
	mul.wide.s32 	%rd931, %r1044, 16;
	add.s64 	%rd932, %rd1, %rd931;
	cvt.u32.u64	%r1045, %rd59;
	cvt.u32.u64	%r1046, %rd930;
	st.global.v4.u32 	[%rd932], {%r1045, %r1046, %r40, %r41};

BB0_36:
	add.s16 	%rs14, %rs14, -1;
	add.s64 	%rd985, %rd985, -32;
	setp.gt.s16	%p18, %rs14, 0;
	@%p18 bra 	BB0_20;

	cvt.u32.u16	%r1059, %rs12;
	add.s32 	%r1060, %r1059, 64;
	cvt.u16.u32	%rs12, %r1060;
	setp.lt.s16	%p19, %rs12, 256;
	@%p19 bra 	BB0_1;

	bar.sync 	0;
	ld.shared.u64 	%rd61, [%r2];
	setp.eq.s64	%p20, %rd61, 0;
	@%p20 bra 	BB0_40;

	mov.u32 	%r1198, %tid.x;
	shr.s32 	%r1061, %r1198, 31;
	shr.u32 	%r1062, %r1061, 22;
	add.s32 	%r1063, %r1198, %r1062;
	shr.s32 	%r1064, %r1063, 10;
	mul.wide.s32 	%rd941, %r1198, 4;
	add.s64 	%rd942, %rd2, %rd941;
	atom.global.add.u32 	%r1065, [%rd942], 2;
	mov.u32 	%r1066, 34300;
	min.s32 	%r1067, %r1066, %r1065;
	and.b32  	%r1068, %r1063, -1024;
	sub.s32 	%r1069, %r1198, %r1068;
	mad.lo.s32 	%r1070, %r1064, 140509184, %r1067;
	mad.lo.s32 	%r1071, %r1069, 34304, %r1070;
	shr.u32 	%r1072, %r1071, 31;
	add.s32 	%r1073, %r1071, %r1072;
	shr.s32 	%r1074, %r1073, 1;
	shr.u64 	%rd943, %rd61, 32;
	mul.wide.s32 	%rd944, %r1074, 16;
	add.s64 	%rd945, %rd1, %rd944;
	cvt.u32.u64	%r1075, %rd61;
	cvt.u32.u64	%r1076, %rd943;
	mov.u32 	%r1077, 0;
	st.global.v4.u32 	[%rd945], {%r1075, %r1076, %r1077, %r1077};

BB0_40:
	ld.shared.u64 	%rd62, [%r2+4096];
	setp.eq.s64	%p21, %rd62, 0;
	@%p21 bra 	BB0_42;

	mov.u32 	%r1200, %tid.x;
	add.s32 	%r1199, %r1200, 512;
	shr.s32 	%r1078, %r1199, 31;
	shr.u32 	%r1079, %r1078, 22;
	add.s32 	%r1080, %r1199, %r1079;
	shr.s32 	%r1081, %r1080, 10;
	mul.wide.s32 	%rd946, %r1199, 4;
	add.s64 	%rd947, %rd2, %rd946;
	atom.global.add.u32 	%r1082, [%rd947], 2;
	mov.u32 	%r1083, 34300;
	min.s32 	%r1084, %r1083, %r1082;
	and.b32  	%r1085, %r1080, -1024;
	sub.s32 	%r1086, %r1199, %r1085;
	mad.lo.s32 	%r1087, %r1081, 140509184, %r1084;
	mad.lo.s32 	%r1088, %r1086, 34304, %r1087;
	shr.u32 	%r1089, %r1088, 31;
	add.s32 	%r1090, %r1088, %r1089;
	shr.s32 	%r1091, %r1090, 1;
	shr.u64 	%rd948, %rd62, 32;
	mul.wide.s32 	%rd949, %r1091, 16;
	add.s64 	%rd950, %rd1, %rd949;
	cvt.u32.u64	%r1092, %rd62;
	cvt.u32.u64	%r1093, %rd948;
	mov.u32 	%r1094, 0;
	st.global.v4.u32 	[%rd950], {%r1092, %r1093, %r1094, %r1094};

BB0_42:
	ld.shared.u64 	%rd63, [%r2+8192];
	setp.eq.s64	%p22, %rd63, 0;
	@%p22 bra 	BB0_44;

	mov.u32 	%r1202, %tid.x;
	add.s32 	%r1201, %r1202, 1024;
	shr.s32 	%r1095, %r1201, 31;
	shr.u32 	%r1096, %r1095, 22;
	add.s32 	%r1097, %r1201, %r1096;
	shr.s32 	%r1098, %r1097, 10;
	mul.wide.s32 	%rd951, %r1201, 4;
	add.s64 	%rd952, %rd2, %rd951;
	atom.global.add.u32 	%r1099, [%rd952], 2;
	mov.u32 	%r1100, 34300;
	min.s32 	%r1101, %r1100, %r1099;
	and.b32  	%r1102, %r1097, -1024;
	sub.s32 	%r1103, %r1201, %r1102;
	mad.lo.s32 	%r1104, %r1098, 140509184, %r1101;
	mad.lo.s32 	%r1105, %r1103, 34304, %r1104;
	shr.u32 	%r1106, %r1105, 31;
	add.s32 	%r1107, %r1105, %r1106;
	shr.s32 	%r1108, %r1107, 1;
	shr.u64 	%rd953, %rd63, 32;
	mul.wide.s32 	%rd954, %r1108, 16;
	add.s64 	%rd955, %rd1, %rd954;
	cvt.u32.u64	%r1109, %rd63;
	cvt.u32.u64	%r1110, %rd953;
	mov.u32 	%r1111, 0;
	st.global.v4.u32 	[%rd955], {%r1109, %r1110, %r1111, %r1111};

BB0_44:
	ld.shared.u64 	%rd64, [%r2+12288];
	setp.eq.s64	%p23, %rd64, 0;
	@%p23 bra 	BB0_46;

	mov.u32 	%r1204, %tid.x;
	add.s32 	%r1203, %r1204, 1536;
	shr.s32 	%r1112, %r1203, 31;
	shr.u32 	%r1113, %r1112, 22;
	add.s32 	%r1114, %r1203, %r1113;
	shr.s32 	%r1115, %r1114, 10;
	mul.wide.s32 	%rd956, %r1203, 4;
	add.s64 	%rd957, %rd2, %rd956;
	atom.global.add.u32 	%r1116, [%rd957], 2;
	mov.u32 	%r1117, 34300;
	min.s32 	%r1118, %r1117, %r1116;
	and.b32  	%r1119, %r1114, -1024;
	sub.s32 	%r1120, %r1203, %r1119;
	mad.lo.s32 	%r1121, %r1115, 140509184, %r1118;
	mad.lo.s32 	%r1122, %r1120, 34304, %r1121;
	shr.u32 	%r1123, %r1122, 31;
	add.s32 	%r1124, %r1122, %r1123;
	shr.s32 	%r1125, %r1124, 1;
	shr.u64 	%rd958, %rd64, 32;
	mul.wide.s32 	%rd959, %r1125, 16;
	add.s64 	%rd960, %rd1, %rd959;
	cvt.u32.u64	%r1126, %rd64;
	cvt.u32.u64	%r1127, %rd958;
	mov.u32 	%r1128, 0;
	st.global.v4.u32 	[%rd960], {%r1126, %r1127, %r1128, %r1128};

BB0_46:
	ld.shared.u64 	%rd65, [%r2+16384];
	setp.eq.s64	%p24, %rd65, 0;
	@%p24 bra 	BB0_48;

	mov.u32 	%r1206, %tid.x;
	add.s32 	%r1205, %r1206, 2048;
	shr.s32 	%r1129, %r1205, 31;
	shr.u32 	%r1130, %r1129, 22;
	add.s32 	%r1131, %r1205, %r1130;
	shr.s32 	%r1132, %r1131, 10;
	mul.wide.s32 	%rd961, %r1205, 4;
	add.s64 	%rd962, %rd2, %rd961;
	atom.global.add.u32 	%r1133, [%rd962], 2;
	mov.u32 	%r1134, 34300;
	min.s32 	%r1135, %r1134, %r1133;
	and.b32  	%r1136, %r1131, -1024;
	sub.s32 	%r1137, %r1205, %r1136;
	mad.lo.s32 	%r1138, %r1132, 140509184, %r1135;
	mad.lo.s32 	%r1139, %r1137, 34304, %r1138;
	shr.u32 	%r1140, %r1139, 31;
	add.s32 	%r1141, %r1139, %r1140;
	shr.s32 	%r1142, %r1141, 1;
	shr.u64 	%rd963, %rd65, 32;
	mul.wide.s32 	%rd964, %r1142, 16;
	add.s64 	%rd965, %rd1, %rd964;
	cvt.u32.u64	%r1143, %rd65;
	cvt.u32.u64	%r1144, %rd963;
	mov.u32 	%r1145, 0;
	st.global.v4.u32 	[%rd965], {%r1143, %r1144, %r1145, %r1145};

BB0_48:
	ld.shared.u64 	%rd66, [%r2+20480];
	setp.eq.s64	%p25, %rd66, 0;
	@%p25 bra 	BB0_50;

	mov.u32 	%r1208, %tid.x;
	add.s32 	%r1207, %r1208, 2560;
	shr.s32 	%r1146, %r1207, 31;
	shr.u32 	%r1147, %r1146, 22;
	add.s32 	%r1148, %r1207, %r1147;
	shr.s32 	%r1149, %r1148, 10;
	mul.wide.s32 	%rd966, %r1207, 4;
	add.s64 	%rd967, %rd2, %rd966;
	atom.global.add.u32 	%r1150, [%rd967], 2;
	mov.u32 	%r1151, 34300;
	min.s32 	%r1152, %r1151, %r1150;
	and.b32  	%r1153, %r1148, -1024;
	sub.s32 	%r1154, %r1207, %r1153;
	mad.lo.s32 	%r1155, %r1149, 140509184, %r1152;
	mad.lo.s32 	%r1156, %r1154, 34304, %r1155;
	shr.u32 	%r1157, %r1156, 31;
	add.s32 	%r1158, %r1156, %r1157;
	shr.s32 	%r1159, %r1158, 1;
	shr.u64 	%rd968, %rd66, 32;
	mul.wide.s32 	%rd969, %r1159, 16;
	add.s64 	%rd970, %rd1, %rd969;
	cvt.u32.u64	%r1160, %rd66;
	cvt.u32.u64	%r1161, %rd968;
	mov.u32 	%r1162, 0;
	st.global.v4.u32 	[%rd970], {%r1160, %r1161, %r1162, %r1162};

BB0_50:
	ld.shared.u64 	%rd67, [%r2+24576];
	setp.eq.s64	%p26, %rd67, 0;
	@%p26 bra 	BB0_52;

	mov.u32 	%r1210, %tid.x;
	add.s32 	%r1209, %r1210, 3072;
	shr.s32 	%r1163, %r1209, 31;
	shr.u32 	%r1164, %r1163, 22;
	add.s32 	%r1165, %r1209, %r1164;
	shr.s32 	%r1166, %r1165, 10;
	mul.wide.s32 	%rd971, %r1209, 4;
	add.s64 	%rd972, %rd2, %rd971;
	atom.global.add.u32 	%r1167, [%rd972], 2;
	mov.u32 	%r1168, 34300;
	min.s32 	%r1169, %r1168, %r1167;
	and.b32  	%r1170, %r1165, -1024;
	sub.s32 	%r1171, %r1209, %r1170;
	mad.lo.s32 	%r1172, %r1166, 140509184, %r1169;
	mad.lo.s32 	%r1173, %r1171, 34304, %r1172;
	shr.u32 	%r1174, %r1173, 31;
	add.s32 	%r1175, %r1173, %r1174;
	shr.s32 	%r1176, %r1175, 1;
	shr.u64 	%rd973, %rd67, 32;
	mul.wide.s32 	%rd974, %r1176, 16;
	add.s64 	%rd975, %rd1, %rd974;
	cvt.u32.u64	%r1177, %rd67;
	cvt.u32.u64	%r1178, %rd973;
	mov.u32 	%r1179, 0;
	st.global.v4.u32 	[%rd975], {%r1177, %r1178, %r1179, %r1179};

BB0_52:
	ld.shared.u64 	%rd68, [%r2+28672];
	setp.eq.s64	%p27, %rd68, 0;
	@%p27 bra 	BB0_54;

	mov.u32 	%r1212, %tid.x;
	add.s32 	%r1211, %r1212, 3584;
	shr.s32 	%r1180, %r1211, 31;
	shr.u32 	%r1181, %r1180, 22;
	add.s32 	%r1182, %r1211, %r1181;
	shr.s32 	%r1183, %r1182, 10;
	mul.wide.s32 	%rd976, %r1211, 4;
	add.s64 	%rd977, %rd2, %rd976;
	atom.global.add.u32 	%r1184, [%rd977], 2;
	mov.u32 	%r1185, 34300;
	min.s32 	%r1186, %r1185, %r1184;
	and.b32  	%r1187, %r1182, -1024;
	sub.s32 	%r1188, %r1211, %r1187;
	mad.lo.s32 	%r1189, %r1183, 140509184, %r1186;
	mad.lo.s32 	%r1190, %r1188, 34304, %r1189;
	shr.u32 	%r1191, %r1190, 31;
	add.s32 	%r1192, %r1190, %r1191;
	shr.s32 	%r1193, %r1192, 1;
	shr.u64 	%rd978, %rd68, 32;
	mul.wide.s32 	%rd979, %r1193, 16;
	add.s64 	%rd980, %rd1, %rd979;
	cvt.u32.u64	%r1194, %rd68;
	cvt.u32.u64	%r1195, %rd978;
	mov.u32 	%r1196, 0;
	st.global.v4.u32 	[%rd980], {%r1194, %r1195, %r1196, %r1196};

BB0_54:
	ret;
}

	// .globl	FluffyRound_A1
.visible .entry FluffyRound_A1(
	.param .u64 FluffyRound_A1_param_0,
	.param .u64 FluffyRound_A1_param_1,
	.param .u64 FluffyRound_A1_param_2,
	.param .u64 FluffyRound_A1_param_3,
	.param .u32 FluffyRound_A1_param_4,
	.param .u32 FluffyRound_A1_param_5,
	.param .u32 FluffyRound_A1_param_6
)
{
	.reg .pred 	%p<249>;
	.reg .b32 	%r<1842>;
	.reg .b64 	%rd<574>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_A1$__cuda_local_var_207252_30_non_const_ecounters[49152];

	ld.param.u64 	%rd36, [FluffyRound_A1_param_0];
	ld.param.u64 	%rd37, [FluffyRound_A1_param_1];
	ld.param.u64 	%rd38, [FluffyRound_A1_param_2];
	ld.param.u64 	%rd39, [FluffyRound_A1_param_3];
	ld.param.u32 	%r299, [FluffyRound_A1_param_5];
	ld.param.u32 	%r300, [FluffyRound_A1_param_6];
	cvta.to.global.u64 	%rd1, %rd37;
	cvta.to.global.u64 	%rd2, %rd36;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r301, %r1, 2;
	mov.u32 	%r302, FluffyRound_A1$__cuda_local_var_207252_30_non_const_ecounters;
	add.s32 	%r303, %r302, %r301;
	mov.u32 	%r304, 0;
	st.shared.u32 	[%r303], %r304;
	add.s32 	%r2, %r1, 1024;
	st.shared.u32 	[%r303+4096], %r304;
	add.s32 	%r3, %r1, 2048;
	st.shared.u32 	[%r303+8192], %r304;
	add.s32 	%r4, %r1, 3072;
	st.shared.u32 	[%r303+12288], %r304;
	st.shared.u32 	[%r303+16384], %r304;
	st.shared.u32 	[%r303+20480], %r304;
	st.shared.u32 	[%r303+24576], %r304;
	st.shared.u32 	[%r303+28672], %r304;
	cvta.to.global.u64 	%rd3, %rd39;
	mov.u32 	%r305, %ctaid.x;
	add.s32 	%r306, %r305, %r300;
	cvta.to.global.u64 	%rd40, %rd38;
	mul.wide.s32 	%rd41, %r306, 4;
	add.s64 	%rd42, %rd40, %rd41;
	ld.global.u32 	%r307, [%rd42];
	mov.u32 	%r308, 34300;
	min.s32 	%r5, %r307, %r308;
	add.s32 	%r6, %r5, 1024;
	shr.s32 	%r309, %r6, 31;
	shr.u32 	%r310, %r309, 22;
	add.s32 	%r311, %r6, %r310;
	shr.s32 	%r7, %r311, 10;
	add.s32 	%r312, %r306, 4096;
	mul.wide.s32 	%rd43, %r312, 4;
	add.s64 	%rd44, %rd40, %rd43;
	ld.global.u32 	%r313, [%rd44];
	min.s32 	%r8, %r313, %r308;
	add.s32 	%r9, %r8, 1024;
	shr.s32 	%r314, %r9, 31;
	shr.u32 	%r315, %r314, 22;
	add.s32 	%r316, %r9, %r315;
	shr.s32 	%r10, %r316, 10;
	add.s32 	%r317, %r306, 8192;
	mul.wide.s32 	%rd45, %r317, 4;
	add.s64 	%rd46, %rd40, %rd45;
	ld.global.u32 	%r318, [%rd46];
	min.s32 	%r11, %r318, %r308;
	add.s32 	%r12, %r11, 1024;
	shr.s32 	%r319, %r12, 31;
	shr.u32 	%r320, %r319, 22;
	add.s32 	%r321, %r12, %r320;
	shr.s32 	%r13, %r321, 10;
	add.s32 	%r322, %r306, 12288;
	mul.wide.s32 	%rd47, %r322, 4;
	add.s64 	%rd48, %rd40, %rd47;
	ld.global.u32 	%r323, [%rd48];
	min.s32 	%r14, %r323, %r308;
	add.s32 	%r15, %r14, 1024;
	shr.s32 	%r324, %r15, 31;
	shr.u32 	%r325, %r324, 22;
	add.s32 	%r326, %r15, %r325;
	shr.s32 	%r16, %r326, 10;
	shl.b32 	%r17, %r300, 2;
	add.s32 	%r327, %r305, %r17;
	mul.lo.s32 	%r18, %r327, 34304;
	add.s32 	%r19, %r18, 35127296;
	add.s32 	%r20, %r18, 70254592;
	add.s32 	%r21, %r18, 105381888;
	bar.sync 	0;
	setp.lt.s32	%p2, %r6, 1024;
	@%p2 bra 	BB1_31;

	mov.u32 	%r332, 1;
	max.s32 	%r22, %r7, %r332;
	and.b32  	%r331, %r22, 3;
	mov.u32 	%r1802, 0;
	setp.eq.s32	%p3, %r331, 0;
	@%p3 bra 	BB1_16;

	setp.eq.s32	%p4, %r331, 1;
	@%p4 bra 	BB1_12;

	setp.eq.s32	%p5, %r331, 2;
	@%p5 bra 	BB1_8;

	setp.ge.s32	%p6, %r1, %r5;
	@%p6 bra 	BB1_5;

	add.s32 	%r335, %r1, %r18;
	mul.wide.s32 	%rd49, %r335, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.v2.u32 	{%r336, %r337}, [%rd50];
	or.b32  	%r339, %r336, %r337;
	setp.eq.s32	%p7, %r339, 0;
	mov.u32 	%r1802, %r332;
	@%p7 bra 	BB1_8;

	and.b32  	%r341, %r336, 131040;
	and.b32  	%r342, %r336, 31;
	mov.u32 	%r1802, 1;
	shl.b32 	%r343, %r1802, %r342;
	shr.u32 	%r344, %r341, 3;
	add.s32 	%r346, %r302, %r344;
	atom.shared.or.b32 	%r347, [%r346], %r343;
	bra.uni 	BB1_8;

BB1_5:
	mov.u32 	%r1802, %r332;

BB1_8:
	shl.b32 	%r348, %r1802, 10;
	add.s32 	%r25, %r348, %r1;
	setp.ge.s32	%p8, %r25, %r5;
	@%p8 bra 	BB1_11;

	add.s32 	%r349, %r25, %r18;
	mul.wide.s32 	%rd51, %r349, 8;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.v2.u32 	{%r350, %r351}, [%rd52];
	or.b32  	%r353, %r350, %r351;
	setp.eq.s32	%p9, %r353, 0;
	@%p9 bra 	BB1_11;

	and.b32  	%r354, %r350, 131040;
	and.b32  	%r355, %r350, 31;
	mov.u32 	%r356, 1;
	shl.b32 	%r357, %r356, %r355;
	shr.u32 	%r358, %r354, 3;
	add.s32 	%r360, %r302, %r358;
	atom.shared.or.b32 	%r361, [%r360], %r357;

BB1_11:
	add.s32 	%r1802, %r1802, 1;

BB1_12:
	shl.b32 	%r362, %r1802, 10;
	add.s32 	%r29, %r362, %r1;
	setp.ge.s32	%p10, %r29, %r5;
	@%p10 bra 	BB1_15;

	add.s32 	%r363, %r29, %r18;
	mul.wide.s32 	%rd53, %r363, 8;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.v2.u32 	{%r364, %r365}, [%rd54];
	or.b32  	%r367, %r364, %r365;
	setp.eq.s32	%p11, %r367, 0;
	@%p11 bra 	BB1_15;

	and.b32  	%r368, %r364, 131040;
	and.b32  	%r369, %r364, 31;
	mov.u32 	%r370, 1;
	shl.b32 	%r371, %r370, %r369;
	shr.u32 	%r372, %r368, 3;
	add.s32 	%r374, %r302, %r372;
	atom.shared.or.b32 	%r375, [%r374], %r371;

BB1_15:
	add.s32 	%r1802, %r1802, 1;

BB1_16:
	setp.lt.u32	%p12, %r22, 4;
	@%p12 bra 	BB1_31;

	mad.lo.s32 	%r1805, %r1802, 1024, %r1;

BB1_18:
	setp.ge.s32	%p13, %r1805, %r5;
	@%p13 bra 	BB1_21;

	add.s32 	%r376, %r1805, %r18;
	mul.wide.s32 	%rd55, %r376, 8;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.v2.u32 	{%r377, %r378}, [%rd56];
	or.b32  	%r380, %r377, %r378;
	setp.eq.s32	%p14, %r380, 0;
	@%p14 bra 	BB1_21;

	and.b32  	%r381, %r377, 131040;
	and.b32  	%r382, %r377, 31;
	mov.u32 	%r383, 1;
	shl.b32 	%r384, %r383, %r382;
	shr.u32 	%r385, %r381, 3;
	add.s32 	%r387, %r302, %r385;
	atom.shared.or.b32 	%r388, [%r387], %r384;

BB1_21:
	add.s32 	%r37, %r1805, 1024;
	setp.ge.s32	%p15, %r37, %r5;
	@%p15 bra 	BB1_24;

	add.s32 	%r389, %r37, %r18;
	mul.wide.s32 	%rd57, %r389, 8;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.v2.u32 	{%r390, %r391}, [%rd58];
	or.b32  	%r393, %r390, %r391;
	setp.eq.s32	%p16, %r393, 0;
	@%p16 bra 	BB1_24;

	and.b32  	%r394, %r390, 131040;
	and.b32  	%r395, %r390, 31;
	mov.u32 	%r396, 1;
	shl.b32 	%r397, %r396, %r395;
	shr.u32 	%r398, %r394, 3;
	add.s32 	%r400, %r302, %r398;
	atom.shared.or.b32 	%r401, [%r400], %r397;

BB1_24:
	add.s32 	%r39, %r1805, 2048;
	setp.ge.s32	%p17, %r39, %r5;
	@%p17 bra 	BB1_27;

	add.s32 	%r402, %r39, %r18;
	mul.wide.s32 	%rd59, %r402, 8;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.v2.u32 	{%r403, %r404}, [%rd60];
	or.b32  	%r406, %r403, %r404;
	setp.eq.s32	%p18, %r406, 0;
	@%p18 bra 	BB1_27;

	and.b32  	%r407, %r403, 131040;
	and.b32  	%r408, %r403, 31;
	mov.u32 	%r409, 1;
	shl.b32 	%r410, %r409, %r408;
	shr.u32 	%r411, %r407, 3;
	add.s32 	%r413, %r302, %r411;
	atom.shared.or.b32 	%r414, [%r413], %r410;

BB1_27:
	add.s32 	%r41, %r1805, 3072;
	setp.ge.s32	%p19, %r41, %r5;
	@%p19 bra 	BB1_30;

	add.s32 	%r415, %r41, %r18;
	mul.wide.s32 	%rd61, %r415, 8;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.v2.u32 	{%r416, %r417}, [%rd62];
	or.b32  	%r419, %r416, %r417;
	setp.eq.s32	%p20, %r419, 0;
	@%p20 bra 	BB1_30;

	and.b32  	%r420, %r416, 131040;
	and.b32  	%r421, %r416, 31;
	mov.u32 	%r422, 1;
	shl.b32 	%r423, %r422, %r421;
	shr.u32 	%r424, %r420, 3;
	add.s32 	%r426, %r302, %r424;
	atom.shared.or.b32 	%r427, [%r426], %r423;

BB1_30:
	add.s32 	%r1802, %r1802, 4;
	add.s32 	%r1805, %r1805, 4096;
	setp.lt.s32	%p21, %r1802, %r7;
	@%p21 bra 	BB1_18;

BB1_31:
	setp.lt.s32	%p22, %r9, 1024;
	@%p22 bra 	BB1_62;

	mov.u32 	%r432, 1;
	max.s32 	%r45, %r10, %r432;
	and.b32  	%r431, %r45, 3;
	mov.u32 	%r1807, 0;
	setp.eq.s32	%p23, %r431, 0;
	@%p23 bra 	BB1_47;

	setp.eq.s32	%p24, %r431, 1;
	@%p24 bra 	BB1_43;

	setp.eq.s32	%p25, %r431, 2;
	@%p25 bra 	BB1_39;

	setp.ge.s32	%p26, %r1, %r8;
	@%p26 bra 	BB1_36;

	add.s32 	%r435, %r1, %r19;
	mul.wide.s32 	%rd63, %r435, 8;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.v2.u32 	{%r436, %r437}, [%rd64];
	or.b32  	%r439, %r436, %r437;
	setp.eq.s32	%p27, %r439, 0;
	mov.u32 	%r1807, %r432;
	@%p27 bra 	BB1_39;

	and.b32  	%r441, %r436, 131040;
	and.b32  	%r442, %r436, 31;
	mov.u32 	%r1807, 1;
	shl.b32 	%r443, %r1807, %r442;
	shr.u32 	%r444, %r441, 3;
	add.s32 	%r446, %r302, %r444;
	atom.shared.or.b32 	%r447, [%r446], %r443;
	bra.uni 	BB1_39;

BB1_36:
	mov.u32 	%r1807, %r432;

BB1_39:
	shl.b32 	%r448, %r1807, 10;
	add.s32 	%r48, %r448, %r1;
	setp.ge.s32	%p28, %r48, %r8;
	@%p28 bra 	BB1_42;

	add.s32 	%r449, %r48, %r19;
	mul.wide.s32 	%rd65, %r449, 8;
	add.s64 	%rd66, %rd2, %rd65;
	ld.global.v2.u32 	{%r450, %r451}, [%rd66];
	or.b32  	%r453, %r450, %r451;
	setp.eq.s32	%p29, %r453, 0;
	@%p29 bra 	BB1_42;

	and.b32  	%r454, %r450, 131040;
	and.b32  	%r455, %r450, 31;
	mov.u32 	%r456, 1;
	shl.b32 	%r457, %r456, %r455;
	shr.u32 	%r458, %r454, 3;
	add.s32 	%r460, %r302, %r458;
	atom.shared.or.b32 	%r461, [%r460], %r457;

BB1_42:
	add.s32 	%r1807, %r1807, 1;

BB1_43:
	shl.b32 	%r462, %r1807, 10;
	add.s32 	%r52, %r462, %r1;
	setp.ge.s32	%p30, %r52, %r8;
	@%p30 bra 	BB1_46;

	add.s32 	%r463, %r52, %r19;
	mul.wide.s32 	%rd67, %r463, 8;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.v2.u32 	{%r464, %r465}, [%rd68];
	or.b32  	%r467, %r464, %r465;
	setp.eq.s32	%p31, %r467, 0;
	@%p31 bra 	BB1_46;

	and.b32  	%r468, %r464, 131040;
	and.b32  	%r469, %r464, 31;
	mov.u32 	%r470, 1;
	shl.b32 	%r471, %r470, %r469;
	shr.u32 	%r472, %r468, 3;
	add.s32 	%r474, %r302, %r472;
	atom.shared.or.b32 	%r475, [%r474], %r471;

BB1_46:
	add.s32 	%r1807, %r1807, 1;

BB1_47:
	setp.lt.u32	%p32, %r45, 4;
	@%p32 bra 	BB1_62;

	mad.lo.s32 	%r1810, %r1807, 1024, %r1;

BB1_49:
	setp.ge.s32	%p33, %r1810, %r8;
	@%p33 bra 	BB1_52;

	add.s32 	%r476, %r1810, %r19;
	mul.wide.s32 	%rd69, %r476, 8;
	add.s64 	%rd70, %rd2, %rd69;
	ld.global.v2.u32 	{%r477, %r478}, [%rd70];
	or.b32  	%r480, %r477, %r478;
	setp.eq.s32	%p34, %r480, 0;
	@%p34 bra 	BB1_52;

	and.b32  	%r481, %r477, 131040;
	and.b32  	%r482, %r477, 31;
	mov.u32 	%r483, 1;
	shl.b32 	%r484, %r483, %r482;
	shr.u32 	%r485, %r481, 3;
	add.s32 	%r487, %r302, %r485;
	atom.shared.or.b32 	%r488, [%r487], %r484;

BB1_52:
	add.s32 	%r60, %r1810, 1024;
	setp.ge.s32	%p35, %r60, %r8;
	@%p35 bra 	BB1_55;

	add.s32 	%r489, %r60, %r19;
	mul.wide.s32 	%rd71, %r489, 8;
	add.s64 	%rd72, %rd2, %rd71;
	ld.global.v2.u32 	{%r490, %r491}, [%rd72];
	or.b32  	%r493, %r490, %r491;
	setp.eq.s32	%p36, %r493, 0;
	@%p36 bra 	BB1_55;

	and.b32  	%r494, %r490, 131040;
	and.b32  	%r495, %r490, 31;
	mov.u32 	%r496, 1;
	shl.b32 	%r497, %r496, %r495;
	shr.u32 	%r498, %r494, 3;
	add.s32 	%r500, %r302, %r498;
	atom.shared.or.b32 	%r501, [%r500], %r497;

BB1_55:
	add.s32 	%r62, %r1810, 2048;
	setp.ge.s32	%p37, %r62, %r8;
	@%p37 bra 	BB1_58;

	add.s32 	%r502, %r62, %r19;
	mul.wide.s32 	%rd73, %r502, 8;
	add.s64 	%rd74, %rd2, %rd73;
	ld.global.v2.u32 	{%r503, %r504}, [%rd74];
	or.b32  	%r506, %r503, %r504;
	setp.eq.s32	%p38, %r506, 0;
	@%p38 bra 	BB1_58;

	and.b32  	%r507, %r503, 131040;
	and.b32  	%r508, %r503, 31;
	mov.u32 	%r509, 1;
	shl.b32 	%r510, %r509, %r508;
	shr.u32 	%r511, %r507, 3;
	add.s32 	%r513, %r302, %r511;
	atom.shared.or.b32 	%r514, [%r513], %r510;

BB1_58:
	add.s32 	%r64, %r1810, 3072;
	setp.ge.s32	%p39, %r64, %r8;
	@%p39 bra 	BB1_61;

	add.s32 	%r515, %r64, %r19;
	mul.wide.s32 	%rd75, %r515, 8;
	add.s64 	%rd76, %rd2, %rd75;
	ld.global.v2.u32 	{%r516, %r517}, [%rd76];
	or.b32  	%r519, %r516, %r517;
	setp.eq.s32	%p40, %r519, 0;
	@%p40 bra 	BB1_61;

	and.b32  	%r520, %r516, 131040;
	and.b32  	%r521, %r516, 31;
	mov.u32 	%r522, 1;
	shl.b32 	%r523, %r522, %r521;
	shr.u32 	%r524, %r520, 3;
	add.s32 	%r526, %r302, %r524;
	atom.shared.or.b32 	%r527, [%r526], %r523;

BB1_61:
	add.s32 	%r1807, %r1807, 4;
	add.s32 	%r1810, %r1810, 4096;
	setp.lt.s32	%p41, %r1807, %r10;
	@%p41 bra 	BB1_49;

BB1_62:
	setp.lt.s32	%p42, %r12, 1024;
	@%p42 bra 	BB1_93;

	mov.u32 	%r532, 1;
	max.s32 	%r68, %r13, %r532;
	and.b32  	%r531, %r68, 3;
	mov.u32 	%r1812, 0;
	setp.eq.s32	%p43, %r531, 0;
	@%p43 bra 	BB1_78;

	setp.eq.s32	%p44, %r531, 1;
	@%p44 bra 	BB1_74;

	setp.eq.s32	%p45, %r531, 2;
	@%p45 bra 	BB1_70;

	setp.ge.s32	%p46, %r1, %r11;
	@%p46 bra 	BB1_67;

	add.s32 	%r535, %r1, %r20;
	mul.wide.s32 	%rd77, %r535, 8;
	add.s64 	%rd78, %rd2, %rd77;
	ld.global.v2.u32 	{%r536, %r537}, [%rd78];
	or.b32  	%r539, %r536, %r537;
	setp.eq.s32	%p47, %r539, 0;
	mov.u32 	%r1812, %r532;
	@%p47 bra 	BB1_70;

	and.b32  	%r541, %r536, 131040;
	and.b32  	%r542, %r536, 31;
	mov.u32 	%r1812, 1;
	shl.b32 	%r543, %r1812, %r542;
	shr.u32 	%r544, %r541, 3;
	add.s32 	%r546, %r302, %r544;
	atom.shared.or.b32 	%r547, [%r546], %r543;
	bra.uni 	BB1_70;

BB1_67:
	mov.u32 	%r1812, %r532;

BB1_70:
	shl.b32 	%r548, %r1812, 10;
	add.s32 	%r71, %r548, %r1;
	setp.ge.s32	%p48, %r71, %r11;
	@%p48 bra 	BB1_73;

	add.s32 	%r549, %r71, %r20;
	mul.wide.s32 	%rd79, %r549, 8;
	add.s64 	%rd80, %rd2, %rd79;
	ld.global.v2.u32 	{%r550, %r551}, [%rd80];
	or.b32  	%r553, %r550, %r551;
	setp.eq.s32	%p49, %r553, 0;
	@%p49 bra 	BB1_73;

	and.b32  	%r554, %r550, 131040;
	and.b32  	%r555, %r550, 31;
	mov.u32 	%r556, 1;
	shl.b32 	%r557, %r556, %r555;
	shr.u32 	%r558, %r554, 3;
	add.s32 	%r560, %r302, %r558;
	atom.shared.or.b32 	%r561, [%r560], %r557;

BB1_73:
	add.s32 	%r1812, %r1812, 1;

BB1_74:
	shl.b32 	%r562, %r1812, 10;
	add.s32 	%r75, %r562, %r1;
	setp.ge.s32	%p50, %r75, %r11;
	@%p50 bra 	BB1_77;

	add.s32 	%r563, %r75, %r20;
	mul.wide.s32 	%rd81, %r563, 8;
	add.s64 	%rd82, %rd2, %rd81;
	ld.global.v2.u32 	{%r564, %r565}, [%rd82];
	or.b32  	%r567, %r564, %r565;
	setp.eq.s32	%p51, %r567, 0;
	@%p51 bra 	BB1_77;

	and.b32  	%r568, %r564, 131040;
	and.b32  	%r569, %r564, 31;
	mov.u32 	%r570, 1;
	shl.b32 	%r571, %r570, %r569;
	shr.u32 	%r572, %r568, 3;
	add.s32 	%r574, %r302, %r572;
	atom.shared.or.b32 	%r575, [%r574], %r571;

BB1_77:
	add.s32 	%r1812, %r1812, 1;

BB1_78:
	setp.lt.u32	%p52, %r68, 4;
	@%p52 bra 	BB1_93;

	mad.lo.s32 	%r1815, %r1812, 1024, %r1;

BB1_80:
	setp.ge.s32	%p53, %r1815, %r11;
	@%p53 bra 	BB1_83;

	add.s32 	%r576, %r1815, %r20;
	mul.wide.s32 	%rd83, %r576, 8;
	add.s64 	%rd84, %rd2, %rd83;
	ld.global.v2.u32 	{%r577, %r578}, [%rd84];
	or.b32  	%r580, %r577, %r578;
	setp.eq.s32	%p54, %r580, 0;
	@%p54 bra 	BB1_83;

	and.b32  	%r581, %r577, 131040;
	and.b32  	%r582, %r577, 31;
	mov.u32 	%r583, 1;
	shl.b32 	%r584, %r583, %r582;
	shr.u32 	%r585, %r581, 3;
	add.s32 	%r587, %r302, %r585;
	atom.shared.or.b32 	%r588, [%r587], %r584;

BB1_83:
	add.s32 	%r83, %r1815, 1024;
	setp.ge.s32	%p55, %r83, %r11;
	@%p55 bra 	BB1_86;

	add.s32 	%r589, %r83, %r20;
	mul.wide.s32 	%rd85, %r589, 8;
	add.s64 	%rd86, %rd2, %rd85;
	ld.global.v2.u32 	{%r590, %r591}, [%rd86];
	or.b32  	%r593, %r590, %r591;
	setp.eq.s32	%p56, %r593, 0;
	@%p56 bra 	BB1_86;

	and.b32  	%r594, %r590, 131040;
	and.b32  	%r595, %r590, 31;
	mov.u32 	%r596, 1;
	shl.b32 	%r597, %r596, %r595;
	shr.u32 	%r598, %r594, 3;
	add.s32 	%r600, %r302, %r598;
	atom.shared.or.b32 	%r601, [%r600], %r597;

BB1_86:
	add.s32 	%r85, %r1815, 2048;
	setp.ge.s32	%p57, %r85, %r11;
	@%p57 bra 	BB1_89;

	add.s32 	%r602, %r85, %r20;
	mul.wide.s32 	%rd87, %r602, 8;
	add.s64 	%rd88, %rd2, %rd87;
	ld.global.v2.u32 	{%r603, %r604}, [%rd88];
	or.b32  	%r606, %r603, %r604;
	setp.eq.s32	%p58, %r606, 0;
	@%p58 bra 	BB1_89;

	and.b32  	%r607, %r603, 131040;
	and.b32  	%r608, %r603, 31;
	mov.u32 	%r609, 1;
	shl.b32 	%r610, %r609, %r608;
	shr.u32 	%r611, %r607, 3;
	add.s32 	%r613, %r302, %r611;
	atom.shared.or.b32 	%r614, [%r613], %r610;

BB1_89:
	add.s32 	%r87, %r1815, 3072;
	setp.ge.s32	%p59, %r87, %r11;
	@%p59 bra 	BB1_92;

	add.s32 	%r615, %r87, %r20;
	mul.wide.s32 	%rd89, %r615, 8;
	add.s64 	%rd90, %rd2, %rd89;
	ld.global.v2.u32 	{%r616, %r617}, [%rd90];
	or.b32  	%r619, %r616, %r617;
	setp.eq.s32	%p60, %r619, 0;
	@%p60 bra 	BB1_92;

	and.b32  	%r620, %r616, 131040;
	and.b32  	%r621, %r616, 31;
	mov.u32 	%r622, 1;
	shl.b32 	%r623, %r622, %r621;
	shr.u32 	%r624, %r620, 3;
	add.s32 	%r626, %r302, %r624;
	atom.shared.or.b32 	%r627, [%r626], %r623;

BB1_92:
	add.s32 	%r1812, %r1812, 4;
	add.s32 	%r1815, %r1815, 4096;
	setp.lt.s32	%p61, %r1812, %r13;
	@%p61 bra 	BB1_80;

BB1_93:
	setp.lt.s32	%p62, %r15, 1024;
	@%p62 bra 	BB1_124;

	mov.u32 	%r632, 1;
	max.s32 	%r91, %r16, %r632;
	and.b32  	%r631, %r91, 3;
	mov.u32 	%r1817, 0;
	setp.eq.s32	%p63, %r631, 0;
	@%p63 bra 	BB1_109;

	setp.eq.s32	%p64, %r631, 1;
	@%p64 bra 	BB1_105;

	setp.eq.s32	%p65, %r631, 2;
	@%p65 bra 	BB1_101;

	setp.ge.s32	%p66, %r1, %r14;
	@%p66 bra 	BB1_98;

	add.s32 	%r635, %r1, %r21;
	mul.wide.s32 	%rd91, %r635, 8;
	add.s64 	%rd92, %rd2, %rd91;
	ld.global.v2.u32 	{%r636, %r637}, [%rd92];
	or.b32  	%r639, %r636, %r637;
	setp.eq.s32	%p67, %r639, 0;
	mov.u32 	%r1817, %r632;
	@%p67 bra 	BB1_101;

	and.b32  	%r641, %r636, 131040;
	and.b32  	%r642, %r636, 31;
	mov.u32 	%r1817, 1;
	shl.b32 	%r643, %r1817, %r642;
	shr.u32 	%r644, %r641, 3;
	add.s32 	%r646, %r302, %r644;
	atom.shared.or.b32 	%r647, [%r646], %r643;
	bra.uni 	BB1_101;

BB1_98:
	mov.u32 	%r1817, %r632;

BB1_101:
	shl.b32 	%r648, %r1817, 10;
	add.s32 	%r94, %r648, %r1;
	setp.ge.s32	%p68, %r94, %r14;
	@%p68 bra 	BB1_104;

	add.s32 	%r649, %r94, %r21;
	mul.wide.s32 	%rd93, %r649, 8;
	add.s64 	%rd94, %rd2, %rd93;
	ld.global.v2.u32 	{%r650, %r651}, [%rd94];
	or.b32  	%r653, %r650, %r651;
	setp.eq.s32	%p69, %r653, 0;
	@%p69 bra 	BB1_104;

	and.b32  	%r654, %r650, 131040;
	and.b32  	%r655, %r650, 31;
	mov.u32 	%r656, 1;
	shl.b32 	%r657, %r656, %r655;
	shr.u32 	%r658, %r654, 3;
	add.s32 	%r660, %r302, %r658;
	atom.shared.or.b32 	%r661, [%r660], %r657;

BB1_104:
	add.s32 	%r1817, %r1817, 1;

BB1_105:
	shl.b32 	%r662, %r1817, 10;
	add.s32 	%r98, %r662, %r1;
	setp.ge.s32	%p70, %r98, %r14;
	@%p70 bra 	BB1_108;

	add.s32 	%r663, %r98, %r21;
	mul.wide.s32 	%rd95, %r663, 8;
	add.s64 	%rd96, %rd2, %rd95;
	ld.global.v2.u32 	{%r664, %r665}, [%rd96];
	or.b32  	%r667, %r664, %r665;
	setp.eq.s32	%p71, %r667, 0;
	@%p71 bra 	BB1_108;

	and.b32  	%r668, %r664, 131040;
	and.b32  	%r669, %r664, 31;
	mov.u32 	%r670, 1;
	shl.b32 	%r671, %r670, %r669;
	shr.u32 	%r672, %r668, 3;
	add.s32 	%r674, %r302, %r672;
	atom.shared.or.b32 	%r675, [%r674], %r671;

BB1_108:
	add.s32 	%r1817, %r1817, 1;

BB1_109:
	setp.lt.u32	%p72, %r91, 4;
	@%p72 bra 	BB1_124;

	mad.lo.s32 	%r1820, %r1817, 1024, %r1;

BB1_111:
	setp.ge.s32	%p73, %r1820, %r14;
	@%p73 bra 	BB1_114;

	add.s32 	%r676, %r1820, %r21;
	mul.wide.s32 	%rd97, %r676, 8;
	add.s64 	%rd98, %rd2, %rd97;
	ld.global.v2.u32 	{%r677, %r678}, [%rd98];
	or.b32  	%r680, %r677, %r678;
	setp.eq.s32	%p74, %r680, 0;
	@%p74 bra 	BB1_114;

	and.b32  	%r681, %r677, 131040;
	and.b32  	%r682, %r677, 31;
	mov.u32 	%r683, 1;
	shl.b32 	%r684, %r683, %r682;
	shr.u32 	%r685, %r681, 3;
	add.s32 	%r687, %r302, %r685;
	atom.shared.or.b32 	%r688, [%r687], %r684;

BB1_114:
	add.s32 	%r106, %r1820, 1024;
	setp.ge.s32	%p75, %r106, %r14;
	@%p75 bra 	BB1_117;

	add.s32 	%r689, %r106, %r21;
	mul.wide.s32 	%rd99, %r689, 8;
	add.s64 	%rd100, %rd2, %rd99;
	ld.global.v2.u32 	{%r690, %r691}, [%rd100];
	or.b32  	%r693, %r690, %r691;
	setp.eq.s32	%p76, %r693, 0;
	@%p76 bra 	BB1_117;

	and.b32  	%r694, %r690, 131040;
	and.b32  	%r695, %r690, 31;
	mov.u32 	%r696, 1;
	shl.b32 	%r697, %r696, %r695;
	shr.u32 	%r698, %r694, 3;
	add.s32 	%r700, %r302, %r698;
	atom.shared.or.b32 	%r701, [%r700], %r697;

BB1_117:
	add.s32 	%r108, %r1820, 2048;
	setp.ge.s32	%p77, %r108, %r14;
	@%p77 bra 	BB1_120;

	add.s32 	%r702, %r108, %r21;
	mul.wide.s32 	%rd101, %r702, 8;
	add.s64 	%rd102, %rd2, %rd101;
	ld.global.v2.u32 	{%r703, %r704}, [%rd102];
	or.b32  	%r706, %r703, %r704;
	setp.eq.s32	%p78, %r706, 0;
	@%p78 bra 	BB1_120;

	and.b32  	%r707, %r703, 131040;
	and.b32  	%r708, %r703, 31;
	mov.u32 	%r709, 1;
	shl.b32 	%r710, %r709, %r708;
	shr.u32 	%r711, %r707, 3;
	add.s32 	%r713, %r302, %r711;
	atom.shared.or.b32 	%r714, [%r713], %r710;

BB1_120:
	add.s32 	%r110, %r1820, 3072;
	setp.ge.s32	%p79, %r110, %r14;
	@%p79 bra 	BB1_123;

	add.s32 	%r715, %r110, %r21;
	mul.wide.s32 	%rd103, %r715, 8;
	add.s64 	%rd104, %rd2, %rd103;
	ld.global.v2.u32 	{%r716, %r717}, [%rd104];
	or.b32  	%r719, %r716, %r717;
	setp.eq.s32	%p80, %r719, 0;
	@%p80 bra 	BB1_123;

	and.b32  	%r720, %r716, 131040;
	and.b32  	%r721, %r716, 31;
	mov.u32 	%r722, 1;
	shl.b32 	%r723, %r722, %r721;
	shr.u32 	%r724, %r720, 3;
	add.s32 	%r726, %r302, %r724;
	atom.shared.or.b32 	%r727, [%r726], %r723;

BB1_123:
	add.s32 	%r1817, %r1817, 4;
	add.s32 	%r1820, %r1820, 4096;
	setp.lt.s32	%p81, %r1817, %r16;
	@%p81 bra 	BB1_111;

BB1_124:
	setp.gt.s32	%p1, %r6, 1023;
	bar.sync 	0;
	shl.b32 	%r728, %r1, 3;
	add.s32 	%r730, %r302, 16384;
	add.s32 	%r114, %r730, %r728;
	mov.u64 	%rd105, 0;
	st.shared.u64 	[%r114], %rd105;
	shl.b32 	%r731, %r2, 3;
	add.s32 	%r115, %r730, %r731;
	st.shared.u64 	[%r115], %rd105;
	shl.b32 	%r732, %r3, 3;
	add.s32 	%r116, %r730, %r732;
	st.shared.u64 	[%r116], %rd105;
	shl.b32 	%r733, %r4, 3;
	add.s32 	%r117, %r730, %r733;
	st.shared.u64 	[%r117], %rd105;
	bar.sync 	0;
	@!%p1 bra 	BB1_183;
	bra.uni 	BB1_125;

BB1_125:
	add.s32 	%r118, %r299, -4;
	mov.u32 	%r738, 1;
	max.s32 	%r119, %r7, %r738;
	and.b32  	%r737, %r119, 3;
	mov.u32 	%r1822, 0;
	setp.eq.s32	%p82, %r737, 0;
	@%p82 bra 	BB1_152;

	setp.eq.s32	%p83, %r737, 1;
	@%p83 bra 	BB1_144;

	setp.eq.s32	%p84, %r737, 2;
	@%p84 bra 	BB1_136;

	setp.ge.s32	%p85, %r1, %r5;
	@%p85 bra 	BB1_129;

	add.s32 	%r741, %r1, %r18;
	mul.wide.s32 	%rd106, %r741, 8;
	add.s64 	%rd107, %rd2, %rd106;
	ld.global.v2.u32 	{%r742, %r743}, [%rd107];
	or.b32  	%r744, %r742, %r743;
	setp.eq.s32	%p86, %r744, 0;
	mov.u32 	%r1822, %r738;
	@%p86 bra 	BB1_136;

	and.b32  	%r746, %r742, 131040;
	and.b32  	%r747, %r742, 31;
	xor.b32  	%r748, %r747, 1;
	shr.u32 	%r749, %r746, 3;
	add.s32 	%r751, %r302, %r749;
	mov.u32 	%r1822, 1;
	shl.b32 	%r752, %r1822, %r748;
	ld.shared.u32 	%r753, [%r751];
	and.b32  	%r754, %r753, %r752;
	setp.eq.s32	%p87, %r754, 0;
	@%p87 bra 	BB1_136;

	bfe.u32 	%r122, %r743, 17, 12;
	shl.b32 	%r755, %r122, 3;
	add.s32 	%r757, %r302, %r755;
	add.s32 	%r123, %r757, 16384;
	atom.shared.exch.b64 	%rd4, [%r123], 0;
	setp.eq.s64	%p88, %rd4, 0;
	@%p88 bra 	BB1_134;

	add.s32 	%r759, %r122, %r17;
	mul.wide.s32 	%rd108, %r759, 4;
	add.s64 	%rd109, %rd3, %rd108;
	atom.global.add.u32 	%r760, [%rd109], 2;
	min.s32 	%r761, %r760, %r118;
	mad.lo.s32 	%r762, %r122, %r299, %r761;
	shr.u32 	%r763, %r762, 31;
	add.s32 	%r764, %r762, %r763;
	shr.s32 	%r765, %r764, 1;
	shr.u64 	%rd110, %rd4, 32;
	mul.wide.s32 	%rd111, %r765, 16;
	add.s64 	%rd112, %rd1, %rd111;
	cvt.u32.u64	%r766, %rd4;
	cvt.u32.u64	%r767, %rd110;
	st.global.v4.u32 	[%rd112], {%r743, %r742, %r767, %r766};
	bra.uni 	BB1_136;

BB1_129:
	mov.u32 	%r1822, %r738;

BB1_136:
	shl.b32 	%r778, %r1822, 10;
	add.s32 	%r125, %r778, %r1;
	setp.ge.s32	%p90, %r125, %r5;
	@%p90 bra 	BB1_143;

	add.s32 	%r779, %r125, %r18;
	mul.wide.s32 	%rd122, %r779, 8;
	add.s64 	%rd123, %rd2, %rd122;
	ld.global.v2.u32 	{%r780, %r781}, [%rd123];
	or.b32  	%r782, %r780, %r781;
	setp.eq.s32	%p91, %r782, 0;
	@%p91 bra 	BB1_143;

	and.b32  	%r783, %r780, 131040;
	and.b32  	%r784, %r780, 31;
	xor.b32  	%r785, %r784, 1;
	shr.u32 	%r786, %r783, 3;
	add.s32 	%r788, %r302, %r786;
	mov.u32 	%r789, 1;
	shl.b32 	%r790, %r789, %r785;
	ld.shared.u32 	%r791, [%r788];
	and.b32  	%r792, %r791, %r790;
	setp.eq.s32	%p92, %r792, 0;
	@%p92 bra 	BB1_143;

	bfe.u32 	%r128, %r781, 17, 12;
	shl.b32 	%r793, %r128, 3;
	add.s32 	%r795, %r302, %r793;
	add.s32 	%r129, %r795, 16384;
	atom.shared.exch.b64 	%rd5, [%r129], 0;
	setp.eq.s64	%p93, %rd5, 0;
	@%p93 bra 	BB1_141;

	add.s32 	%r796, %r128, %r17;
	mul.wide.s32 	%rd124, %r796, 4;
	add.s64 	%rd125, %rd3, %rd124;
	atom.global.add.u32 	%r797, [%rd125], 2;
	min.s32 	%r798, %r797, %r118;
	mad.lo.s32 	%r799, %r128, %r299, %r798;
	shr.u32 	%r800, %r799, 31;
	add.s32 	%r801, %r799, %r800;
	shr.s32 	%r802, %r801, 1;
	shr.u64 	%rd126, %rd5, 32;
	mul.wide.s32 	%rd127, %r802, 16;
	add.s64 	%rd128, %rd1, %rd127;
	cvt.u32.u64	%r803, %rd5;
	cvt.u32.u64	%r804, %rd126;
	st.global.v4.u32 	[%rd128], {%r781, %r780, %r804, %r803};
	bra.uni 	BB1_143;

BB1_141:
	cvt.u64.u32	%rd129, %r781;
	cvt.u64.u32	%rd130, %r780;
	bfi.b64 	%rd131, %rd129, %rd130, 32, 32;
	add.s32 	%r1775, %r795, 16384;
	atom.shared.cas.b64 	%rd133, [%r1775], %rd105, %rd131;
	setp.eq.s64	%p94, %rd133, 0;
	@%p94 bra 	BB1_143;

	add.s32 	%r805, %r128, %r17;
	mul.wide.s32 	%rd134, %r805, 4;
	add.s64 	%rd135, %rd3, %rd134;
	atom.global.add.u32 	%r806, [%rd135], 2;
	min.s32 	%r807, %r806, %r118;
	mad.lo.s32 	%r808, %r128, %r299, %r807;
	shr.u32 	%r809, %r808, 31;
	add.s32 	%r810, %r808, %r809;
	shr.s32 	%r811, %r810, 1;
	mul.wide.s32 	%rd136, %r811, 16;
	add.s64 	%rd137, %rd1, %rd136;
	mov.u32 	%r812, 0;
	st.global.v4.u32 	[%rd137], {%r781, %r780, %r812, %r812};

BB1_143:
	add.s32 	%r1822, %r1822, 1;

BB1_144:
	shl.b32 	%r813, %r1822, 10;
	add.s32 	%r132, %r813, %r1;
	setp.ge.s32	%p95, %r132, %r5;
	@%p95 bra 	BB1_151;

	add.s32 	%r814, %r132, %r18;
	mul.wide.s32 	%rd138, %r814, 8;
	add.s64 	%rd139, %rd2, %rd138;
	ld.global.v2.u32 	{%r815, %r816}, [%rd139];
	or.b32  	%r817, %r815, %r816;
	setp.eq.s32	%p96, %r817, 0;
	@%p96 bra 	BB1_151;

	and.b32  	%r818, %r815, 131040;
	and.b32  	%r819, %r815, 31;
	xor.b32  	%r820, %r819, 1;
	shr.u32 	%r821, %r818, 3;
	add.s32 	%r823, %r302, %r821;
	mov.u32 	%r824, 1;
	shl.b32 	%r825, %r824, %r820;
	ld.shared.u32 	%r826, [%r823];
	and.b32  	%r827, %r826, %r825;
	setp.eq.s32	%p97, %r827, 0;
	@%p97 bra 	BB1_151;

	bfe.u32 	%r135, %r816, 17, 12;
	shl.b32 	%r828, %r135, 3;
	add.s32 	%r830, %r302, %r828;
	add.s32 	%r136, %r830, 16384;
	atom.shared.exch.b64 	%rd6, [%r136], 0;
	setp.eq.s64	%p98, %rd6, 0;
	@%p98 bra 	BB1_149;

	add.s32 	%r831, %r135, %r17;
	mul.wide.s32 	%rd140, %r831, 4;
	add.s64 	%rd141, %rd3, %rd140;
	atom.global.add.u32 	%r832, [%rd141], 2;
	min.s32 	%r833, %r832, %r118;
	mad.lo.s32 	%r834, %r135, %r299, %r833;
	shr.u32 	%r835, %r834, 31;
	add.s32 	%r836, %r834, %r835;
	shr.s32 	%r837, %r836, 1;
	shr.u64 	%rd142, %rd6, 32;
	mul.wide.s32 	%rd143, %r837, 16;
	add.s64 	%rd144, %rd1, %rd143;
	cvt.u32.u64	%r838, %rd6;
	cvt.u32.u64	%r839, %rd142;
	st.global.v4.u32 	[%rd144], {%r816, %r815, %r839, %r838};
	bra.uni 	BB1_151;

BB1_149:
	cvt.u64.u32	%rd145, %r816;
	cvt.u64.u32	%rd146, %r815;
	bfi.b64 	%rd147, %rd145, %rd146, 32, 32;
	add.s32 	%r1776, %r830, 16384;
	atom.shared.cas.b64 	%rd149, [%r1776], %rd105, %rd147;
	setp.eq.s64	%p99, %rd149, 0;
	@%p99 bra 	BB1_151;

	add.s32 	%r840, %r135, %r17;
	mul.wide.s32 	%rd150, %r840, 4;
	add.s64 	%rd151, %rd3, %rd150;
	atom.global.add.u32 	%r841, [%rd151], 2;
	min.s32 	%r842, %r841, %r118;
	mad.lo.s32 	%r843, %r135, %r299, %r842;
	shr.u32 	%r844, %r843, 31;
	add.s32 	%r845, %r843, %r844;
	shr.s32 	%r846, %r845, 1;
	mul.wide.s32 	%rd152, %r846, 16;
	add.s64 	%rd153, %rd1, %rd152;
	mov.u32 	%r847, 0;
	st.global.v4.u32 	[%rd153], {%r816, %r815, %r847, %r847};

BB1_151:
	add.s32 	%r1822, %r1822, 1;

BB1_152:
	setp.lt.u32	%p100, %r119, 4;
	@%p100 bra 	BB1_183;

	mad.lo.s32 	%r1825, %r1822, 1024, %r1;

BB1_154:
	setp.ge.s32	%p101, %r1825, %r5;
	@%p101 bra 	BB1_161;

	add.s32 	%r848, %r1825, %r18;
	mul.wide.s32 	%rd154, %r848, 8;
	add.s64 	%rd155, %rd2, %rd154;
	ld.global.v2.u32 	{%r849, %r850}, [%rd155];
	or.b32  	%r851, %r849, %r850;
	setp.eq.s32	%p102, %r851, 0;
	@%p102 bra 	BB1_161;

	and.b32  	%r852, %r849, 131040;
	and.b32  	%r853, %r849, 31;
	xor.b32  	%r854, %r853, 1;
	shr.u32 	%r855, %r852, 3;
	add.s32 	%r857, %r302, %r855;
	mov.u32 	%r858, 1;
	shl.b32 	%r859, %r858, %r854;
	ld.shared.u32 	%r860, [%r857];
	and.b32  	%r861, %r860, %r859;
	setp.eq.s32	%p103, %r861, 0;
	@%p103 bra 	BB1_161;

	bfe.u32 	%r144, %r850, 17, 12;
	shl.b32 	%r862, %r144, 3;
	add.s32 	%r864, %r302, %r862;
	add.s32 	%r145, %r864, 16384;
	atom.shared.exch.b64 	%rd7, [%r145], 0;
	setp.eq.s64	%p104, %rd7, 0;
	@%p104 bra 	BB1_159;
	bra.uni 	BB1_158;

BB1_159:
	cvt.u64.u32	%rd161, %r850;
	cvt.u64.u32	%rd162, %r849;
	bfi.b64 	%rd163, %rd161, %rd162, 32, 32;
	add.s32 	%r1777, %r864, 16384;
	atom.shared.cas.b64 	%rd165, [%r1777], %rd105, %rd163;
	setp.eq.s64	%p105, %rd165, 0;
	@%p105 bra 	BB1_161;

	add.s32 	%r874, %r144, %r17;
	mul.wide.s32 	%rd166, %r874, 4;
	add.s64 	%rd167, %rd3, %rd166;
	atom.global.add.u32 	%r875, [%rd167], 2;
	min.s32 	%r876, %r875, %r118;
	mad.lo.s32 	%r877, %r144, %r299, %r876;
	shr.u32 	%r878, %r877, 31;
	add.s32 	%r879, %r877, %r878;
	shr.s32 	%r880, %r879, 1;
	mul.wide.s32 	%rd168, %r880, 16;
	add.s64 	%rd169, %rd1, %rd168;
	mov.u32 	%r881, 0;
	st.global.v4.u32 	[%rd169], {%r850, %r849, %r881, %r881};
	bra.uni 	BB1_161;

BB1_158:
	add.s32 	%r865, %r144, %r17;
	mul.wide.s32 	%rd156, %r865, 4;
	add.s64 	%rd157, %rd3, %rd156;
	atom.global.add.u32 	%r866, [%rd157], 2;
	min.s32 	%r867, %r866, %r118;
	mad.lo.s32 	%r868, %r144, %r299, %r867;
	shr.u32 	%r869, %r868, 31;
	add.s32 	%r870, %r868, %r869;
	shr.s32 	%r871, %r870, 1;
	shr.u64 	%rd158, %rd7, 32;
	mul.wide.s32 	%rd159, %r871, 16;
	add.s64 	%rd160, %rd1, %rd159;
	cvt.u32.u64	%r872, %rd7;
	cvt.u32.u64	%r873, %rd158;
	st.global.v4.u32 	[%rd160], {%r850, %r849, %r873, %r872};

BB1_161:
	add.s32 	%r146, %r1825, 1024;
	setp.ge.s32	%p106, %r146, %r5;
	@%p106 bra 	BB1_168;

	add.s32 	%r882, %r146, %r18;
	mul.wide.s32 	%rd170, %r882, 8;
	add.s64 	%rd171, %rd2, %rd170;
	ld.global.v2.u32 	{%r883, %r884}, [%rd171];
	or.b32  	%r885, %r883, %r884;
	setp.eq.s32	%p107, %r885, 0;
	@%p107 bra 	BB1_168;

	and.b32  	%r886, %r883, 131040;
	and.b32  	%r887, %r883, 31;
	xor.b32  	%r888, %r887, 1;
	shr.u32 	%r889, %r886, 3;
	add.s32 	%r891, %r302, %r889;
	mov.u32 	%r892, 1;
	shl.b32 	%r893, %r892, %r888;
	ld.shared.u32 	%r894, [%r891];
	and.b32  	%r895, %r894, %r893;
	setp.eq.s32	%p108, %r895, 0;
	@%p108 bra 	BB1_168;

	bfe.u32 	%r149, %r884, 17, 12;
	shl.b32 	%r896, %r149, 3;
	add.s32 	%r898, %r302, %r896;
	add.s32 	%r150, %r898, 16384;
	atom.shared.exch.b64 	%rd8, [%r150], 0;
	setp.eq.s64	%p109, %rd8, 0;
	@%p109 bra 	BB1_166;
	bra.uni 	BB1_165;

BB1_166:
	cvt.u64.u32	%rd177, %r884;
	cvt.u64.u32	%rd178, %r883;
	bfi.b64 	%rd179, %rd177, %rd178, 32, 32;
	add.s32 	%r1778, %r898, 16384;
	atom.shared.cas.b64 	%rd181, [%r1778], %rd105, %rd179;
	setp.eq.s64	%p110, %rd181, 0;
	@%p110 bra 	BB1_168;

	add.s32 	%r908, %r149, %r17;
	mul.wide.s32 	%rd182, %r908, 4;
	add.s64 	%rd183, %rd3, %rd182;
	atom.global.add.u32 	%r909, [%rd183], 2;
	min.s32 	%r910, %r909, %r118;
	mad.lo.s32 	%r911, %r149, %r299, %r910;
	shr.u32 	%r912, %r911, 31;
	add.s32 	%r913, %r911, %r912;
	shr.s32 	%r914, %r913, 1;
	mul.wide.s32 	%rd184, %r914, 16;
	add.s64 	%rd185, %rd1, %rd184;
	mov.u32 	%r915, 0;
	st.global.v4.u32 	[%rd185], {%r884, %r883, %r915, %r915};
	bra.uni 	BB1_168;

BB1_165:
	add.s32 	%r899, %r149, %r17;
	mul.wide.s32 	%rd172, %r899, 4;
	add.s64 	%rd173, %rd3, %rd172;
	atom.global.add.u32 	%r900, [%rd173], 2;
	min.s32 	%r901, %r900, %r118;
	mad.lo.s32 	%r902, %r149, %r299, %r901;
	shr.u32 	%r903, %r902, 31;
	add.s32 	%r904, %r902, %r903;
	shr.s32 	%r905, %r904, 1;
	shr.u64 	%rd174, %rd8, 32;
	mul.wide.s32 	%rd175, %r905, 16;
	add.s64 	%rd176, %rd1, %rd175;
	cvt.u32.u64	%r906, %rd8;
	cvt.u32.u64	%r907, %rd174;
	st.global.v4.u32 	[%rd176], {%r884, %r883, %r907, %r906};

BB1_168:
	add.s32 	%r151, %r1825, 2048;
	setp.ge.s32	%p111, %r151, %r5;
	@%p111 bra 	BB1_175;

	add.s32 	%r916, %r151, %r18;
	mul.wide.s32 	%rd186, %r916, 8;
	add.s64 	%rd187, %rd2, %rd186;
	ld.global.v2.u32 	{%r917, %r918}, [%rd187];
	or.b32  	%r919, %r917, %r918;
	setp.eq.s32	%p112, %r919, 0;
	@%p112 bra 	BB1_175;

	and.b32  	%r920, %r917, 131040;
	and.b32  	%r921, %r917, 31;
	xor.b32  	%r922, %r921, 1;
	shr.u32 	%r923, %r920, 3;
	add.s32 	%r925, %r302, %r923;
	mov.u32 	%r926, 1;
	shl.b32 	%r927, %r926, %r922;
	ld.shared.u32 	%r928, [%r925];
	and.b32  	%r929, %r928, %r927;
	setp.eq.s32	%p113, %r929, 0;
	@%p113 bra 	BB1_175;

	bfe.u32 	%r154, %r918, 17, 12;
	shl.b32 	%r930, %r154, 3;
	add.s32 	%r932, %r302, %r930;
	add.s32 	%r155, %r932, 16384;
	atom.shared.exch.b64 	%rd9, [%r155], 0;
	setp.eq.s64	%p114, %rd9, 0;
	@%p114 bra 	BB1_173;
	bra.uni 	BB1_172;

BB1_173:
	cvt.u64.u32	%rd193, %r918;
	cvt.u64.u32	%rd194, %r917;
	bfi.b64 	%rd195, %rd193, %rd194, 32, 32;
	add.s32 	%r1779, %r932, 16384;
	atom.shared.cas.b64 	%rd197, [%r1779], %rd105, %rd195;
	setp.eq.s64	%p115, %rd197, 0;
	@%p115 bra 	BB1_175;

	add.s32 	%r942, %r154, %r17;
	mul.wide.s32 	%rd198, %r942, 4;
	add.s64 	%rd199, %rd3, %rd198;
	atom.global.add.u32 	%r943, [%rd199], 2;
	min.s32 	%r944, %r943, %r118;
	mad.lo.s32 	%r945, %r154, %r299, %r944;
	shr.u32 	%r946, %r945, 31;
	add.s32 	%r947, %r945, %r946;
	shr.s32 	%r948, %r947, 1;
	mul.wide.s32 	%rd200, %r948, 16;
	add.s64 	%rd201, %rd1, %rd200;
	mov.u32 	%r949, 0;
	st.global.v4.u32 	[%rd201], {%r918, %r917, %r949, %r949};
	bra.uni 	BB1_175;

BB1_172:
	add.s32 	%r933, %r154, %r17;
	mul.wide.s32 	%rd188, %r933, 4;
	add.s64 	%rd189, %rd3, %rd188;
	atom.global.add.u32 	%r934, [%rd189], 2;
	min.s32 	%r935, %r934, %r118;
	mad.lo.s32 	%r936, %r154, %r299, %r935;
	shr.u32 	%r937, %r936, 31;
	add.s32 	%r938, %r936, %r937;
	shr.s32 	%r939, %r938, 1;
	shr.u64 	%rd190, %rd9, 32;
	mul.wide.s32 	%rd191, %r939, 16;
	add.s64 	%rd192, %rd1, %rd191;
	cvt.u32.u64	%r940, %rd9;
	cvt.u32.u64	%r941, %rd190;
	st.global.v4.u32 	[%rd192], {%r918, %r917, %r941, %r940};

BB1_175:
	add.s32 	%r156, %r1825, 3072;
	setp.ge.s32	%p116, %r156, %r5;
	@%p116 bra 	BB1_182;

	add.s32 	%r950, %r156, %r18;
	mul.wide.s32 	%rd202, %r950, 8;
	add.s64 	%rd203, %rd2, %rd202;
	ld.global.v2.u32 	{%r951, %r952}, [%rd203];
	or.b32  	%r953, %r951, %r952;
	setp.eq.s32	%p117, %r953, 0;
	@%p117 bra 	BB1_182;

	and.b32  	%r954, %r951, 131040;
	and.b32  	%r955, %r951, 31;
	xor.b32  	%r956, %r955, 1;
	shr.u32 	%r957, %r954, 3;
	add.s32 	%r959, %r302, %r957;
	mov.u32 	%r960, 1;
	shl.b32 	%r961, %r960, %r956;
	ld.shared.u32 	%r962, [%r959];
	and.b32  	%r963, %r962, %r961;
	setp.eq.s32	%p118, %r963, 0;
	@%p118 bra 	BB1_182;

	bfe.u32 	%r159, %r952, 17, 12;
	shl.b32 	%r964, %r159, 3;
	add.s32 	%r966, %r302, %r964;
	add.s32 	%r160, %r966, 16384;
	atom.shared.exch.b64 	%rd10, [%r160], 0;
	setp.eq.s64	%p119, %rd10, 0;
	@%p119 bra 	BB1_180;
	bra.uni 	BB1_179;

BB1_180:
	cvt.u64.u32	%rd209, %r952;
	cvt.u64.u32	%rd210, %r951;
	bfi.b64 	%rd211, %rd209, %rd210, 32, 32;
	add.s32 	%r1780, %r966, 16384;
	atom.shared.cas.b64 	%rd213, [%r1780], %rd105, %rd211;
	setp.eq.s64	%p120, %rd213, 0;
	@%p120 bra 	BB1_182;

	add.s32 	%r976, %r159, %r17;
	mul.wide.s32 	%rd214, %r976, 4;
	add.s64 	%rd215, %rd3, %rd214;
	atom.global.add.u32 	%r977, [%rd215], 2;
	min.s32 	%r978, %r977, %r118;
	mad.lo.s32 	%r979, %r159, %r299, %r978;
	shr.u32 	%r980, %r979, 31;
	add.s32 	%r981, %r979, %r980;
	shr.s32 	%r982, %r981, 1;
	mul.wide.s32 	%rd216, %r982, 16;
	add.s64 	%rd217, %rd1, %rd216;
	mov.u32 	%r983, 0;
	st.global.v4.u32 	[%rd217], {%r952, %r951, %r983, %r983};
	bra.uni 	BB1_182;

BB1_179:
	add.s32 	%r967, %r159, %r17;
	mul.wide.s32 	%rd204, %r967, 4;
	add.s64 	%rd205, %rd3, %rd204;
	atom.global.add.u32 	%r968, [%rd205], 2;
	min.s32 	%r969, %r968, %r118;
	mad.lo.s32 	%r970, %r159, %r299, %r969;
	shr.u32 	%r971, %r970, 31;
	add.s32 	%r972, %r970, %r971;
	shr.s32 	%r973, %r972, 1;
	shr.u64 	%rd206, %rd10, 32;
	mul.wide.s32 	%rd207, %r973, 16;
	add.s64 	%rd208, %rd1, %rd207;
	cvt.u32.u64	%r974, %rd10;
	cvt.u32.u64	%r975, %rd206;
	st.global.v4.u32 	[%rd208], {%r952, %r951, %r975, %r974};

BB1_182:
	add.s32 	%r1822, %r1822, 4;
	add.s32 	%r1825, %r1825, 4096;
	setp.lt.s32	%p121, %r1822, %r7;
	@%p121 bra 	BB1_154;

BB1_183:
	@%p22 bra 	BB1_242;

	add.s32 	%r163, %r299, -4;
	mov.u32 	%r988, 1;
	max.s32 	%r164, %r10, %r988;
	and.b32  	%r987, %r164, 3;
	mov.u32 	%r1827, 0;
	setp.eq.s32	%p123, %r987, 0;
	@%p123 bra 	BB1_211;

	setp.eq.s32	%p124, %r987, 1;
	@%p124 bra 	BB1_203;

	setp.eq.s32	%p125, %r987, 2;
	@%p125 bra 	BB1_195;

	setp.ge.s32	%p126, %r1, %r8;
	@%p126 bra 	BB1_188;

	add.s32 	%r991, %r1, %r19;
	mul.wide.s32 	%rd218, %r991, 8;
	add.s64 	%rd219, %rd2, %rd218;
	ld.global.v2.u32 	{%r992, %r993}, [%rd219];
	or.b32  	%r994, %r992, %r993;
	setp.eq.s32	%p127, %r994, 0;
	mov.u32 	%r1827, %r988;
	@%p127 bra 	BB1_195;

	and.b32  	%r996, %r992, 131040;
	and.b32  	%r997, %r992, 31;
	xor.b32  	%r998, %r997, 1;
	shr.u32 	%r999, %r996, 3;
	add.s32 	%r1001, %r302, %r999;
	mov.u32 	%r1827, 1;
	shl.b32 	%r1002, %r1827, %r998;
	ld.shared.u32 	%r1003, [%r1001];
	and.b32  	%r1004, %r1003, %r1002;
	setp.eq.s32	%p128, %r1004, 0;
	@%p128 bra 	BB1_195;

	bfe.u32 	%r167, %r993, 17, 12;
	shl.b32 	%r1005, %r167, 3;
	add.s32 	%r1007, %r302, %r1005;
	add.s32 	%r168, %r1007, 16384;
	atom.shared.exch.b64 	%rd11, [%r168], 0;
	setp.eq.s64	%p129, %rd11, 0;
	@%p129 bra 	BB1_193;

	add.s32 	%r1009, %r167, %r17;
	mul.wide.s32 	%rd220, %r1009, 4;
	add.s64 	%rd221, %rd3, %rd220;
	atom.global.add.u32 	%r1010, [%rd221], 2;
	min.s32 	%r1011, %r1010, %r163;
	mad.lo.s32 	%r1012, %r167, %r299, %r1011;
	shr.u32 	%r1013, %r1012, 31;
	add.s32 	%r1014, %r1012, %r1013;
	shr.s32 	%r1015, %r1014, 1;
	shr.u64 	%rd222, %rd11, 32;
	mul.wide.s32 	%rd223, %r1015, 16;
	add.s64 	%rd224, %rd1, %rd223;
	cvt.u32.u64	%r1016, %rd11;
	cvt.u32.u64	%r1017, %rd222;
	st.global.v4.u32 	[%rd224], {%r993, %r992, %r1017, %r1016};
	bra.uni 	BB1_195;

BB1_188:
	mov.u32 	%r1827, %r988;

BB1_195:
	shl.b32 	%r1028, %r1827, 10;
	add.s32 	%r170, %r1028, %r1;
	setp.ge.s32	%p131, %r170, %r8;
	@%p131 bra 	BB1_202;

	add.s32 	%r1029, %r170, %r19;
	mul.wide.s32 	%rd234, %r1029, 8;
	add.s64 	%rd235, %rd2, %rd234;
	ld.global.v2.u32 	{%r1030, %r1031}, [%rd235];
	or.b32  	%r1032, %r1030, %r1031;
	setp.eq.s32	%p132, %r1032, 0;
	@%p132 bra 	BB1_202;

	and.b32  	%r1033, %r1030, 131040;
	and.b32  	%r1034, %r1030, 31;
	xor.b32  	%r1035, %r1034, 1;
	shr.u32 	%r1036, %r1033, 3;
	add.s32 	%r1038, %r302, %r1036;
	mov.u32 	%r1039, 1;
	shl.b32 	%r1040, %r1039, %r1035;
	ld.shared.u32 	%r1041, [%r1038];
	and.b32  	%r1042, %r1041, %r1040;
	setp.eq.s32	%p133, %r1042, 0;
	@%p133 bra 	BB1_202;

	bfe.u32 	%r173, %r1031, 17, 12;
	shl.b32 	%r1043, %r173, 3;
	add.s32 	%r1045, %r302, %r1043;
	add.s32 	%r174, %r1045, 16384;
	atom.shared.exch.b64 	%rd12, [%r174], 0;
	setp.eq.s64	%p134, %rd12, 0;
	@%p134 bra 	BB1_200;

	add.s32 	%r1046, %r173, %r17;
	mul.wide.s32 	%rd236, %r1046, 4;
	add.s64 	%rd237, %rd3, %rd236;
	atom.global.add.u32 	%r1047, [%rd237], 2;
	min.s32 	%r1048, %r1047, %r163;
	mad.lo.s32 	%r1049, %r173, %r299, %r1048;
	shr.u32 	%r1050, %r1049, 31;
	add.s32 	%r1051, %r1049, %r1050;
	shr.s32 	%r1052, %r1051, 1;
	shr.u64 	%rd238, %rd12, 32;
	mul.wide.s32 	%rd239, %r1052, 16;
	add.s64 	%rd240, %rd1, %rd239;
	cvt.u32.u64	%r1053, %rd12;
	cvt.u32.u64	%r1054, %rd238;
	st.global.v4.u32 	[%rd240], {%r1031, %r1030, %r1054, %r1053};
	bra.uni 	BB1_202;

BB1_200:
	cvt.u64.u32	%rd241, %r1031;
	cvt.u64.u32	%rd242, %r1030;
	bfi.b64 	%rd243, %rd241, %rd242, 32, 32;
	add.s32 	%r1782, %r1045, 16384;
	atom.shared.cas.b64 	%rd245, [%r1782], %rd105, %rd243;
	setp.eq.s64	%p135, %rd245, 0;
	@%p135 bra 	BB1_202;

	add.s32 	%r1055, %r173, %r17;
	mul.wide.s32 	%rd246, %r1055, 4;
	add.s64 	%rd247, %rd3, %rd246;
	atom.global.add.u32 	%r1056, [%rd247], 2;
	min.s32 	%r1057, %r1056, %r163;
	mad.lo.s32 	%r1058, %r173, %r299, %r1057;
	shr.u32 	%r1059, %r1058, 31;
	add.s32 	%r1060, %r1058, %r1059;
	shr.s32 	%r1061, %r1060, 1;
	mul.wide.s32 	%rd248, %r1061, 16;
	add.s64 	%rd249, %rd1, %rd248;
	mov.u32 	%r1062, 0;
	st.global.v4.u32 	[%rd249], {%r1031, %r1030, %r1062, %r1062};

BB1_202:
	add.s32 	%r1827, %r1827, 1;

BB1_203:
	shl.b32 	%r1063, %r1827, 10;
	add.s32 	%r177, %r1063, %r1;
	setp.ge.s32	%p136, %r177, %r8;
	@%p136 bra 	BB1_210;

	add.s32 	%r1064, %r177, %r19;
	mul.wide.s32 	%rd250, %r1064, 8;
	add.s64 	%rd251, %rd2, %rd250;
	ld.global.v2.u32 	{%r1065, %r1066}, [%rd251];
	or.b32  	%r1067, %r1065, %r1066;
	setp.eq.s32	%p137, %r1067, 0;
	@%p137 bra 	BB1_210;

	and.b32  	%r1068, %r1065, 131040;
	and.b32  	%r1069, %r1065, 31;
	xor.b32  	%r1070, %r1069, 1;
	shr.u32 	%r1071, %r1068, 3;
	add.s32 	%r1073, %r302, %r1071;
	mov.u32 	%r1074, 1;
	shl.b32 	%r1075, %r1074, %r1070;
	ld.shared.u32 	%r1076, [%r1073];
	and.b32  	%r1077, %r1076, %r1075;
	setp.eq.s32	%p138, %r1077, 0;
	@%p138 bra 	BB1_210;

	bfe.u32 	%r180, %r1066, 17, 12;
	shl.b32 	%r1078, %r180, 3;
	add.s32 	%r1080, %r302, %r1078;
	add.s32 	%r181, %r1080, 16384;
	atom.shared.exch.b64 	%rd13, [%r181], 0;
	setp.eq.s64	%p139, %rd13, 0;
	@%p139 bra 	BB1_208;

	add.s32 	%r1081, %r180, %r17;
	mul.wide.s32 	%rd252, %r1081, 4;
	add.s64 	%rd253, %rd3, %rd252;
	atom.global.add.u32 	%r1082, [%rd253], 2;
	min.s32 	%r1083, %r1082, %r163;
	mad.lo.s32 	%r1084, %r180, %r299, %r1083;
	shr.u32 	%r1085, %r1084, 31;
	add.s32 	%r1086, %r1084, %r1085;
	shr.s32 	%r1087, %r1086, 1;
	shr.u64 	%rd254, %rd13, 32;
	mul.wide.s32 	%rd255, %r1087, 16;
	add.s64 	%rd256, %rd1, %rd255;
	cvt.u32.u64	%r1088, %rd13;
	cvt.u32.u64	%r1089, %rd254;
	st.global.v4.u32 	[%rd256], {%r1066, %r1065, %r1089, %r1088};
	bra.uni 	BB1_210;

BB1_208:
	cvt.u64.u32	%rd257, %r1066;
	cvt.u64.u32	%rd258, %r1065;
	bfi.b64 	%rd259, %rd257, %rd258, 32, 32;
	add.s32 	%r1783, %r1080, 16384;
	atom.shared.cas.b64 	%rd261, [%r1783], %rd105, %rd259;
	setp.eq.s64	%p140, %rd261, 0;
	@%p140 bra 	BB1_210;

	add.s32 	%r1090, %r180, %r17;
	mul.wide.s32 	%rd262, %r1090, 4;
	add.s64 	%rd263, %rd3, %rd262;
	atom.global.add.u32 	%r1091, [%rd263], 2;
	min.s32 	%r1092, %r1091, %r163;
	mad.lo.s32 	%r1093, %r180, %r299, %r1092;
	shr.u32 	%r1094, %r1093, 31;
	add.s32 	%r1095, %r1093, %r1094;
	shr.s32 	%r1096, %r1095, 1;
	mul.wide.s32 	%rd264, %r1096, 16;
	add.s64 	%rd265, %rd1, %rd264;
	mov.u32 	%r1097, 0;
	st.global.v4.u32 	[%rd265], {%r1066, %r1065, %r1097, %r1097};

BB1_210:
	add.s32 	%r1827, %r1827, 1;

BB1_211:
	setp.lt.u32	%p141, %r164, 4;
	@%p141 bra 	BB1_242;

	mad.lo.s32 	%r1830, %r1827, 1024, %r1;

BB1_213:
	setp.ge.s32	%p142, %r1830, %r8;
	@%p142 bra 	BB1_220;

	add.s32 	%r1098, %r1830, %r19;
	mul.wide.s32 	%rd266, %r1098, 8;
	add.s64 	%rd267, %rd2, %rd266;
	ld.global.v2.u32 	{%r1099, %r1100}, [%rd267];
	or.b32  	%r1101, %r1099, %r1100;
	setp.eq.s32	%p143, %r1101, 0;
	@%p143 bra 	BB1_220;

	and.b32  	%r1102, %r1099, 131040;
	and.b32  	%r1103, %r1099, 31;
	xor.b32  	%r1104, %r1103, 1;
	shr.u32 	%r1105, %r1102, 3;
	add.s32 	%r1107, %r302, %r1105;
	mov.u32 	%r1108, 1;
	shl.b32 	%r1109, %r1108, %r1104;
	ld.shared.u32 	%r1110, [%r1107];
	and.b32  	%r1111, %r1110, %r1109;
	setp.eq.s32	%p144, %r1111, 0;
	@%p144 bra 	BB1_220;

	bfe.u32 	%r189, %r1100, 17, 12;
	shl.b32 	%r1112, %r189, 3;
	add.s32 	%r1114, %r302, %r1112;
	add.s32 	%r190, %r1114, 16384;
	atom.shared.exch.b64 	%rd14, [%r190], 0;
	setp.eq.s64	%p145, %rd14, 0;
	@%p145 bra 	BB1_218;
	bra.uni 	BB1_217;

BB1_218:
	cvt.u64.u32	%rd273, %r1100;
	cvt.u64.u32	%rd274, %r1099;
	bfi.b64 	%rd275, %rd273, %rd274, 32, 32;
	add.s32 	%r1784, %r1114, 16384;
	atom.shared.cas.b64 	%rd277, [%r1784], %rd105, %rd275;
	setp.eq.s64	%p146, %rd277, 0;
	@%p146 bra 	BB1_220;

	add.s32 	%r1124, %r189, %r17;
	mul.wide.s32 	%rd278, %r1124, 4;
	add.s64 	%rd279, %rd3, %rd278;
	atom.global.add.u32 	%r1125, [%rd279], 2;
	min.s32 	%r1126, %r1125, %r163;
	mad.lo.s32 	%r1127, %r189, %r299, %r1126;
	shr.u32 	%r1128, %r1127, 31;
	add.s32 	%r1129, %r1127, %r1128;
	shr.s32 	%r1130, %r1129, 1;
	mul.wide.s32 	%rd280, %r1130, 16;
	add.s64 	%rd281, %rd1, %rd280;
	mov.u32 	%r1131, 0;
	st.global.v4.u32 	[%rd281], {%r1100, %r1099, %r1131, %r1131};
	bra.uni 	BB1_220;

BB1_217:
	add.s32 	%r1115, %r189, %r17;
	mul.wide.s32 	%rd268, %r1115, 4;
	add.s64 	%rd269, %rd3, %rd268;
	atom.global.add.u32 	%r1116, [%rd269], 2;
	min.s32 	%r1117, %r1116, %r163;
	mad.lo.s32 	%r1118, %r189, %r299, %r1117;
	shr.u32 	%r1119, %r1118, 31;
	add.s32 	%r1120, %r1118, %r1119;
	shr.s32 	%r1121, %r1120, 1;
	shr.u64 	%rd270, %rd14, 32;
	mul.wide.s32 	%rd271, %r1121, 16;
	add.s64 	%rd272, %rd1, %rd271;
	cvt.u32.u64	%r1122, %rd14;
	cvt.u32.u64	%r1123, %rd270;
	st.global.v4.u32 	[%rd272], {%r1100, %r1099, %r1123, %r1122};

BB1_220:
	add.s32 	%r191, %r1830, 1024;
	setp.ge.s32	%p147, %r191, %r8;
	@%p147 bra 	BB1_227;

	add.s32 	%r1132, %r191, %r19;
	mul.wide.s32 	%rd282, %r1132, 8;
	add.s64 	%rd283, %rd2, %rd282;
	ld.global.v2.u32 	{%r1133, %r1134}, [%rd283];
	or.b32  	%r1135, %r1133, %r1134;
	setp.eq.s32	%p148, %r1135, 0;
	@%p148 bra 	BB1_227;

	and.b32  	%r1136, %r1133, 131040;
	and.b32  	%r1137, %r1133, 31;
	xor.b32  	%r1138, %r1137, 1;
	shr.u32 	%r1139, %r1136, 3;
	add.s32 	%r1141, %r302, %r1139;
	mov.u32 	%r1142, 1;
	shl.b32 	%r1143, %r1142, %r1138;
	ld.shared.u32 	%r1144, [%r1141];
	and.b32  	%r1145, %r1144, %r1143;
	setp.eq.s32	%p149, %r1145, 0;
	@%p149 bra 	BB1_227;

	bfe.u32 	%r194, %r1134, 17, 12;
	shl.b32 	%r1146, %r194, 3;
	add.s32 	%r1148, %r302, %r1146;
	add.s32 	%r195, %r1148, 16384;
	atom.shared.exch.b64 	%rd15, [%r195], 0;
	setp.eq.s64	%p150, %rd15, 0;
	@%p150 bra 	BB1_225;
	bra.uni 	BB1_224;

BB1_225:
	cvt.u64.u32	%rd289, %r1134;
	cvt.u64.u32	%rd290, %r1133;
	bfi.b64 	%rd291, %rd289, %rd290, 32, 32;
	add.s32 	%r1785, %r1148, 16384;
	atom.shared.cas.b64 	%rd293, [%r1785], %rd105, %rd291;
	setp.eq.s64	%p151, %rd293, 0;
	@%p151 bra 	BB1_227;

	add.s32 	%r1158, %r194, %r17;
	mul.wide.s32 	%rd294, %r1158, 4;
	add.s64 	%rd295, %rd3, %rd294;
	atom.global.add.u32 	%r1159, [%rd295], 2;
	min.s32 	%r1160, %r1159, %r163;
	mad.lo.s32 	%r1161, %r194, %r299, %r1160;
	shr.u32 	%r1162, %r1161, 31;
	add.s32 	%r1163, %r1161, %r1162;
	shr.s32 	%r1164, %r1163, 1;
	mul.wide.s32 	%rd296, %r1164, 16;
	add.s64 	%rd297, %rd1, %rd296;
	mov.u32 	%r1165, 0;
	st.global.v4.u32 	[%rd297], {%r1134, %r1133, %r1165, %r1165};
	bra.uni 	BB1_227;

BB1_224:
	add.s32 	%r1149, %r194, %r17;
	mul.wide.s32 	%rd284, %r1149, 4;
	add.s64 	%rd285, %rd3, %rd284;
	atom.global.add.u32 	%r1150, [%rd285], 2;
	min.s32 	%r1151, %r1150, %r163;
	mad.lo.s32 	%r1152, %r194, %r299, %r1151;
	shr.u32 	%r1153, %r1152, 31;
	add.s32 	%r1154, %r1152, %r1153;
	shr.s32 	%r1155, %r1154, 1;
	shr.u64 	%rd286, %rd15, 32;
	mul.wide.s32 	%rd287, %r1155, 16;
	add.s64 	%rd288, %rd1, %rd287;
	cvt.u32.u64	%r1156, %rd15;
	cvt.u32.u64	%r1157, %rd286;
	st.global.v4.u32 	[%rd288], {%r1134, %r1133, %r1157, %r1156};

BB1_227:
	add.s32 	%r196, %r1830, 2048;
	setp.ge.s32	%p152, %r196, %r8;
	@%p152 bra 	BB1_234;

	add.s32 	%r1166, %r196, %r19;
	mul.wide.s32 	%rd298, %r1166, 8;
	add.s64 	%rd299, %rd2, %rd298;
	ld.global.v2.u32 	{%r1167, %r1168}, [%rd299];
	or.b32  	%r1169, %r1167, %r1168;
	setp.eq.s32	%p153, %r1169, 0;
	@%p153 bra 	BB1_234;

	and.b32  	%r1170, %r1167, 131040;
	and.b32  	%r1171, %r1167, 31;
	xor.b32  	%r1172, %r1171, 1;
	shr.u32 	%r1173, %r1170, 3;
	add.s32 	%r1175, %r302, %r1173;
	mov.u32 	%r1176, 1;
	shl.b32 	%r1177, %r1176, %r1172;
	ld.shared.u32 	%r1178, [%r1175];
	and.b32  	%r1179, %r1178, %r1177;
	setp.eq.s32	%p154, %r1179, 0;
	@%p154 bra 	BB1_234;

	bfe.u32 	%r199, %r1168, 17, 12;
	shl.b32 	%r1180, %r199, 3;
	add.s32 	%r1182, %r302, %r1180;
	add.s32 	%r200, %r1182, 16384;
	atom.shared.exch.b64 	%rd16, [%r200], 0;
	setp.eq.s64	%p155, %rd16, 0;
	@%p155 bra 	BB1_232;
	bra.uni 	BB1_231;

BB1_232:
	cvt.u64.u32	%rd305, %r1168;
	cvt.u64.u32	%rd306, %r1167;
	bfi.b64 	%rd307, %rd305, %rd306, 32, 32;
	add.s32 	%r1786, %r1182, 16384;
	atom.shared.cas.b64 	%rd309, [%r1786], %rd105, %rd307;
	setp.eq.s64	%p156, %rd309, 0;
	@%p156 bra 	BB1_234;

	add.s32 	%r1192, %r199, %r17;
	mul.wide.s32 	%rd310, %r1192, 4;
	add.s64 	%rd311, %rd3, %rd310;
	atom.global.add.u32 	%r1193, [%rd311], 2;
	min.s32 	%r1194, %r1193, %r163;
	mad.lo.s32 	%r1195, %r199, %r299, %r1194;
	shr.u32 	%r1196, %r1195, 31;
	add.s32 	%r1197, %r1195, %r1196;
	shr.s32 	%r1198, %r1197, 1;
	mul.wide.s32 	%rd312, %r1198, 16;
	add.s64 	%rd313, %rd1, %rd312;
	mov.u32 	%r1199, 0;
	st.global.v4.u32 	[%rd313], {%r1168, %r1167, %r1199, %r1199};
	bra.uni 	BB1_234;

BB1_231:
	add.s32 	%r1183, %r199, %r17;
	mul.wide.s32 	%rd300, %r1183, 4;
	add.s64 	%rd301, %rd3, %rd300;
	atom.global.add.u32 	%r1184, [%rd301], 2;
	min.s32 	%r1185, %r1184, %r163;
	mad.lo.s32 	%r1186, %r199, %r299, %r1185;
	shr.u32 	%r1187, %r1186, 31;
	add.s32 	%r1188, %r1186, %r1187;
	shr.s32 	%r1189, %r1188, 1;
	shr.u64 	%rd302, %rd16, 32;
	mul.wide.s32 	%rd303, %r1189, 16;
	add.s64 	%rd304, %rd1, %rd303;
	cvt.u32.u64	%r1190, %rd16;
	cvt.u32.u64	%r1191, %rd302;
	st.global.v4.u32 	[%rd304], {%r1168, %r1167, %r1191, %r1190};

BB1_234:
	add.s32 	%r201, %r1830, 3072;
	setp.ge.s32	%p157, %r201, %r8;
	@%p157 bra 	BB1_241;

	add.s32 	%r1200, %r201, %r19;
	mul.wide.s32 	%rd314, %r1200, 8;
	add.s64 	%rd315, %rd2, %rd314;
	ld.global.v2.u32 	{%r1201, %r1202}, [%rd315];
	or.b32  	%r1203, %r1201, %r1202;
	setp.eq.s32	%p158, %r1203, 0;
	@%p158 bra 	BB1_241;

	and.b32  	%r1204, %r1201, 131040;
	and.b32  	%r1205, %r1201, 31;
	xor.b32  	%r1206, %r1205, 1;
	shr.u32 	%r1207, %r1204, 3;
	add.s32 	%r1209, %r302, %r1207;
	mov.u32 	%r1210, 1;
	shl.b32 	%r1211, %r1210, %r1206;
	ld.shared.u32 	%r1212, [%r1209];
	and.b32  	%r1213, %r1212, %r1211;
	setp.eq.s32	%p159, %r1213, 0;
	@%p159 bra 	BB1_241;

	bfe.u32 	%r204, %r1202, 17, 12;
	shl.b32 	%r1214, %r204, 3;
	add.s32 	%r1216, %r302, %r1214;
	add.s32 	%r205, %r1216, 16384;
	atom.shared.exch.b64 	%rd17, [%r205], 0;
	setp.eq.s64	%p160, %rd17, 0;
	@%p160 bra 	BB1_239;
	bra.uni 	BB1_238;

BB1_239:
	cvt.u64.u32	%rd321, %r1202;
	cvt.u64.u32	%rd322, %r1201;
	bfi.b64 	%rd323, %rd321, %rd322, 32, 32;
	add.s32 	%r1787, %r1216, 16384;
	atom.shared.cas.b64 	%rd325, [%r1787], %rd105, %rd323;
	setp.eq.s64	%p161, %rd325, 0;
	@%p161 bra 	BB1_241;

	add.s32 	%r1226, %r204, %r17;
	mul.wide.s32 	%rd326, %r1226, 4;
	add.s64 	%rd327, %rd3, %rd326;
	atom.global.add.u32 	%r1227, [%rd327], 2;
	min.s32 	%r1228, %r1227, %r163;
	mad.lo.s32 	%r1229, %r204, %r299, %r1228;
	shr.u32 	%r1230, %r1229, 31;
	add.s32 	%r1231, %r1229, %r1230;
	shr.s32 	%r1232, %r1231, 1;
	mul.wide.s32 	%rd328, %r1232, 16;
	add.s64 	%rd329, %rd1, %rd328;
	mov.u32 	%r1233, 0;
	st.global.v4.u32 	[%rd329], {%r1202, %r1201, %r1233, %r1233};
	bra.uni 	BB1_241;

BB1_238:
	add.s32 	%r1217, %r204, %r17;
	mul.wide.s32 	%rd316, %r1217, 4;
	add.s64 	%rd317, %rd3, %rd316;
	atom.global.add.u32 	%r1218, [%rd317], 2;
	min.s32 	%r1219, %r1218, %r163;
	mad.lo.s32 	%r1220, %r204, %r299, %r1219;
	shr.u32 	%r1221, %r1220, 31;
	add.s32 	%r1222, %r1220, %r1221;
	shr.s32 	%r1223, %r1222, 1;
	shr.u64 	%rd318, %rd17, 32;
	mul.wide.s32 	%rd319, %r1223, 16;
	add.s64 	%rd320, %rd1, %rd319;
	cvt.u32.u64	%r1224, %rd17;
	cvt.u32.u64	%r1225, %rd318;
	st.global.v4.u32 	[%rd320], {%r1202, %r1201, %r1225, %r1224};

BB1_241:
	add.s32 	%r1827, %r1827, 4;
	add.s32 	%r1830, %r1830, 4096;
	setp.lt.s32	%p162, %r1827, %r10;
	@%p162 bra 	BB1_213;

BB1_242:
	@%p42 bra 	BB1_301;

	add.s32 	%r208, %r299, -4;
	mov.u32 	%r1238, 1;
	max.s32 	%r209, %r13, %r1238;
	and.b32  	%r1237, %r209, 3;
	mov.u32 	%r1832, 0;
	setp.eq.s32	%p164, %r1237, 0;
	@%p164 bra 	BB1_270;

	setp.eq.s32	%p165, %r1237, 1;
	@%p165 bra 	BB1_262;

	setp.eq.s32	%p166, %r1237, 2;
	@%p166 bra 	BB1_254;

	setp.ge.s32	%p167, %r1, %r11;
	@%p167 bra 	BB1_247;

	add.s32 	%r1241, %r1, %r20;
	mul.wide.s32 	%rd330, %r1241, 8;
	add.s64 	%rd331, %rd2, %rd330;
	ld.global.v2.u32 	{%r1242, %r1243}, [%rd331];
	or.b32  	%r1244, %r1242, %r1243;
	setp.eq.s32	%p168, %r1244, 0;
	mov.u32 	%r1832, %r1238;
	@%p168 bra 	BB1_254;

	and.b32  	%r1246, %r1242, 131040;
	and.b32  	%r1247, %r1242, 31;
	xor.b32  	%r1248, %r1247, 1;
	shr.u32 	%r1249, %r1246, 3;
	add.s32 	%r1251, %r302, %r1249;
	mov.u32 	%r1832, 1;
	shl.b32 	%r1252, %r1832, %r1248;
	ld.shared.u32 	%r1253, [%r1251];
	and.b32  	%r1254, %r1253, %r1252;
	setp.eq.s32	%p169, %r1254, 0;
	@%p169 bra 	BB1_254;

	bfe.u32 	%r212, %r1243, 17, 12;
	shl.b32 	%r1255, %r212, 3;
	add.s32 	%r1257, %r302, %r1255;
	add.s32 	%r213, %r1257, 16384;
	atom.shared.exch.b64 	%rd18, [%r213], 0;
	setp.eq.s64	%p170, %rd18, 0;
	@%p170 bra 	BB1_252;

	add.s32 	%r1259, %r212, %r17;
	mul.wide.s32 	%rd332, %r1259, 4;
	add.s64 	%rd333, %rd3, %rd332;
	atom.global.add.u32 	%r1260, [%rd333], 2;
	min.s32 	%r1261, %r1260, %r208;
	mad.lo.s32 	%r1262, %r212, %r299, %r1261;
	shr.u32 	%r1263, %r1262, 31;
	add.s32 	%r1264, %r1262, %r1263;
	shr.s32 	%r1265, %r1264, 1;
	shr.u64 	%rd334, %rd18, 32;
	mul.wide.s32 	%rd335, %r1265, 16;
	add.s64 	%rd336, %rd1, %rd335;
	cvt.u32.u64	%r1266, %rd18;
	cvt.u32.u64	%r1267, %rd334;
	st.global.v4.u32 	[%rd336], {%r1243, %r1242, %r1267, %r1266};
	bra.uni 	BB1_254;

BB1_247:
	mov.u32 	%r1832, %r1238;

BB1_254:
	shl.b32 	%r1278, %r1832, 10;
	add.s32 	%r215, %r1278, %r1;
	setp.ge.s32	%p172, %r215, %r11;
	@%p172 bra 	BB1_261;

	add.s32 	%r1279, %r215, %r20;
	mul.wide.s32 	%rd346, %r1279, 8;
	add.s64 	%rd347, %rd2, %rd346;
	ld.global.v2.u32 	{%r1280, %r1281}, [%rd347];
	or.b32  	%r1282, %r1280, %r1281;
	setp.eq.s32	%p173, %r1282, 0;
	@%p173 bra 	BB1_261;

	and.b32  	%r1283, %r1280, 131040;
	and.b32  	%r1284, %r1280, 31;
	xor.b32  	%r1285, %r1284, 1;
	shr.u32 	%r1286, %r1283, 3;
	add.s32 	%r1288, %r302, %r1286;
	mov.u32 	%r1289, 1;
	shl.b32 	%r1290, %r1289, %r1285;
	ld.shared.u32 	%r1291, [%r1288];
	and.b32  	%r1292, %r1291, %r1290;
	setp.eq.s32	%p174, %r1292, 0;
	@%p174 bra 	BB1_261;

	bfe.u32 	%r218, %r1281, 17, 12;
	shl.b32 	%r1293, %r218, 3;
	add.s32 	%r1295, %r302, %r1293;
	add.s32 	%r219, %r1295, 16384;
	atom.shared.exch.b64 	%rd19, [%r219], 0;
	setp.eq.s64	%p175, %rd19, 0;
	@%p175 bra 	BB1_259;

	add.s32 	%r1296, %r218, %r17;
	mul.wide.s32 	%rd348, %r1296, 4;
	add.s64 	%rd349, %rd3, %rd348;
	atom.global.add.u32 	%r1297, [%rd349], 2;
	min.s32 	%r1298, %r1297, %r208;
	mad.lo.s32 	%r1299, %r218, %r299, %r1298;
	shr.u32 	%r1300, %r1299, 31;
	add.s32 	%r1301, %r1299, %r1300;
	shr.s32 	%r1302, %r1301, 1;
	shr.u64 	%rd350, %rd19, 32;
	mul.wide.s32 	%rd351, %r1302, 16;
	add.s64 	%rd352, %rd1, %rd351;
	cvt.u32.u64	%r1303, %rd19;
	cvt.u32.u64	%r1304, %rd350;
	st.global.v4.u32 	[%rd352], {%r1281, %r1280, %r1304, %r1303};
	bra.uni 	BB1_261;

BB1_259:
	cvt.u64.u32	%rd353, %r1281;
	cvt.u64.u32	%rd354, %r1280;
	bfi.b64 	%rd355, %rd353, %rd354, 32, 32;
	add.s32 	%r1789, %r1295, 16384;
	atom.shared.cas.b64 	%rd357, [%r1789], %rd105, %rd355;
	setp.eq.s64	%p176, %rd357, 0;
	@%p176 bra 	BB1_261;

	add.s32 	%r1305, %r218, %r17;
	mul.wide.s32 	%rd358, %r1305, 4;
	add.s64 	%rd359, %rd3, %rd358;
	atom.global.add.u32 	%r1306, [%rd359], 2;
	min.s32 	%r1307, %r1306, %r208;
	mad.lo.s32 	%r1308, %r218, %r299, %r1307;
	shr.u32 	%r1309, %r1308, 31;
	add.s32 	%r1310, %r1308, %r1309;
	shr.s32 	%r1311, %r1310, 1;
	mul.wide.s32 	%rd360, %r1311, 16;
	add.s64 	%rd361, %rd1, %rd360;
	mov.u32 	%r1312, 0;
	st.global.v4.u32 	[%rd361], {%r1281, %r1280, %r1312, %r1312};

BB1_261:
	add.s32 	%r1832, %r1832, 1;

BB1_262:
	shl.b32 	%r1313, %r1832, 10;
	add.s32 	%r222, %r1313, %r1;
	setp.ge.s32	%p177, %r222, %r11;
	@%p177 bra 	BB1_269;

	add.s32 	%r1314, %r222, %r20;
	mul.wide.s32 	%rd362, %r1314, 8;
	add.s64 	%rd363, %rd2, %rd362;
	ld.global.v2.u32 	{%r1315, %r1316}, [%rd363];
	or.b32  	%r1317, %r1315, %r1316;
	setp.eq.s32	%p178, %r1317, 0;
	@%p178 bra 	BB1_269;

	and.b32  	%r1318, %r1315, 131040;
	and.b32  	%r1319, %r1315, 31;
	xor.b32  	%r1320, %r1319, 1;
	shr.u32 	%r1321, %r1318, 3;
	add.s32 	%r1323, %r302, %r1321;
	mov.u32 	%r1324, 1;
	shl.b32 	%r1325, %r1324, %r1320;
	ld.shared.u32 	%r1326, [%r1323];
	and.b32  	%r1327, %r1326, %r1325;
	setp.eq.s32	%p179, %r1327, 0;
	@%p179 bra 	BB1_269;

	bfe.u32 	%r225, %r1316, 17, 12;
	shl.b32 	%r1328, %r225, 3;
	add.s32 	%r1330, %r302, %r1328;
	add.s32 	%r226, %r1330, 16384;
	atom.shared.exch.b64 	%rd20, [%r226], 0;
	setp.eq.s64	%p180, %rd20, 0;
	@%p180 bra 	BB1_267;

	add.s32 	%r1331, %r225, %r17;
	mul.wide.s32 	%rd364, %r1331, 4;
	add.s64 	%rd365, %rd3, %rd364;
	atom.global.add.u32 	%r1332, [%rd365], 2;
	min.s32 	%r1333, %r1332, %r208;
	mad.lo.s32 	%r1334, %r225, %r299, %r1333;
	shr.u32 	%r1335, %r1334, 31;
	add.s32 	%r1336, %r1334, %r1335;
	shr.s32 	%r1337, %r1336, 1;
	shr.u64 	%rd366, %rd20, 32;
	mul.wide.s32 	%rd367, %r1337, 16;
	add.s64 	%rd368, %rd1, %rd367;
	cvt.u32.u64	%r1338, %rd20;
	cvt.u32.u64	%r1339, %rd366;
	st.global.v4.u32 	[%rd368], {%r1316, %r1315, %r1339, %r1338};
	bra.uni 	BB1_269;

BB1_267:
	cvt.u64.u32	%rd369, %r1316;
	cvt.u64.u32	%rd370, %r1315;
	bfi.b64 	%rd371, %rd369, %rd370, 32, 32;
	add.s32 	%r1790, %r1330, 16384;
	atom.shared.cas.b64 	%rd373, [%r1790], %rd105, %rd371;
	setp.eq.s64	%p181, %rd373, 0;
	@%p181 bra 	BB1_269;

	add.s32 	%r1340, %r225, %r17;
	mul.wide.s32 	%rd374, %r1340, 4;
	add.s64 	%rd375, %rd3, %rd374;
	atom.global.add.u32 	%r1341, [%rd375], 2;
	min.s32 	%r1342, %r1341, %r208;
	mad.lo.s32 	%r1343, %r225, %r299, %r1342;
	shr.u32 	%r1344, %r1343, 31;
	add.s32 	%r1345, %r1343, %r1344;
	shr.s32 	%r1346, %r1345, 1;
	mul.wide.s32 	%rd376, %r1346, 16;
	add.s64 	%rd377, %rd1, %rd376;
	mov.u32 	%r1347, 0;
	st.global.v4.u32 	[%rd377], {%r1316, %r1315, %r1347, %r1347};

BB1_269:
	add.s32 	%r1832, %r1832, 1;

BB1_270:
	setp.lt.u32	%p182, %r209, 4;
	@%p182 bra 	BB1_301;

	mad.lo.s32 	%r1835, %r1832, 1024, %r1;

BB1_272:
	setp.ge.s32	%p183, %r1835, %r11;
	@%p183 bra 	BB1_279;

	add.s32 	%r1348, %r1835, %r20;
	mul.wide.s32 	%rd378, %r1348, 8;
	add.s64 	%rd379, %rd2, %rd378;
	ld.global.v2.u32 	{%r1349, %r1350}, [%rd379];
	or.b32  	%r1351, %r1349, %r1350;
	setp.eq.s32	%p184, %r1351, 0;
	@%p184 bra 	BB1_279;

	and.b32  	%r1352, %r1349, 131040;
	and.b32  	%r1353, %r1349, 31;
	xor.b32  	%r1354, %r1353, 1;
	shr.u32 	%r1355, %r1352, 3;
	add.s32 	%r1357, %r302, %r1355;
	mov.u32 	%r1358, 1;
	shl.b32 	%r1359, %r1358, %r1354;
	ld.shared.u32 	%r1360, [%r1357];
	and.b32  	%r1361, %r1360, %r1359;
	setp.eq.s32	%p185, %r1361, 0;
	@%p185 bra 	BB1_279;

	bfe.u32 	%r234, %r1350, 17, 12;
	shl.b32 	%r1362, %r234, 3;
	add.s32 	%r1364, %r302, %r1362;
	add.s32 	%r235, %r1364, 16384;
	atom.shared.exch.b64 	%rd21, [%r235], 0;
	setp.eq.s64	%p186, %rd21, 0;
	@%p186 bra 	BB1_277;
	bra.uni 	BB1_276;

BB1_277:
	cvt.u64.u32	%rd385, %r1350;
	cvt.u64.u32	%rd386, %r1349;
	bfi.b64 	%rd387, %rd385, %rd386, 32, 32;
	add.s32 	%r1791, %r1364, 16384;
	atom.shared.cas.b64 	%rd389, [%r1791], %rd105, %rd387;
	setp.eq.s64	%p187, %rd389, 0;
	@%p187 bra 	BB1_279;

	add.s32 	%r1374, %r234, %r17;
	mul.wide.s32 	%rd390, %r1374, 4;
	add.s64 	%rd391, %rd3, %rd390;
	atom.global.add.u32 	%r1375, [%rd391], 2;
	min.s32 	%r1376, %r1375, %r208;
	mad.lo.s32 	%r1377, %r234, %r299, %r1376;
	shr.u32 	%r1378, %r1377, 31;
	add.s32 	%r1379, %r1377, %r1378;
	shr.s32 	%r1380, %r1379, 1;
	mul.wide.s32 	%rd392, %r1380, 16;
	add.s64 	%rd393, %rd1, %rd392;
	mov.u32 	%r1381, 0;
	st.global.v4.u32 	[%rd393], {%r1350, %r1349, %r1381, %r1381};
	bra.uni 	BB1_279;

BB1_276:
	add.s32 	%r1365, %r234, %r17;
	mul.wide.s32 	%rd380, %r1365, 4;
	add.s64 	%rd381, %rd3, %rd380;
	atom.global.add.u32 	%r1366, [%rd381], 2;
	min.s32 	%r1367, %r1366, %r208;
	mad.lo.s32 	%r1368, %r234, %r299, %r1367;
	shr.u32 	%r1369, %r1368, 31;
	add.s32 	%r1370, %r1368, %r1369;
	shr.s32 	%r1371, %r1370, 1;
	shr.u64 	%rd382, %rd21, 32;
	mul.wide.s32 	%rd383, %r1371, 16;
	add.s64 	%rd384, %rd1, %rd383;
	cvt.u32.u64	%r1372, %rd21;
	cvt.u32.u64	%r1373, %rd382;
	st.global.v4.u32 	[%rd384], {%r1350, %r1349, %r1373, %r1372};

BB1_279:
	add.s32 	%r236, %r1835, 1024;
	setp.ge.s32	%p188, %r236, %r11;
	@%p188 bra 	BB1_286;

	add.s32 	%r1382, %r236, %r20;
	mul.wide.s32 	%rd394, %r1382, 8;
	add.s64 	%rd395, %rd2, %rd394;
	ld.global.v2.u32 	{%r1383, %r1384}, [%rd395];
	or.b32  	%r1385, %r1383, %r1384;
	setp.eq.s32	%p189, %r1385, 0;
	@%p189 bra 	BB1_286;

	and.b32  	%r1386, %r1383, 131040;
	and.b32  	%r1387, %r1383, 31;
	xor.b32  	%r1388, %r1387, 1;
	shr.u32 	%r1389, %r1386, 3;
	add.s32 	%r1391, %r302, %r1389;
	mov.u32 	%r1392, 1;
	shl.b32 	%r1393, %r1392, %r1388;
	ld.shared.u32 	%r1394, [%r1391];
	and.b32  	%r1395, %r1394, %r1393;
	setp.eq.s32	%p190, %r1395, 0;
	@%p190 bra 	BB1_286;

	bfe.u32 	%r239, %r1384, 17, 12;
	shl.b32 	%r1396, %r239, 3;
	add.s32 	%r1398, %r302, %r1396;
	add.s32 	%r240, %r1398, 16384;
	atom.shared.exch.b64 	%rd22, [%r240], 0;
	setp.eq.s64	%p191, %rd22, 0;
	@%p191 bra 	BB1_284;
	bra.uni 	BB1_283;

BB1_284:
	cvt.u64.u32	%rd401, %r1384;
	cvt.u64.u32	%rd402, %r1383;
	bfi.b64 	%rd403, %rd401, %rd402, 32, 32;
	add.s32 	%r1792, %r1398, 16384;
	atom.shared.cas.b64 	%rd405, [%r1792], %rd105, %rd403;
	setp.eq.s64	%p192, %rd405, 0;
	@%p192 bra 	BB1_286;

	add.s32 	%r1408, %r239, %r17;
	mul.wide.s32 	%rd406, %r1408, 4;
	add.s64 	%rd407, %rd3, %rd406;
	atom.global.add.u32 	%r1409, [%rd407], 2;
	min.s32 	%r1410, %r1409, %r208;
	mad.lo.s32 	%r1411, %r239, %r299, %r1410;
	shr.u32 	%r1412, %r1411, 31;
	add.s32 	%r1413, %r1411, %r1412;
	shr.s32 	%r1414, %r1413, 1;
	mul.wide.s32 	%rd408, %r1414, 16;
	add.s64 	%rd409, %rd1, %rd408;
	mov.u32 	%r1415, 0;
	st.global.v4.u32 	[%rd409], {%r1384, %r1383, %r1415, %r1415};
	bra.uni 	BB1_286;

BB1_283:
	add.s32 	%r1399, %r239, %r17;
	mul.wide.s32 	%rd396, %r1399, 4;
	add.s64 	%rd397, %rd3, %rd396;
	atom.global.add.u32 	%r1400, [%rd397], 2;
	min.s32 	%r1401, %r1400, %r208;
	mad.lo.s32 	%r1402, %r239, %r299, %r1401;
	shr.u32 	%r1403, %r1402, 31;
	add.s32 	%r1404, %r1402, %r1403;
	shr.s32 	%r1405, %r1404, 1;
	shr.u64 	%rd398, %rd22, 32;
	mul.wide.s32 	%rd399, %r1405, 16;
	add.s64 	%rd400, %rd1, %rd399;
	cvt.u32.u64	%r1406, %rd22;
	cvt.u32.u64	%r1407, %rd398;
	st.global.v4.u32 	[%rd400], {%r1384, %r1383, %r1407, %r1406};

BB1_286:
	add.s32 	%r241, %r1835, 2048;
	setp.ge.s32	%p193, %r241, %r11;
	@%p193 bra 	BB1_293;

	add.s32 	%r1416, %r241, %r20;
	mul.wide.s32 	%rd410, %r1416, 8;
	add.s64 	%rd411, %rd2, %rd410;
	ld.global.v2.u32 	{%r1417, %r1418}, [%rd411];
	or.b32  	%r1419, %r1417, %r1418;
	setp.eq.s32	%p194, %r1419, 0;
	@%p194 bra 	BB1_293;

	and.b32  	%r1420, %r1417, 131040;
	and.b32  	%r1421, %r1417, 31;
	xor.b32  	%r1422, %r1421, 1;
	shr.u32 	%r1423, %r1420, 3;
	add.s32 	%r1425, %r302, %r1423;
	mov.u32 	%r1426, 1;
	shl.b32 	%r1427, %r1426, %r1422;
	ld.shared.u32 	%r1428, [%r1425];
	and.b32  	%r1429, %r1428, %r1427;
	setp.eq.s32	%p195, %r1429, 0;
	@%p195 bra 	BB1_293;

	bfe.u32 	%r244, %r1418, 17, 12;
	shl.b32 	%r1430, %r244, 3;
	add.s32 	%r1432, %r302, %r1430;
	add.s32 	%r245, %r1432, 16384;
	atom.shared.exch.b64 	%rd23, [%r245], 0;
	setp.eq.s64	%p196, %rd23, 0;
	@%p196 bra 	BB1_291;
	bra.uni 	BB1_290;

BB1_291:
	cvt.u64.u32	%rd417, %r1418;
	cvt.u64.u32	%rd418, %r1417;
	bfi.b64 	%rd419, %rd417, %rd418, 32, 32;
	add.s32 	%r1793, %r1432, 16384;
	atom.shared.cas.b64 	%rd421, [%r1793], %rd105, %rd419;
	setp.eq.s64	%p197, %rd421, 0;
	@%p197 bra 	BB1_293;

	add.s32 	%r1442, %r244, %r17;
	mul.wide.s32 	%rd422, %r1442, 4;
	add.s64 	%rd423, %rd3, %rd422;
	atom.global.add.u32 	%r1443, [%rd423], 2;
	min.s32 	%r1444, %r1443, %r208;
	mad.lo.s32 	%r1445, %r244, %r299, %r1444;
	shr.u32 	%r1446, %r1445, 31;
	add.s32 	%r1447, %r1445, %r1446;
	shr.s32 	%r1448, %r1447, 1;
	mul.wide.s32 	%rd424, %r1448, 16;
	add.s64 	%rd425, %rd1, %rd424;
	mov.u32 	%r1449, 0;
	st.global.v4.u32 	[%rd425], {%r1418, %r1417, %r1449, %r1449};
	bra.uni 	BB1_293;

BB1_290:
	add.s32 	%r1433, %r244, %r17;
	mul.wide.s32 	%rd412, %r1433, 4;
	add.s64 	%rd413, %rd3, %rd412;
	atom.global.add.u32 	%r1434, [%rd413], 2;
	min.s32 	%r1435, %r1434, %r208;
	mad.lo.s32 	%r1436, %r244, %r299, %r1435;
	shr.u32 	%r1437, %r1436, 31;
	add.s32 	%r1438, %r1436, %r1437;
	shr.s32 	%r1439, %r1438, 1;
	shr.u64 	%rd414, %rd23, 32;
	mul.wide.s32 	%rd415, %r1439, 16;
	add.s64 	%rd416, %rd1, %rd415;
	cvt.u32.u64	%r1440, %rd23;
	cvt.u32.u64	%r1441, %rd414;
	st.global.v4.u32 	[%rd416], {%r1418, %r1417, %r1441, %r1440};

BB1_293:
	add.s32 	%r246, %r1835, 3072;
	setp.ge.s32	%p198, %r246, %r11;
	@%p198 bra 	BB1_300;

	add.s32 	%r1450, %r246, %r20;
	mul.wide.s32 	%rd426, %r1450, 8;
	add.s64 	%rd427, %rd2, %rd426;
	ld.global.v2.u32 	{%r1451, %r1452}, [%rd427];
	or.b32  	%r1453, %r1451, %r1452;
	setp.eq.s32	%p199, %r1453, 0;
	@%p199 bra 	BB1_300;

	and.b32  	%r1454, %r1451, 131040;
	and.b32  	%r1455, %r1451, 31;
	xor.b32  	%r1456, %r1455, 1;
	shr.u32 	%r1457, %r1454, 3;
	add.s32 	%r1459, %r302, %r1457;
	mov.u32 	%r1460, 1;
	shl.b32 	%r1461, %r1460, %r1456;
	ld.shared.u32 	%r1462, [%r1459];
	and.b32  	%r1463, %r1462, %r1461;
	setp.eq.s32	%p200, %r1463, 0;
	@%p200 bra 	BB1_300;

	bfe.u32 	%r249, %r1452, 17, 12;
	shl.b32 	%r1464, %r249, 3;
	add.s32 	%r1466, %r302, %r1464;
	add.s32 	%r250, %r1466, 16384;
	atom.shared.exch.b64 	%rd24, [%r250], 0;
	setp.eq.s64	%p201, %rd24, 0;
	@%p201 bra 	BB1_298;
	bra.uni 	BB1_297;

BB1_298:
	cvt.u64.u32	%rd433, %r1452;
	cvt.u64.u32	%rd434, %r1451;
	bfi.b64 	%rd435, %rd433, %rd434, 32, 32;
	add.s32 	%r1794, %r1466, 16384;
	atom.shared.cas.b64 	%rd437, [%r1794], %rd105, %rd435;
	setp.eq.s64	%p202, %rd437, 0;
	@%p202 bra 	BB1_300;

	add.s32 	%r1476, %r249, %r17;
	mul.wide.s32 	%rd438, %r1476, 4;
	add.s64 	%rd439, %rd3, %rd438;
	atom.global.add.u32 	%r1477, [%rd439], 2;
	min.s32 	%r1478, %r1477, %r208;
	mad.lo.s32 	%r1479, %r249, %r299, %r1478;
	shr.u32 	%r1480, %r1479, 31;
	add.s32 	%r1481, %r1479, %r1480;
	shr.s32 	%r1482, %r1481, 1;
	mul.wide.s32 	%rd440, %r1482, 16;
	add.s64 	%rd441, %rd1, %rd440;
	mov.u32 	%r1483, 0;
	st.global.v4.u32 	[%rd441], {%r1452, %r1451, %r1483, %r1483};
	bra.uni 	BB1_300;

BB1_297:
	add.s32 	%r1467, %r249, %r17;
	mul.wide.s32 	%rd428, %r1467, 4;
	add.s64 	%rd429, %rd3, %rd428;
	atom.global.add.u32 	%r1468, [%rd429], 2;
	min.s32 	%r1469, %r1468, %r208;
	mad.lo.s32 	%r1470, %r249, %r299, %r1469;
	shr.u32 	%r1471, %r1470, 31;
	add.s32 	%r1472, %r1470, %r1471;
	shr.s32 	%r1473, %r1472, 1;
	shr.u64 	%rd430, %rd24, 32;
	mul.wide.s32 	%rd431, %r1473, 16;
	add.s64 	%rd432, %rd1, %rd431;
	cvt.u32.u64	%r1474, %rd24;
	cvt.u32.u64	%r1475, %rd430;
	st.global.v4.u32 	[%rd432], {%r1452, %r1451, %r1475, %r1474};

BB1_300:
	add.s32 	%r1832, %r1832, 4;
	add.s32 	%r1835, %r1835, 4096;
	setp.lt.s32	%p203, %r1832, %r13;
	@%p203 bra 	BB1_272;

BB1_301:
	add.s32 	%r253, %r299, -4;
	@%p62 bra 	BB1_360;

	mov.u32 	%r1488, 1;
	max.s32 	%r254, %r16, %r1488;
	and.b32  	%r1487, %r254, 3;
	setp.eq.s32	%p205, %r1487, 0;
	mov.u32 	%r1841, %r304;
	@%p205 bra 	BB1_329;

	setp.eq.s32	%p206, %r1487, 1;
	mov.u32 	%r1838, %r304;
	@%p206 bra 	BB1_321;

	setp.eq.s32	%p207, %r1487, 2;
	mov.u32 	%r1837, %r304;
	@%p207 bra 	BB1_313;

	setp.ge.s32	%p208, %r1, %r14;
	@%p208 bra 	BB1_306;

	add.s32 	%r1491, %r1, %r21;
	mul.wide.s32 	%rd442, %r1491, 8;
	add.s64 	%rd443, %rd2, %rd442;
	ld.global.v2.u32 	{%r1492, %r1493}, [%rd443];
	or.b32  	%r1494, %r1492, %r1493;
	setp.eq.s32	%p209, %r1494, 0;
	mov.u32 	%r1837, %r1488;
	@%p209 bra 	BB1_313;

	and.b32  	%r1496, %r1492, 131040;
	and.b32  	%r1497, %r1492, 31;
	xor.b32  	%r1498, %r1497, 1;
	shr.u32 	%r1499, %r1496, 3;
	add.s32 	%r1501, %r302, %r1499;
	mov.u32 	%r1837, 1;
	shl.b32 	%r1502, %r1837, %r1498;
	ld.shared.u32 	%r1503, [%r1501];
	and.b32  	%r1504, %r1503, %r1502;
	setp.eq.s32	%p210, %r1504, 0;
	@%p210 bra 	BB1_313;

	bfe.u32 	%r257, %r1493, 17, 12;
	shl.b32 	%r1505, %r257, 3;
	add.s32 	%r1507, %r302, %r1505;
	add.s32 	%r258, %r1507, 16384;
	atom.shared.exch.b64 	%rd25, [%r258], 0;
	setp.eq.s64	%p211, %rd25, 0;
	@%p211 bra 	BB1_311;

	add.s32 	%r1509, %r257, %r17;
	mul.wide.s32 	%rd444, %r1509, 4;
	add.s64 	%rd445, %rd3, %rd444;
	atom.global.add.u32 	%r1510, [%rd445], 2;
	min.s32 	%r1511, %r1510, %r253;
	mad.lo.s32 	%r1512, %r257, %r299, %r1511;
	shr.u32 	%r1513, %r1512, 31;
	add.s32 	%r1514, %r1512, %r1513;
	shr.s32 	%r1515, %r1514, 1;
	shr.u64 	%rd446, %rd25, 32;
	mul.wide.s32 	%rd447, %r1515, 16;
	add.s64 	%rd448, %rd1, %rd447;
	cvt.u32.u64	%r1516, %rd25;
	cvt.u32.u64	%r1517, %rd446;
	st.global.v4.u32 	[%rd448], {%r1493, %r1492, %r1517, %r1516};
	bra.uni 	BB1_313;

BB1_306:
	mov.u32 	%r1837, %r1488;

BB1_313:
	shl.b32 	%r1528, %r1837, 10;
	add.s32 	%r260, %r1528, %r1;
	setp.ge.s32	%p213, %r260, %r14;
	@%p213 bra 	BB1_320;

	add.s32 	%r1529, %r260, %r21;
	mul.wide.s32 	%rd458, %r1529, 8;
	add.s64 	%rd459, %rd2, %rd458;
	ld.global.v2.u32 	{%r1530, %r1531}, [%rd459];
	or.b32  	%r1532, %r1530, %r1531;
	setp.eq.s32	%p214, %r1532, 0;
	@%p214 bra 	BB1_320;

	and.b32  	%r1533, %r1530, 131040;
	and.b32  	%r1534, %r1530, 31;
	xor.b32  	%r1535, %r1534, 1;
	shr.u32 	%r1536, %r1533, 3;
	add.s32 	%r1538, %r302, %r1536;
	mov.u32 	%r1539, 1;
	shl.b32 	%r1540, %r1539, %r1535;
	ld.shared.u32 	%r1541, [%r1538];
	and.b32  	%r1542, %r1541, %r1540;
	setp.eq.s32	%p215, %r1542, 0;
	@%p215 bra 	BB1_320;

	bfe.u32 	%r263, %r1531, 17, 12;
	shl.b32 	%r1543, %r263, 3;
	add.s32 	%r1545, %r302, %r1543;
	add.s32 	%r264, %r1545, 16384;
	atom.shared.exch.b64 	%rd26, [%r264], 0;
	setp.eq.s64	%p216, %rd26, 0;
	@%p216 bra 	BB1_318;

	add.s32 	%r1546, %r263, %r17;
	mul.wide.s32 	%rd460, %r1546, 4;
	add.s64 	%rd461, %rd3, %rd460;
	atom.global.add.u32 	%r1547, [%rd461], 2;
	min.s32 	%r1548, %r1547, %r253;
	mad.lo.s32 	%r1549, %r263, %r299, %r1548;
	shr.u32 	%r1550, %r1549, 31;
	add.s32 	%r1551, %r1549, %r1550;
	shr.s32 	%r1552, %r1551, 1;
	shr.u64 	%rd462, %rd26, 32;
	mul.wide.s32 	%rd463, %r1552, 16;
	add.s64 	%rd464, %rd1, %rd463;
	cvt.u32.u64	%r1553, %rd26;
	cvt.u32.u64	%r1554, %rd462;
	st.global.v4.u32 	[%rd464], {%r1531, %r1530, %r1554, %r1553};
	bra.uni 	BB1_320;

BB1_318:
	cvt.u64.u32	%rd465, %r1531;
	cvt.u64.u32	%rd466, %r1530;
	bfi.b64 	%rd467, %rd465, %rd466, 32, 32;
	add.s32 	%r1796, %r1545, 16384;
	atom.shared.cas.b64 	%rd469, [%r1796], %rd105, %rd467;
	setp.eq.s64	%p217, %rd469, 0;
	@%p217 bra 	BB1_320;

	add.s32 	%r1555, %r263, %r17;
	mul.wide.s32 	%rd470, %r1555, 4;
	add.s64 	%rd471, %rd3, %rd470;
	atom.global.add.u32 	%r1556, [%rd471], 2;
	min.s32 	%r1557, %r1556, %r253;
	mad.lo.s32 	%r1558, %r263, %r299, %r1557;
	shr.u32 	%r1559, %r1558, 31;
	add.s32 	%r1560, %r1558, %r1559;
	shr.s32 	%r1561, %r1560, 1;
	mul.wide.s32 	%rd472, %r1561, 16;
	add.s64 	%rd473, %rd1, %rd472;
	mov.u32 	%r1562, 0;
	st.global.v4.u32 	[%rd473], {%r1531, %r1530, %r1562, %r1562};

BB1_320:
	add.s32 	%r1838, %r1837, 1;

BB1_321:
	shl.b32 	%r1563, %r1838, 10;
	add.s32 	%r267, %r1563, %r1;
	setp.ge.s32	%p218, %r267, %r14;
	@%p218 bra 	BB1_328;

	add.s32 	%r1564, %r267, %r21;
	mul.wide.s32 	%rd474, %r1564, 8;
	add.s64 	%rd475, %rd2, %rd474;
	ld.global.v2.u32 	{%r1565, %r1566}, [%rd475];
	or.b32  	%r1567, %r1565, %r1566;
	setp.eq.s32	%p219, %r1567, 0;
	@%p219 bra 	BB1_328;

	and.b32  	%r1568, %r1565, 131040;
	and.b32  	%r1569, %r1565, 31;
	xor.b32  	%r1570, %r1569, 1;
	shr.u32 	%r1571, %r1568, 3;
	add.s32 	%r1573, %r302, %r1571;
	mov.u32 	%r1574, 1;
	shl.b32 	%r1575, %r1574, %r1570;
	ld.shared.u32 	%r1576, [%r1573];
	and.b32  	%r1577, %r1576, %r1575;
	setp.eq.s32	%p220, %r1577, 0;
	@%p220 bra 	BB1_328;

	bfe.u32 	%r270, %r1566, 17, 12;
	shl.b32 	%r1578, %r270, 3;
	add.s32 	%r1580, %r302, %r1578;
	add.s32 	%r271, %r1580, 16384;
	atom.shared.exch.b64 	%rd27, [%r271], 0;
	setp.eq.s64	%p221, %rd27, 0;
	@%p221 bra 	BB1_326;

	add.s32 	%r1581, %r270, %r17;
	mul.wide.s32 	%rd476, %r1581, 4;
	add.s64 	%rd477, %rd3, %rd476;
	atom.global.add.u32 	%r1582, [%rd477], 2;
	min.s32 	%r1583, %r1582, %r253;
	mad.lo.s32 	%r1584, %r270, %r299, %r1583;
	shr.u32 	%r1585, %r1584, 31;
	add.s32 	%r1586, %r1584, %r1585;
	shr.s32 	%r1587, %r1586, 1;
	shr.u64 	%rd478, %rd27, 32;
	mul.wide.s32 	%rd479, %r1587, 16;
	add.s64 	%rd480, %rd1, %rd479;
	cvt.u32.u64	%r1588, %rd27;
	cvt.u32.u64	%r1589, %rd478;
	st.global.v4.u32 	[%rd480], {%r1566, %r1565, %r1589, %r1588};
	bra.uni 	BB1_328;

BB1_326:
	cvt.u64.u32	%rd481, %r1566;
	cvt.u64.u32	%rd482, %r1565;
	bfi.b64 	%rd483, %rd481, %rd482, 32, 32;
	add.s32 	%r1797, %r1580, 16384;
	atom.shared.cas.b64 	%rd485, [%r1797], %rd105, %rd483;
	setp.eq.s64	%p222, %rd485, 0;
	@%p222 bra 	BB1_328;

	add.s32 	%r1590, %r270, %r17;
	mul.wide.s32 	%rd486, %r1590, 4;
	add.s64 	%rd487, %rd3, %rd486;
	atom.global.add.u32 	%r1591, [%rd487], 2;
	min.s32 	%r1592, %r1591, %r253;
	mad.lo.s32 	%r1593, %r270, %r299, %r1592;
	shr.u32 	%r1594, %r1593, 31;
	add.s32 	%r1595, %r1593, %r1594;
	shr.s32 	%r1596, %r1595, 1;
	mul.wide.s32 	%rd488, %r1596, 16;
	add.s64 	%rd489, %rd1, %rd488;
	mov.u32 	%r1597, 0;
	st.global.v4.u32 	[%rd489], {%r1566, %r1565, %r1597, %r1597};

BB1_328:
	add.s32 	%r1841, %r1838, 1;

BB1_329:
	setp.lt.u32	%p223, %r254, 4;
	@%p223 bra 	BB1_360;

	mad.lo.s32 	%r1840, %r1841, 1024, %r1;

BB1_331:
	setp.ge.s32	%p224, %r1840, %r14;
	@%p224 bra 	BB1_338;

	add.s32 	%r1598, %r1840, %r21;
	mul.wide.s32 	%rd490, %r1598, 8;
	add.s64 	%rd491, %rd2, %rd490;
	ld.global.v2.u32 	{%r1599, %r1600}, [%rd491];
	or.b32  	%r1601, %r1599, %r1600;
	setp.eq.s32	%p225, %r1601, 0;
	@%p225 bra 	BB1_338;

	and.b32  	%r1602, %r1599, 131040;
	and.b32  	%r1603, %r1599, 31;
	xor.b32  	%r1604, %r1603, 1;
	shr.u32 	%r1605, %r1602, 3;
	add.s32 	%r1607, %r302, %r1605;
	mov.u32 	%r1608, 1;
	shl.b32 	%r1609, %r1608, %r1604;
	ld.shared.u32 	%r1610, [%r1607];
	and.b32  	%r1611, %r1610, %r1609;
	setp.eq.s32	%p226, %r1611, 0;
	@%p226 bra 	BB1_338;

	bfe.u32 	%r279, %r1600, 17, 12;
	shl.b32 	%r1612, %r279, 3;
	add.s32 	%r1614, %r302, %r1612;
	add.s32 	%r280, %r1614, 16384;
	atom.shared.exch.b64 	%rd28, [%r280], 0;
	setp.eq.s64	%p227, %rd28, 0;
	@%p227 bra 	BB1_336;
	bra.uni 	BB1_335;

BB1_336:
	cvt.u64.u32	%rd497, %r1600;
	cvt.u64.u32	%rd498, %r1599;
	bfi.b64 	%rd499, %rd497, %rd498, 32, 32;
	add.s32 	%r1798, %r1614, 16384;
	atom.shared.cas.b64 	%rd501, [%r1798], %rd105, %rd499;
	setp.eq.s64	%p228, %rd501, 0;
	@%p228 bra 	BB1_338;

	add.s32 	%r1624, %r279, %r17;
	mul.wide.s32 	%rd502, %r1624, 4;
	add.s64 	%rd503, %rd3, %rd502;
	atom.global.add.u32 	%r1625, [%rd503], 2;
	min.s32 	%r1626, %r1625, %r253;
	mad.lo.s32 	%r1627, %r279, %r299, %r1626;
	shr.u32 	%r1628, %r1627, 31;
	add.s32 	%r1629, %r1627, %r1628;
	shr.s32 	%r1630, %r1629, 1;
	mul.wide.s32 	%rd504, %r1630, 16;
	add.s64 	%rd505, %rd1, %rd504;
	mov.u32 	%r1631, 0;
	st.global.v4.u32 	[%rd505], {%r1600, %r1599, %r1631, %r1631};
	bra.uni 	BB1_338;

BB1_335:
	add.s32 	%r1615, %r279, %r17;
	mul.wide.s32 	%rd492, %r1615, 4;
	add.s64 	%rd493, %rd3, %rd492;
	atom.global.add.u32 	%r1616, [%rd493], 2;
	min.s32 	%r1617, %r1616, %r253;
	mad.lo.s32 	%r1618, %r279, %r299, %r1617;
	shr.u32 	%r1619, %r1618, 31;
	add.s32 	%r1620, %r1618, %r1619;
	shr.s32 	%r1621, %r1620, 1;
	shr.u64 	%rd494, %rd28, 32;
	mul.wide.s32 	%rd495, %r1621, 16;
	add.s64 	%rd496, %rd1, %rd495;
	cvt.u32.u64	%r1622, %rd28;
	cvt.u32.u64	%r1623, %rd494;
	st.global.v4.u32 	[%rd496], {%r1600, %r1599, %r1623, %r1622};

BB1_338:
	add.s32 	%r281, %r1840, 1024;
	setp.ge.s32	%p229, %r281, %r14;
	@%p229 bra 	BB1_345;

	add.s32 	%r1632, %r281, %r21;
	mul.wide.s32 	%rd506, %r1632, 8;
	add.s64 	%rd507, %rd2, %rd506;
	ld.global.v2.u32 	{%r1633, %r1634}, [%rd507];
	or.b32  	%r1635, %r1633, %r1634;
	setp.eq.s32	%p230, %r1635, 0;
	@%p230 bra 	BB1_345;

	and.b32  	%r1636, %r1633, 131040;
	and.b32  	%r1637, %r1633, 31;
	xor.b32  	%r1638, %r1637, 1;
	shr.u32 	%r1639, %r1636, 3;
	add.s32 	%r1641, %r302, %r1639;
	mov.u32 	%r1642, 1;
	shl.b32 	%r1643, %r1642, %r1638;
	ld.shared.u32 	%r1644, [%r1641];
	and.b32  	%r1645, %r1644, %r1643;
	setp.eq.s32	%p231, %r1645, 0;
	@%p231 bra 	BB1_345;

	bfe.u32 	%r284, %r1634, 17, 12;
	shl.b32 	%r1646, %r284, 3;
	add.s32 	%r1648, %r302, %r1646;
	add.s32 	%r285, %r1648, 16384;
	atom.shared.exch.b64 	%rd29, [%r285], 0;
	setp.eq.s64	%p232, %rd29, 0;
	@%p232 bra 	BB1_343;
	bra.uni 	BB1_342;

BB1_343:
	cvt.u64.u32	%rd513, %r1634;
	cvt.u64.u32	%rd514, %r1633;
	bfi.b64 	%rd515, %rd513, %rd514, 32, 32;
	add.s32 	%r1799, %r1648, 16384;
	atom.shared.cas.b64 	%rd517, [%r1799], %rd105, %rd515;
	setp.eq.s64	%p233, %rd517, 0;
	@%p233 bra 	BB1_345;

	add.s32 	%r1658, %r284, %r17;
	mul.wide.s32 	%rd518, %r1658, 4;
	add.s64 	%rd519, %rd3, %rd518;
	atom.global.add.u32 	%r1659, [%rd519], 2;
	min.s32 	%r1660, %r1659, %r253;
	mad.lo.s32 	%r1661, %r284, %r299, %r1660;
	shr.u32 	%r1662, %r1661, 31;
	add.s32 	%r1663, %r1661, %r1662;
	shr.s32 	%r1664, %r1663, 1;
	mul.wide.s32 	%rd520, %r1664, 16;
	add.s64 	%rd521, %rd1, %rd520;
	mov.u32 	%r1665, 0;
	st.global.v4.u32 	[%rd521], {%r1634, %r1633, %r1665, %r1665};
	bra.uni 	BB1_345;

BB1_342:
	add.s32 	%r1649, %r284, %r17;
	mul.wide.s32 	%rd508, %r1649, 4;
	add.s64 	%rd509, %rd3, %rd508;
	atom.global.add.u32 	%r1650, [%rd509], 2;
	min.s32 	%r1651, %r1650, %r253;
	mad.lo.s32 	%r1652, %r284, %r299, %r1651;
	shr.u32 	%r1653, %r1652, 31;
	add.s32 	%r1654, %r1652, %r1653;
	shr.s32 	%r1655, %r1654, 1;
	shr.u64 	%rd510, %rd29, 32;
	mul.wide.s32 	%rd511, %r1655, 16;
	add.s64 	%rd512, %rd1, %rd511;
	cvt.u32.u64	%r1656, %rd29;
	cvt.u32.u64	%r1657, %rd510;
	st.global.v4.u32 	[%rd512], {%r1634, %r1633, %r1657, %r1656};

BB1_345:
	add.s32 	%r286, %r1840, 2048;
	setp.ge.s32	%p234, %r286, %r14;
	@%p234 bra 	BB1_352;

	add.s32 	%r1666, %r286, %r21;
	mul.wide.s32 	%rd522, %r1666, 8;
	add.s64 	%rd523, %rd2, %rd522;
	ld.global.v2.u32 	{%r1667, %r1668}, [%rd523];
	or.b32  	%r1669, %r1667, %r1668;
	setp.eq.s32	%p235, %r1669, 0;
	@%p235 bra 	BB1_352;

	and.b32  	%r1670, %r1667, 131040;
	and.b32  	%r1671, %r1667, 31;
	xor.b32  	%r1672, %r1671, 1;
	shr.u32 	%r1673, %r1670, 3;
	add.s32 	%r1675, %r302, %r1673;
	mov.u32 	%r1676, 1;
	shl.b32 	%r1677, %r1676, %r1672;
	ld.shared.u32 	%r1678, [%r1675];
	and.b32  	%r1679, %r1678, %r1677;
	setp.eq.s32	%p236, %r1679, 0;
	@%p236 bra 	BB1_352;

	bfe.u32 	%r289, %r1668, 17, 12;
	shl.b32 	%r1680, %r289, 3;
	add.s32 	%r1682, %r302, %r1680;
	add.s32 	%r290, %r1682, 16384;
	atom.shared.exch.b64 	%rd30, [%r290], 0;
	setp.eq.s64	%p237, %rd30, 0;
	@%p237 bra 	BB1_350;
	bra.uni 	BB1_349;

BB1_350:
	cvt.u64.u32	%rd529, %r1668;
	cvt.u64.u32	%rd530, %r1667;
	bfi.b64 	%rd531, %rd529, %rd530, 32, 32;
	add.s32 	%r1800, %r1682, 16384;
	atom.shared.cas.b64 	%rd533, [%r1800], %rd105, %rd531;
	setp.eq.s64	%p238, %rd533, 0;
	@%p238 bra 	BB1_352;

	add.s32 	%r1692, %r289, %r17;
	mul.wide.s32 	%rd534, %r1692, 4;
	add.s64 	%rd535, %rd3, %rd534;
	atom.global.add.u32 	%r1693, [%rd535], 2;
	min.s32 	%r1694, %r1693, %r253;
	mad.lo.s32 	%r1695, %r289, %r299, %r1694;
	shr.u32 	%r1696, %r1695, 31;
	add.s32 	%r1697, %r1695, %r1696;
	shr.s32 	%r1698, %r1697, 1;
	mul.wide.s32 	%rd536, %r1698, 16;
	add.s64 	%rd537, %rd1, %rd536;
	mov.u32 	%r1699, 0;
	st.global.v4.u32 	[%rd537], {%r1668, %r1667, %r1699, %r1699};
	bra.uni 	BB1_352;

BB1_349:
	add.s32 	%r1683, %r289, %r17;
	mul.wide.s32 	%rd524, %r1683, 4;
	add.s64 	%rd525, %rd3, %rd524;
	atom.global.add.u32 	%r1684, [%rd525], 2;
	min.s32 	%r1685, %r1684, %r253;
	mad.lo.s32 	%r1686, %r289, %r299, %r1685;
	shr.u32 	%r1687, %r1686, 31;
	add.s32 	%r1688, %r1686, %r1687;
	shr.s32 	%r1689, %r1688, 1;
	shr.u64 	%rd526, %rd30, 32;
	mul.wide.s32 	%rd527, %r1689, 16;
	add.s64 	%rd528, %rd1, %rd527;
	cvt.u32.u64	%r1690, %rd30;
	cvt.u32.u64	%r1691, %rd526;
	st.global.v4.u32 	[%rd528], {%r1668, %r1667, %r1691, %r1690};

BB1_352:
	add.s32 	%r291, %r1840, 3072;
	setp.ge.s32	%p239, %r291, %r14;
	@%p239 bra 	BB1_359;

	add.s32 	%r1700, %r291, %r21;
	mul.wide.s32 	%rd538, %r1700, 8;
	add.s64 	%rd539, %rd2, %rd538;
	ld.global.v2.u32 	{%r1701, %r1702}, [%rd539];
	or.b32  	%r1703, %r1701, %r1702;
	setp.eq.s32	%p240, %r1703, 0;
	@%p240 bra 	BB1_359;

	and.b32  	%r1704, %r1701, 131040;
	and.b32  	%r1705, %r1701, 31;
	xor.b32  	%r1706, %r1705, 1;
	shr.u32 	%r1707, %r1704, 3;
	add.s32 	%r1709, %r302, %r1707;
	mov.u32 	%r1710, 1;
	shl.b32 	%r1711, %r1710, %r1706;
	ld.shared.u32 	%r1712, [%r1709];
	and.b32  	%r1713, %r1712, %r1711;
	setp.eq.s32	%p241, %r1713, 0;
	@%p241 bra 	BB1_359;

	bfe.u32 	%r294, %r1702, 17, 12;
	shl.b32 	%r1714, %r294, 3;
	add.s32 	%r1716, %r302, %r1714;
	add.s32 	%r295, %r1716, 16384;
	atom.shared.exch.b64 	%rd31, [%r295], 0;
	setp.eq.s64	%p242, %rd31, 0;
	@%p242 bra 	BB1_357;
	bra.uni 	BB1_356;

BB1_357:
	cvt.u64.u32	%rd545, %r1702;
	cvt.u64.u32	%rd546, %r1701;
	bfi.b64 	%rd547, %rd545, %rd546, 32, 32;
	add.s32 	%r1801, %r1716, 16384;
	atom.shared.cas.b64 	%rd549, [%r1801], %rd105, %rd547;
	setp.eq.s64	%p243, %rd549, 0;
	@%p243 bra 	BB1_359;

	add.s32 	%r1726, %r294, %r17;
	mul.wide.s32 	%rd550, %r1726, 4;
	add.s64 	%rd551, %rd3, %rd550;
	atom.global.add.u32 	%r1727, [%rd551], 2;
	min.s32 	%r1728, %r1727, %r253;
	mad.lo.s32 	%r1729, %r294, %r299, %r1728;
	shr.u32 	%r1730, %r1729, 31;
	add.s32 	%r1731, %r1729, %r1730;
	shr.s32 	%r1732, %r1731, 1;
	mul.wide.s32 	%rd552, %r1732, 16;
	add.s64 	%rd553, %rd1, %rd552;
	mov.u32 	%r1733, 0;
	st.global.v4.u32 	[%rd553], {%r1702, %r1701, %r1733, %r1733};
	bra.uni 	BB1_359;

BB1_356:
	add.s32 	%r1717, %r294, %r17;
	mul.wide.s32 	%rd540, %r1717, 4;
	add.s64 	%rd541, %rd3, %rd540;
	atom.global.add.u32 	%r1718, [%rd541], 2;
	min.s32 	%r1719, %r1718, %r253;
	mad.lo.s32 	%r1720, %r294, %r299, %r1719;
	shr.u32 	%r1721, %r1720, 31;
	add.s32 	%r1722, %r1720, %r1721;
	shr.s32 	%r1723, %r1722, 1;
	shr.u64 	%rd542, %rd31, 32;
	mul.wide.s32 	%rd543, %r1723, 16;
	add.s64 	%rd544, %rd1, %rd543;
	cvt.u32.u64	%r1724, %rd31;
	cvt.u32.u64	%r1725, %rd542;
	st.global.v4.u32 	[%rd544], {%r1702, %r1701, %r1725, %r1724};

BB1_359:
	add.s32 	%r1841, %r1841, 4;
	add.s32 	%r1840, %r1840, 4096;
	setp.lt.s32	%p244, %r1841, %r16;
	@%p244 bra 	BB1_331;

BB1_360:
	bar.sync 	0;
	ld.shared.u64 	%rd32, [%r114];
	setp.eq.s64	%p245, %rd32, 0;
	@%p245 bra 	BB1_362;

	add.s32 	%r1734, %r1, %r17;
	mul.wide.s32 	%rd554, %r1734, 4;
	add.s64 	%rd555, %rd3, %rd554;
	atom.global.add.u32 	%r1735, [%rd555], 2;
	min.s32 	%r1736, %r1735, %r253;
	mad.lo.s32 	%r1737, %r1, %r299, %r1736;
	shr.u32 	%r1738, %r1737, 31;
	add.s32 	%r1739, %r1737, %r1738;
	shr.s32 	%r1740, %r1739, 1;
	shr.u64 	%rd556, %rd32, 32;
	mul.wide.s32 	%rd557, %r1740, 16;
	add.s64 	%rd558, %rd1, %rd557;
	cvt.u32.u64	%r1741, %rd32;
	cvt.u32.u64	%r1742, %rd556;
	st.global.v4.u32 	[%rd558], {%r1742, %r1741, %r304, %r304};

BB1_362:
	ld.shared.u64 	%rd33, [%r115];
	setp.eq.s64	%p246, %rd33, 0;
	@%p246 bra 	BB1_364;

	add.s32 	%r1744, %r2, %r17;
	mul.wide.s32 	%rd559, %r1744, 4;
	add.s64 	%rd560, %rd3, %rd559;
	atom.global.add.u32 	%r1745, [%rd560], 2;
	min.s32 	%r1746, %r1745, %r253;
	mad.lo.s32 	%r1747, %r2, %r299, %r1746;
	shr.u32 	%r1748, %r1747, 31;
	add.s32 	%r1749, %r1747, %r1748;
	shr.s32 	%r1750, %r1749, 1;
	shr.u64 	%rd561, %rd33, 32;
	mul.wide.s32 	%rd562, %r1750, 16;
	add.s64 	%rd563, %rd1, %rd562;
	cvt.u32.u64	%r1751, %rd33;
	cvt.u32.u64	%r1752, %rd561;
	st.global.v4.u32 	[%rd563], {%r1752, %r1751, %r304, %r304};

BB1_364:
	ld.shared.u64 	%rd34, [%r116];
	setp.eq.s64	%p247, %rd34, 0;
	@%p247 bra 	BB1_366;

	add.s32 	%r1754, %r3, %r17;
	mul.wide.s32 	%rd564, %r1754, 4;
	add.s64 	%rd565, %rd3, %rd564;
	atom.global.add.u32 	%r1755, [%rd565], 2;
	min.s32 	%r1756, %r1755, %r253;
	mad.lo.s32 	%r1757, %r3, %r299, %r1756;
	shr.u32 	%r1758, %r1757, 31;
	add.s32 	%r1759, %r1757, %r1758;
	shr.s32 	%r1760, %r1759, 1;
	shr.u64 	%rd566, %rd34, 32;
	mul.wide.s32 	%rd567, %r1760, 16;
	add.s64 	%rd568, %rd1, %rd567;
	cvt.u32.u64	%r1761, %rd34;
	cvt.u32.u64	%r1762, %rd566;
	st.global.v4.u32 	[%rd568], {%r1762, %r1761, %r304, %r304};

BB1_366:
	ld.shared.u64 	%rd35, [%r117];
	setp.eq.s64	%p248, %rd35, 0;
	@%p248 bra 	BB1_368;

	add.s32 	%r1764, %r4, %r17;
	mul.wide.s32 	%rd569, %r1764, 4;
	add.s64 	%rd570, %rd3, %rd569;
	atom.global.add.u32 	%r1765, [%rd570], 2;
	min.s32 	%r1766, %r1765, %r253;
	mad.lo.s32 	%r1767, %r4, %r299, %r1766;
	shr.u32 	%r1768, %r1767, 31;
	add.s32 	%r1769, %r1767, %r1768;
	shr.s32 	%r1770, %r1769, 1;
	shr.u64 	%rd571, %rd35, 32;
	mul.wide.s32 	%rd572, %r1770, 16;
	add.s64 	%rd573, %rd1, %rd572;
	cvt.u32.u64	%r1771, %rd35;
	cvt.u32.u64	%r1772, %rd571;
	st.global.v4.u32 	[%rd573], {%r1772, %r1771, %r304, %r304};

BB1_368:
	ret;

BB1_134:
	cvt.u64.u32	%rd113, %r743;
	cvt.u64.u32	%rd114, %r742;
	bfi.b64 	%rd115, %rd113, %rd114, 32, 32;
	add.s32 	%r1774, %r757, 16384;
	atom.shared.cas.b64 	%rd117, [%r1774], %rd105, %rd115;
	setp.eq.s64	%p89, %rd117, 0;
	@%p89 bra 	BB1_136;

	add.s32 	%r770, %r122, %r17;
	mul.wide.s32 	%rd118, %r770, 4;
	add.s64 	%rd119, %rd3, %rd118;
	atom.global.add.u32 	%r771, [%rd119], 2;
	min.s32 	%r772, %r771, %r118;
	mad.lo.s32 	%r773, %r122, %r299, %r772;
	shr.u32 	%r774, %r773, 31;
	add.s32 	%r775, %r773, %r774;
	shr.s32 	%r776, %r775, 1;
	mul.wide.s32 	%rd120, %r776, 16;
	add.s64 	%rd121, %rd1, %rd120;
	mov.u32 	%r777, 0;
	st.global.v4.u32 	[%rd121], {%r743, %r742, %r777, %r777};
	bra.uni 	BB1_136;

BB1_193:
	cvt.u64.u32	%rd225, %r993;
	cvt.u64.u32	%rd226, %r992;
	bfi.b64 	%rd227, %rd225, %rd226, 32, 32;
	add.s32 	%r1781, %r1007, 16384;
	atom.shared.cas.b64 	%rd229, [%r1781], %rd105, %rd227;
	setp.eq.s64	%p130, %rd229, 0;
	@%p130 bra 	BB1_195;

	add.s32 	%r1020, %r167, %r17;
	mul.wide.s32 	%rd230, %r1020, 4;
	add.s64 	%rd231, %rd3, %rd230;
	atom.global.add.u32 	%r1021, [%rd231], 2;
	min.s32 	%r1022, %r1021, %r163;
	mad.lo.s32 	%r1023, %r167, %r299, %r1022;
	shr.u32 	%r1024, %r1023, 31;
	add.s32 	%r1025, %r1023, %r1024;
	shr.s32 	%r1026, %r1025, 1;
	mul.wide.s32 	%rd232, %r1026, 16;
	add.s64 	%rd233, %rd1, %rd232;
	mov.u32 	%r1027, 0;
	st.global.v4.u32 	[%rd233], {%r993, %r992, %r1027, %r1027};
	bra.uni 	BB1_195;

BB1_252:
	cvt.u64.u32	%rd337, %r1243;
	cvt.u64.u32	%rd338, %r1242;
	bfi.b64 	%rd339, %rd337, %rd338, 32, 32;
	add.s32 	%r1788, %r1257, 16384;
	atom.shared.cas.b64 	%rd341, [%r1788], %rd105, %rd339;
	setp.eq.s64	%p171, %rd341, 0;
	@%p171 bra 	BB1_254;

	add.s32 	%r1270, %r212, %r17;
	mul.wide.s32 	%rd342, %r1270, 4;
	add.s64 	%rd343, %rd3, %rd342;
	atom.global.add.u32 	%r1271, [%rd343], 2;
	min.s32 	%r1272, %r1271, %r208;
	mad.lo.s32 	%r1273, %r212, %r299, %r1272;
	shr.u32 	%r1274, %r1273, 31;
	add.s32 	%r1275, %r1273, %r1274;
	shr.s32 	%r1276, %r1275, 1;
	mul.wide.s32 	%rd344, %r1276, 16;
	add.s64 	%rd345, %rd1, %rd344;
	mov.u32 	%r1277, 0;
	st.global.v4.u32 	[%rd345], {%r1243, %r1242, %r1277, %r1277};
	bra.uni 	BB1_254;

BB1_311:
	cvt.u64.u32	%rd449, %r1493;
	cvt.u64.u32	%rd450, %r1492;
	bfi.b64 	%rd451, %rd449, %rd450, 32, 32;
	add.s32 	%r1795, %r1507, 16384;
	atom.shared.cas.b64 	%rd453, [%r1795], %rd105, %rd451;
	setp.eq.s64	%p212, %rd453, 0;
	@%p212 bra 	BB1_313;

	add.s32 	%r1520, %r257, %r17;
	mul.wide.s32 	%rd454, %r1520, 4;
	add.s64 	%rd455, %rd3, %rd454;
	atom.global.add.u32 	%r1521, [%rd455], 2;
	min.s32 	%r1522, %r1521, %r253;
	mad.lo.s32 	%r1523, %r257, %r299, %r1522;
	shr.u32 	%r1524, %r1523, 31;
	add.s32 	%r1525, %r1523, %r1524;
	shr.s32 	%r1526, %r1525, 1;
	mul.wide.s32 	%rd456, %r1526, 16;
	add.s64 	%rd457, %rd1, %rd456;
	mov.u32 	%r1527, 0;
	st.global.v4.u32 	[%rd457], {%r1493, %r1492, %r1527, %r1527};
	bra.uni 	BB1_313;
}

	// .globl	FluffyRound_A2
.visible .entry FluffyRound_A2(
	.param .u64 FluffyRound_A2_param_0,
	.param .u64 FluffyRound_A2_param_1,
	.param .u64 FluffyRound_A2_param_2,
	.param .u64 FluffyRound_A2_param_3,
	.param .u32 FluffyRound_A2_param_4,
	.param .u32 FluffyRound_A2_param_5,
	.param .u32 FluffyRound_A2_param_6,
	.param .u64 FluffyRound_A2_param_7
)
{
	.reg .pred 	%p<41>;
	.reg .b32 	%r<382>;
	.reg .b64 	%rd<86>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_A2$__cuda_local_var_207398_30_non_const_ecounters[16384];

	ld.param.u64 	%rd2, [FluffyRound_A2_param_0];
	ld.param.u64 	%rd3, [FluffyRound_A2_param_1];
	ld.param.u64 	%rd5, [FluffyRound_A2_param_2];
	ld.param.u64 	%rd4, [FluffyRound_A2_param_3];
	ld.param.u32 	%r70, [FluffyRound_A2_param_4];
	ld.param.u32 	%r69, [FluffyRound_A2_param_5];
	mov.u32 	%r71, %ctaid.x;
	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r71, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r72, [%rd8];
	min.s32 	%r1, %r72, %r70;
	add.s32 	%r2, %r1, 512;
	shr.s32 	%r73, %r2, 31;
	shr.u32 	%r74, %r73, 23;
	add.s32 	%r75, %r2, %r74;
	shr.s32 	%r3, %r75, 9;
	mov.u32 	%r4, %tid.x;
	shl.b32 	%r76, %r4, 2;
	mov.u32 	%r77, FluffyRound_A2$__cuda_local_var_207398_30_non_const_ecounters;
	add.s32 	%r78, %r77, %r76;
	mov.u32 	%r372, 0;
	st.shared.u32 	[%r78], %r372;
	st.shared.u32 	[%r78+2048], %r372;
	st.shared.u32 	[%r78+4096], %r372;
	st.shared.u32 	[%r78+6144], %r372;
	st.shared.u32 	[%r78+8192], %r372;
	st.shared.u32 	[%r78+10240], %r372;
	st.shared.u32 	[%r78+12288], %r372;
	st.shared.u32 	[%r78+14336], %r372;
	cvta.to.global.u64 	%rd1, %rd2;
	mul.lo.s32 	%r5, %r71, %r70;
	bar.sync 	0;
	setp.gt.s32	%p1, %r2, 1023;
	@%p1 bra 	BB2_6;
	bra.uni 	BB2_1;

BB2_6:
	add.s32 	%r10, %r3, -1;
	setp.lt.s32	%p5, %r10, 1;
	@%p5 bra 	BB2_28;

	and.b32  	%r112, %r10, 3;
	setp.eq.s32	%p6, %r112, 0;
	@%p6 bra 	BB2_18;

	setp.eq.s32	%p7, %r112, 1;
	@%p7 bra 	BB2_15;

	setp.eq.s32	%p8, %r112, 2;
	@%p8 bra 	BB2_12;

	mov.u32 	%r372, 1;
	setp.ge.s32	%p9, %r4, %r1;
	@%p9 bra 	BB2_12;

	add.s32 	%r115, %r4, %r5;
	mul.wide.s32 	%rd16, %r115, 8;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.u32 	%r116, [%rd17];
	and.b32  	%r117, %r116, 131040;
	and.b32  	%r118, %r116, 31;
	mov.u32 	%r372, 1;
	shl.b32 	%r119, %r372, %r118;
	shr.u32 	%r120, %r117, 3;
	add.s32 	%r122, %r77, %r120;
	atom.shared.or.b32 	%r123, [%r122], %r119;

BB2_12:
	shl.b32 	%r124, %r372, 9;
	add.s32 	%r12, %r124, %r4;
	setp.ge.s32	%p10, %r12, %r1;
	@%p10 bra 	BB2_14;

	add.s32 	%r125, %r12, %r5;
	mul.wide.s32 	%rd18, %r125, 8;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.u32 	%r126, [%rd19];
	and.b32  	%r127, %r126, 131040;
	and.b32  	%r128, %r126, 31;
	mov.u32 	%r129, 1;
	shl.b32 	%r130, %r129, %r128;
	shr.u32 	%r131, %r127, 3;
	add.s32 	%r133, %r77, %r131;
	atom.shared.or.b32 	%r134, [%r133], %r130;

BB2_14:
	add.s32 	%r372, %r372, 1;

BB2_15:
	shl.b32 	%r135, %r372, 9;
	add.s32 	%r15, %r135, %r4;
	setp.ge.s32	%p11, %r15, %r1;
	@%p11 bra 	BB2_17;

	add.s32 	%r136, %r15, %r5;
	mul.wide.s32 	%rd20, %r136, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r137, [%rd21];
	and.b32  	%r138, %r137, 131040;
	and.b32  	%r139, %r137, 31;
	mov.u32 	%r140, 1;
	shl.b32 	%r141, %r140, %r139;
	shr.u32 	%r142, %r138, 3;
	add.s32 	%r144, %r77, %r142;
	atom.shared.or.b32 	%r145, [%r144], %r141;

BB2_17:
	add.s32 	%r372, %r372, 1;

BB2_18:
	setp.lt.u32	%p12, %r10, 4;
	@%p12 bra 	BB2_28;

BB2_19:
	shl.b32 	%r146, %r372, 9;
	add.s32 	%r19, %r146, %r4;
	setp.ge.s32	%p13, %r19, %r1;
	@%p13 bra 	BB2_21;

	add.s32 	%r147, %r19, %r5;
	mul.wide.s32 	%rd22, %r147, 8;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.u32 	%r148, [%rd23];
	and.b32  	%r149, %r148, 131040;
	and.b32  	%r150, %r148, 31;
	mov.u32 	%r151, 1;
	shl.b32 	%r152, %r151, %r150;
	shr.u32 	%r153, %r149, 3;
	add.s32 	%r155, %r77, %r153;
	atom.shared.or.b32 	%r156, [%r155], %r152;

BB2_21:
	add.s32 	%r20, %r19, 512;
	setp.ge.s32	%p14, %r20, %r1;
	@%p14 bra 	BB2_23;

	add.s32 	%r159, %r20, %r5;
	mul.wide.s32 	%rd24, %r159, 8;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.u32 	%r160, [%rd25];
	and.b32  	%r161, %r160, 131040;
	and.b32  	%r162, %r160, 31;
	mov.u32 	%r163, 1;
	shl.b32 	%r164, %r163, %r162;
	shr.u32 	%r165, %r161, 3;
	add.s32 	%r167, %r77, %r165;
	atom.shared.or.b32 	%r168, [%r167], %r164;

BB2_23:
	add.s32 	%r21, %r19, 1024;
	setp.ge.s32	%p15, %r21, %r1;
	@%p15 bra 	BB2_25;

	add.s32 	%r171, %r21, %r5;
	mul.wide.s32 	%rd26, %r171, 8;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.u32 	%r172, [%rd27];
	and.b32  	%r173, %r172, 131040;
	and.b32  	%r174, %r172, 31;
	mov.u32 	%r175, 1;
	shl.b32 	%r176, %r175, %r174;
	shr.u32 	%r177, %r173, 3;
	add.s32 	%r179, %r77, %r177;
	atom.shared.or.b32 	%r180, [%r179], %r176;

BB2_25:
	add.s32 	%r22, %r19, 1536;
	setp.ge.s32	%p16, %r22, %r1;
	@%p16 bra 	BB2_27;

	add.s32 	%r183, %r22, %r5;
	mul.wide.s32 	%rd28, %r183, 8;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.u32 	%r184, [%rd29];
	and.b32  	%r185, %r184, 131040;
	and.b32  	%r186, %r184, 31;
	mov.u32 	%r187, 1;
	shl.b32 	%r188, %r187, %r186;
	shr.u32 	%r189, %r185, 3;
	add.s32 	%r191, %r77, %r189;
	atom.shared.or.b32 	%r192, [%r191], %r188;

BB2_27:
	add.s32 	%r372, %r372, 4;
	setp.lt.s32	%p17, %r372, %r10;
	@%p17 bra 	BB2_19;

BB2_28:
	shl.b32 	%r194, %r10, 9;
	add.s32 	%r24, %r194, %r4;
	setp.ge.s32	%p18, %r24, %r1;
	@%p18 bra 	BB2_30;

	add.s32 	%r197, %r24, %r5;
	mul.wide.s32 	%rd31, %r197, 8;
	add.s64 	%rd30, %rd2, %rd31;
	// inline asm
	ld.global.cs.v2.u32 {%r376,%r377}, [%rd30];
	// inline asm
	and.b32  	%r198, %r376, 131040;
	and.b32  	%r199, %r376, 31;
	mov.u32 	%r200, 1;
	shl.b32 	%r201, %r200, %r199;
	shr.u32 	%r202, %r198, 3;
	add.s32 	%r204, %r77, %r202;
	atom.shared.or.b32 	%r205, [%r204], %r201;

BB2_30:
	bar.sync 	0;
	@%p18 bra 	BB2_33;

	and.b32  	%r206, %r376, 131040;
	and.b32  	%r207, %r376, 31;
	xor.b32  	%r208, %r207, 1;
	shr.u32 	%r209, %r206, 3;
	add.s32 	%r211, %r77, %r209;
	mov.u32 	%r212, 1;
	shl.b32 	%r213, %r212, %r208;
	ld.shared.u32 	%r214, [%r211];
	and.b32  	%r215, %r214, %r213;
	setp.eq.s32	%p20, %r215, 0;
	@%p20 bra 	BB2_33;

	bfe.u32 	%r218, %r377, 17, 12;
	cvta.to.global.u64 	%rd33, %rd4;
	mul.wide.u32 	%rd34, %r218, 4;
	add.s64 	%rd35, %rd33, %rd34;
	atom.global.add.u32 	%r219, [%rd35], 1;
	add.s32 	%r220, %r69, -2;
	min.s32 	%r221, %r219, %r220;
	mad.lo.s32 	%r222, %r218, %r69, %r221;
	mul.wide.s32 	%rd36, %r222, 8;
	add.s64 	%rd32, %rd3, %rd36;
	// inline asm
	st.global.cg.v2.u32 [%rd32], {%r377, %r376};
	// inline asm

BB2_33:
	add.s32 	%r29, %r3, -2;
	setp.lt.s32	%p21, %r29, 0;
	@%p21 bra 	BB2_63;

	add.s32 	%r30, %r69, -2;
	mov.u32 	%r224, 1;
	sub.s32 	%r225, %r224, %r3;
	mov.u32 	%r226, -1;
	max.s32 	%r227, %r225, %r226;
	add.s32 	%r31, %r3, %r227;
	and.b32  	%r223, %r31, 3;
	setp.eq.s32	%p22, %r223, 0;
	@%p22 bra 	BB2_49;

	setp.eq.s32	%p23, %r223, 1;
	@%p23 bra 	BB2_45;

	setp.eq.s32	%p24, %r223, 2;
	@%p24 bra 	BB2_41;

	shl.b32 	%r228, %r29, 9;
	add.s32 	%r32, %r228, %r4;
	setp.ge.s32	%p25, %r32, %r1;
	@%p25 bra 	BB2_40;

	add.s32 	%r229, %r32, %r5;
	mul.wide.s32 	%rd37, %r229, 8;
	add.s64 	%rd38, %rd1, %rd37;
	ld.global.v2.u32 	{%r230, %r231}, [%rd38];
	and.b32  	%r232, %r230, 131040;
	and.b32  	%r233, %r230, 31;
	xor.b32  	%r234, %r233, 1;
	shr.u32 	%r235, %r232, 3;
	add.s32 	%r237, %r77, %r235;
	shl.b32 	%r239, %r224, %r234;
	ld.shared.u32 	%r240, [%r237];
	and.b32  	%r241, %r239, %r240;
	setp.eq.s32	%p26, %r241, 0;
	@%p26 bra 	BB2_40;

	bfe.u32 	%r244, %r231, 17, 12;
	cvta.to.global.u64 	%rd40, %rd4;
	mul.wide.u32 	%rd41, %r244, 4;
	add.s64 	%rd42, %rd40, %rd41;
	atom.global.add.u32 	%r245, [%rd42], 1;
	min.s32 	%r246, %r245, %r30;
	mad.lo.s32 	%r247, %r244, %r69, %r246;
	mul.wide.s32 	%rd43, %r247, 8;
	add.s64 	%rd39, %rd3, %rd43;
	// inline asm
	st.global.cg.v2.u32 [%rd39], {%r231, %r230};
	// inline asm

BB2_40:
	add.s32 	%r29, %r3, -3;

BB2_41:
	shl.b32 	%r248, %r29, 9;
	add.s32 	%r38, %r248, %r4;
	setp.ge.s32	%p27, %r38, %r1;
	@%p27 bra 	BB2_44;

	add.s32 	%r249, %r38, %r5;
	mul.wide.s32 	%rd44, %r249, 8;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.v2.u32 	{%r250, %r251}, [%rd45];
	and.b32  	%r252, %r250, 131040;
	and.b32  	%r253, %r250, 31;
	xor.b32  	%r254, %r253, 1;
	shr.u32 	%r255, %r252, 3;
	add.s32 	%r257, %r77, %r255;
	shl.b32 	%r259, %r224, %r254;
	ld.shared.u32 	%r260, [%r257];
	and.b32  	%r261, %r259, %r260;
	setp.eq.s32	%p28, %r261, 0;
	@%p28 bra 	BB2_44;

	bfe.u32 	%r264, %r251, 17, 12;
	cvta.to.global.u64 	%rd47, %rd4;
	mul.wide.u32 	%rd48, %r264, 4;
	add.s64 	%rd49, %rd47, %rd48;
	atom.global.add.u32 	%r265, [%rd49], 1;
	min.s32 	%r266, %r265, %r30;
	mad.lo.s32 	%r267, %r264, %r69, %r266;
	mul.wide.s32 	%rd50, %r267, 8;
	add.s64 	%rd46, %rd3, %rd50;
	// inline asm
	st.global.cg.v2.u32 [%rd46], {%r251, %r250};
	// inline asm

BB2_44:
	add.s32 	%r29, %r29, -1;

BB2_45:
	shl.b32 	%r268, %r29, 9;
	add.s32 	%r44, %r268, %r4;
	setp.ge.s32	%p29, %r44, %r1;
	@%p29 bra 	BB2_48;

	add.s32 	%r269, %r44, %r5;
	mul.wide.s32 	%rd51, %r269, 8;
	add.s64 	%rd52, %rd1, %rd51;
	ld.global.v2.u32 	{%r270, %r271}, [%rd52];
	and.b32  	%r272, %r270, 131040;
	and.b32  	%r273, %r270, 31;
	xor.b32  	%r274, %r273, 1;
	shr.u32 	%r275, %r272, 3;
	add.s32 	%r277, %r77, %r275;
	shl.b32 	%r279, %r224, %r274;
	ld.shared.u32 	%r280, [%r277];
	and.b32  	%r281, %r279, %r280;
	setp.eq.s32	%p30, %r281, 0;
	@%p30 bra 	BB2_48;

	bfe.u32 	%r284, %r271, 17, 12;
	cvta.to.global.u64 	%rd54, %rd4;
	mul.wide.u32 	%rd55, %r284, 4;
	add.s64 	%rd56, %rd54, %rd55;
	atom.global.add.u32 	%r285, [%rd56], 1;
	min.s32 	%r286, %r285, %r30;
	mad.lo.s32 	%r287, %r284, %r69, %r286;
	mul.wide.s32 	%rd57, %r287, 8;
	add.s64 	%rd53, %rd3, %rd57;
	// inline asm
	st.global.cg.v2.u32 [%rd53], {%r271, %r270};
	// inline asm

BB2_48:
	add.s32 	%r29, %r29, -1;

BB2_49:
	setp.lt.u32	%p31, %r31, 4;
	@%p31 bra 	BB2_63;

BB2_50:
	shl.b32 	%r288, %r29, 9;
	add.s32 	%r51, %r288, %r4;
	setp.ge.s32	%p32, %r51, %r1;
	@%p32 bra 	BB2_53;

	add.s32 	%r289, %r51, %r5;
	mul.wide.s32 	%rd58, %r289, 8;
	add.s64 	%rd59, %rd1, %rd58;
	ld.global.v2.u32 	{%r290, %r291}, [%rd59];
	and.b32  	%r292, %r290, 131040;
	and.b32  	%r293, %r290, 31;
	xor.b32  	%r294, %r293, 1;
	shr.u32 	%r295, %r292, 3;
	add.s32 	%r297, %r77, %r295;
	shl.b32 	%r299, %r224, %r294;
	ld.shared.u32 	%r300, [%r297];
	and.b32  	%r301, %r299, %r300;
	setp.eq.s32	%p33, %r301, 0;
	@%p33 bra 	BB2_53;

	bfe.u32 	%r304, %r291, 17, 12;
	cvta.to.global.u64 	%rd61, %rd4;
	mul.wide.u32 	%rd62, %r304, 4;
	add.s64 	%rd63, %rd61, %rd62;
	atom.global.add.u32 	%r305, [%rd63], 1;
	min.s32 	%r306, %r305, %r30;
	mad.lo.s32 	%r307, %r304, %r69, %r306;
	mul.wide.s32 	%rd64, %r307, 8;
	add.s64 	%rd60, %rd3, %rd64;
	// inline asm
	st.global.cg.v2.u32 [%rd60], {%r291, %r290};
	// inline asm

BB2_53:
	add.s32 	%r55, %r51, -512;
	setp.ge.s32	%p34, %r55, %r1;
	@%p34 bra 	BB2_56;

	add.s32 	%r310, %r55, %r5;
	mul.wide.s32 	%rd65, %r310, 8;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.v2.u32 	{%r311, %r312}, [%rd66];
	and.b32  	%r313, %r311, 131040;
	and.b32  	%r314, %r311, 31;
	xor.b32  	%r315, %r314, 1;
	shr.u32 	%r316, %r313, 3;
	add.s32 	%r318, %r77, %r316;
	shl.b32 	%r320, %r224, %r315;
	ld.shared.u32 	%r321, [%r318];
	and.b32  	%r322, %r320, %r321;
	setp.eq.s32	%p35, %r322, 0;
	@%p35 bra 	BB2_56;

	bfe.u32 	%r325, %r312, 17, 12;
	cvta.to.global.u64 	%rd68, %rd4;
	mul.wide.u32 	%rd69, %r325, 4;
	add.s64 	%rd70, %rd68, %rd69;
	atom.global.add.u32 	%r326, [%rd70], 1;
	min.s32 	%r327, %r326, %r30;
	mad.lo.s32 	%r328, %r325, %r69, %r327;
	mul.wide.s32 	%rd71, %r328, 8;
	add.s64 	%rd67, %rd3, %rd71;
	// inline asm
	st.global.cg.v2.u32 [%rd67], {%r312, %r311};
	// inline asm

BB2_56:
	add.s32 	%r59, %r51, -1024;
	setp.ge.s32	%p36, %r59, %r1;
	@%p36 bra 	BB2_59;

	add.s32 	%r331, %r59, %r5;
	mul.wide.s32 	%rd72, %r331, 8;
	add.s64 	%rd73, %rd1, %rd72;
	ld.global.v2.u32 	{%r332, %r333}, [%rd73];
	and.b32  	%r334, %r332, 131040;
	and.b32  	%r335, %r332, 31;
	xor.b32  	%r336, %r335, 1;
	shr.u32 	%r337, %r334, 3;
	add.s32 	%r339, %r77, %r337;
	shl.b32 	%r341, %r224, %r336;
	ld.shared.u32 	%r342, [%r339];
	and.b32  	%r343, %r341, %r342;
	setp.eq.s32	%p37, %r343, 0;
	@%p37 bra 	BB2_59;

	bfe.u32 	%r346, %r333, 17, 12;
	cvta.to.global.u64 	%rd75, %rd4;
	mul.wide.u32 	%rd76, %r346, 4;
	add.s64 	%rd77, %rd75, %rd76;
	atom.global.add.u32 	%r347, [%rd77], 1;
	min.s32 	%r348, %r347, %r30;
	mad.lo.s32 	%r349, %r346, %r69, %r348;
	mul.wide.s32 	%rd78, %r349, 8;
	add.s64 	%rd74, %rd3, %rd78;
	// inline asm
	st.global.cg.v2.u32 [%rd74], {%r333, %r332};
	// inline asm

BB2_59:
	add.s32 	%r63, %r29, -3;
	shl.b32 	%r350, %r63, 9;
	add.s32 	%r64, %r350, %r4;
	setp.ge.s32	%p38, %r64, %r1;
	@%p38 bra 	BB2_62;

	add.s32 	%r351, %r64, %r5;
	mul.wide.s32 	%rd79, %r351, 8;
	add.s64 	%rd80, %rd1, %rd79;
	ld.global.v2.u32 	{%r352, %r353}, [%rd80];
	and.b32  	%r354, %r352, 131040;
	and.b32  	%r355, %r352, 31;
	xor.b32  	%r356, %r355, 1;
	shr.u32 	%r357, %r354, 3;
	add.s32 	%r359, %r77, %r357;
	shl.b32 	%r361, %r224, %r356;
	ld.shared.u32 	%r362, [%r359];
	and.b32  	%r363, %r361, %r362;
	setp.eq.s32	%p39, %r363, 0;
	@%p39 bra 	BB2_62;

	bfe.u32 	%r366, %r353, 17, 12;
	cvta.to.global.u64 	%rd82, %rd4;
	mul.wide.u32 	%rd83, %r366, 4;
	add.s64 	%rd84, %rd82, %rd83;
	atom.global.add.u32 	%r367, [%rd84], 1;
	min.s32 	%r368, %r367, %r30;
	mad.lo.s32 	%r369, %r366, %r69, %r368;
	mul.wide.s32 	%rd85, %r369, 8;
	add.s64 	%rd81, %rd3, %rd85;
	// inline asm
	st.global.cg.v2.u32 [%rd81], {%r353, %r352};
	// inline asm

BB2_62:
	add.s32 	%r29, %r29, -4;
	setp.gt.s32	%p40, %r63, 0;
	@%p40 bra 	BB2_50;
	bra.uni 	BB2_63;

BB2_1:
	setp.ge.s32	%p2, %r4, %r1;
	@%p2 bra 	BB2_3;

	add.s32 	%r83, %r5, %r4;
	mul.wide.s32 	%rd10, %r83, 8;
	add.s64 	%rd9, %rd2, %rd10;
	// inline asm
	ld.global.cs.v2.u32 {%r371,%r370}, [%rd9];
	// inline asm
	and.b32  	%r84, %r371, 131040;
	and.b32  	%r85, %r371, 31;
	mov.u32 	%r86, 1;
	shl.b32 	%r87, %r86, %r85;
	shr.u32 	%r88, %r84, 3;
	add.s32 	%r90, %r77, %r88;
	atom.shared.or.b32 	%r91, [%r90], %r87;

BB2_3:
	bar.sync 	0;
	@%p2 bra 	BB2_63;

	and.b32  	%r92, %r371, 131040;
	and.b32  	%r93, %r371, 31;
	xor.b32  	%r94, %r93, 1;
	shr.u32 	%r95, %r92, 3;
	add.s32 	%r97, %r77, %r95;
	mov.u32 	%r98, 1;
	shl.b32 	%r99, %r98, %r94;
	ld.shared.u32 	%r100, [%r97];
	and.b32  	%r101, %r100, %r99;
	setp.eq.s32	%p4, %r101, 0;
	@%p4 bra 	BB2_63;

	cvta.to.global.u64 	%rd12, %rd4;
	bfe.u32 	%r104, %r370, 17, 12;
	mul.wide.u32 	%rd13, %r104, 4;
	add.s64 	%rd14, %rd12, %rd13;
	atom.global.add.u32 	%r105, [%rd14], 1;
	add.s32 	%r106, %r69, -2;
	min.s32 	%r107, %r105, %r106;
	mad.lo.s32 	%r108, %r104, %r69, %r107;
	mul.wide.s32 	%rd15, %r108, 8;
	add.s64 	%rd11, %rd3, %rd15;
	// inline asm
	st.global.cg.v2.u32 [%rd11], {%r370, %r371};
	// inline asm

BB2_63:
	ret;
}

	// .globl	FluffyRound_A3
.visible .entry FluffyRound_A3(
	.param .u64 FluffyRound_A3_param_0,
	.param .u64 FluffyRound_A3_param_1,
	.param .u64 FluffyRound_A3_param_2,
	.param .u64 FluffyRound_A3_param_3,
	.param .u64 FluffyRound_A3_param_4,
	.param .u64 FluffyRound_A3_param_5,
	.param .u64 FluffyRound_A3_param_6,
	.param .u32 FluffyRound_A3_param_7,
	.param .u32 FluffyRound_A3_param_8
)
{
	.reg .pred 	%p<189>;
	.reg .b32 	%r<1301>;
	.reg .b64 	%rd<242>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_A3$__cuda_local_var_207504_30_non_const_ecounters[32768];

	ld.param.u64 	%rd2, [FluffyRound_A3_param_0];
	ld.param.u64 	%rd3, [FluffyRound_A3_param_1];
	ld.param.u64 	%rd4, [FluffyRound_A3_param_2];
	ld.param.u64 	%rd5, [FluffyRound_A3_param_3];
	ld.param.u64 	%rd6, [FluffyRound_A3_param_4];
	ld.param.u64 	%rd7, [FluffyRound_A3_param_5];
	ld.param.u64 	%rd8, [FluffyRound_A3_param_6];
	ld.param.u32 	%r216, [FluffyRound_A3_param_7];
	ld.param.u32 	%r215, [FluffyRound_A3_param_8];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r217, %r1, 2;
	mov.u32 	%r218, FluffyRound_A3$__cuda_local_var_207504_30_non_const_ecounters;
	add.s32 	%r219, %r218, %r217;
	mov.u32 	%r1297, 0;
	st.shared.u32 	[%r219], %r1297;
	st.shared.u32 	[%r219+4096], %r1297;
	st.shared.u32 	[%r219+8192], %r1297;
	st.shared.u32 	[%r219+12288], %r1297;
	st.shared.u32 	[%r219+16384], %r1297;
	st.shared.u32 	[%r219+20480], %r1297;
	st.shared.u32 	[%r219+24576], %r1297;
	st.shared.u32 	[%r219+28672], %r1297;
	mov.u32 	%r221, %ctaid.x;
	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.s32 	%rd10, %r221, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%r222, [%rd11];
	min.s32 	%r2, %r222, %r216;
	add.s32 	%r223, %r221, 4096;
	mul.wide.s32 	%rd12, %r223, 4;
	add.s64 	%rd13, %rd9, %rd12;
	ld.global.u32 	%r224, [%rd13];
	min.s32 	%r3, %r224, %r216;
	add.s32 	%r225, %r221, 8192;
	mul.wide.s32 	%rd14, %r225, 4;
	add.s64 	%rd15, %rd9, %rd14;
	ld.global.u32 	%r226, [%rd15];
	min.s32 	%r4, %r226, %r216;
	add.s32 	%r227, %r221, 12288;
	mul.wide.s32 	%rd16, %r227, 4;
	add.s64 	%rd17, %rd9, %rd16;
	ld.global.u32 	%r228, [%rd17];
	min.s32 	%r5, %r228, %r216;
	add.s32 	%r6, %r2, 1024;
	shr.s32 	%r229, %r6, 31;
	shr.u32 	%r230, %r229, 22;
	add.s32 	%r231, %r6, %r230;
	shr.s32 	%r7, %r231, 10;
	add.s32 	%r8, %r3, 1024;
	shr.s32 	%r232, %r8, 31;
	shr.u32 	%r233, %r232, 22;
	add.s32 	%r234, %r8, %r233;
	shr.s32 	%r9, %r234, 10;
	add.s32 	%r10, %r4, 1024;
	shr.s32 	%r235, %r10, 31;
	shr.u32 	%r236, %r235, 22;
	add.s32 	%r237, %r10, %r236;
	shr.s32 	%r11, %r237, 10;
	add.s32 	%r12, %r5, 1024;
	shr.s32 	%r238, %r12, 31;
	shr.u32 	%r239, %r238, 22;
	add.s32 	%r240, %r12, %r239;
	shr.s32 	%r13, %r240, 10;
	mul.lo.s32 	%r14, %r221, %r216;
	bar.sync 	0;
	setp.lt.s32	%p2, %r6, 1024;
	@%p2 bra 	BB3_30;

	mov.u32 	%r245, 1;
	max.s32 	%r15, %r7, %r245;
	and.b32  	%r244, %r15, 3;
	mov.u32 	%r1269, 0;
	setp.eq.s32	%p3, %r244, 0;
	@%p3 bra 	BB3_16;

	setp.eq.s32	%p4, %r244, 1;
	@%p4 bra 	BB3_12;

	setp.eq.s32	%p5, %r244, 2;
	@%p5 bra 	BB3_8;

	setp.ge.s32	%p6, %r1, %r2;
	@%p6 bra 	BB3_5;

	add.s32 	%r250, %r1, %r14;
	mul.wide.s32 	%rd19, %r250, 8;
	add.s64 	%rd18, %rd2, %rd19;
	// inline asm
	ld.global.cs.v2.u32 {%r247,%r248}, [%rd18];
	// inline asm
	or.b32  	%r251, %r247, %r248;
	setp.eq.s32	%p7, %r251, 0;
	mov.u32 	%r1269, %r245;
	@%p7 bra 	BB3_8;

	and.b32  	%r253, %r247, 131040;
	and.b32  	%r254, %r247, 31;
	mov.u32 	%r1269, 1;
	shl.b32 	%r255, %r1269, %r254;
	shr.u32 	%r256, %r253, 3;
	add.s32 	%r258, %r218, %r256;
	atom.shared.or.b32 	%r259, [%r258], %r255;
	bra.uni 	BB3_8;

BB3_5:
	mov.u32 	%r1269, %r245;

BB3_8:
	shl.b32 	%r260, %r1269, 10;
	add.s32 	%r18, %r260, %r1;
	setp.ge.s32	%p8, %r18, %r2;
	@%p8 bra 	BB3_11;

	add.s32 	%r263, %r18, %r14;
	mul.wide.s32 	%rd21, %r263, 8;
	add.s64 	%rd20, %rd2, %rd21;
	// inline asm
	ld.global.cs.v2.u32 {%r261,%r262}, [%rd20];
	// inline asm
	or.b32  	%r264, %r261, %r262;
	setp.eq.s32	%p9, %r264, 0;
	@%p9 bra 	BB3_11;

	and.b32  	%r265, %r261, 131040;
	and.b32  	%r266, %r261, 31;
	mov.u32 	%r267, 1;
	shl.b32 	%r268, %r267, %r266;
	shr.u32 	%r269, %r265, 3;
	add.s32 	%r271, %r218, %r269;
	atom.shared.or.b32 	%r272, [%r271], %r268;

BB3_11:
	add.s32 	%r1269, %r1269, 1;

BB3_12:
	shl.b32 	%r273, %r1269, 10;
	add.s32 	%r22, %r273, %r1;
	setp.ge.s32	%p10, %r22, %r2;
	@%p10 bra 	BB3_15;

	add.s32 	%r276, %r22, %r14;
	mul.wide.s32 	%rd23, %r276, 8;
	add.s64 	%rd22, %rd2, %rd23;
	// inline asm
	ld.global.cs.v2.u32 {%r274,%r275}, [%rd22];
	// inline asm
	or.b32  	%r277, %r274, %r275;
	setp.eq.s32	%p11, %r277, 0;
	@%p11 bra 	BB3_15;

	and.b32  	%r278, %r274, 131040;
	and.b32  	%r279, %r274, 31;
	mov.u32 	%r280, 1;
	shl.b32 	%r281, %r280, %r279;
	shr.u32 	%r282, %r278, 3;
	add.s32 	%r284, %r218, %r282;
	atom.shared.or.b32 	%r285, [%r284], %r281;

BB3_15:
	add.s32 	%r1269, %r1269, 1;

BB3_16:
	setp.lt.u32	%p12, %r15, 4;
	@%p12 bra 	BB3_30;

BB3_17:
	shl.b32 	%r286, %r1269, 10;
	add.s32 	%r27, %r286, %r1;
	setp.ge.s32	%p13, %r27, %r2;
	@%p13 bra 	BB3_20;

	add.s32 	%r289, %r27, %r14;
	mul.wide.s32 	%rd25, %r289, 8;
	add.s64 	%rd24, %rd2, %rd25;
	// inline asm
	ld.global.cs.v2.u32 {%r287,%r288}, [%rd24];
	// inline asm
	or.b32  	%r290, %r287, %r288;
	setp.eq.s32	%p14, %r290, 0;
	@%p14 bra 	BB3_20;

	and.b32  	%r291, %r287, 131040;
	and.b32  	%r292, %r287, 31;
	mov.u32 	%r293, 1;
	shl.b32 	%r294, %r293, %r292;
	shr.u32 	%r295, %r291, 3;
	add.s32 	%r297, %r218, %r295;
	atom.shared.or.b32 	%r298, [%r297], %r294;

BB3_20:
	add.s32 	%r29, %r27, 1024;
	setp.ge.s32	%p15, %r29, %r2;
	@%p15 bra 	BB3_23;

	add.s32 	%r303, %r29, %r14;
	mul.wide.s32 	%rd27, %r303, 8;
	add.s64 	%rd26, %rd2, %rd27;
	// inline asm
	ld.global.cs.v2.u32 {%r301,%r302}, [%rd26];
	// inline asm
	or.b32  	%r304, %r301, %r302;
	setp.eq.s32	%p16, %r304, 0;
	@%p16 bra 	BB3_23;

	and.b32  	%r305, %r301, 131040;
	and.b32  	%r306, %r301, 31;
	mov.u32 	%r307, 1;
	shl.b32 	%r308, %r307, %r306;
	shr.u32 	%r309, %r305, 3;
	add.s32 	%r311, %r218, %r309;
	atom.shared.or.b32 	%r312, [%r311], %r308;

BB3_23:
	add.s32 	%r31, %r27, 2048;
	setp.ge.s32	%p17, %r31, %r2;
	@%p17 bra 	BB3_26;

	add.s32 	%r317, %r31, %r14;
	mul.wide.s32 	%rd29, %r317, 8;
	add.s64 	%rd28, %rd2, %rd29;
	// inline asm
	ld.global.cs.v2.u32 {%r315,%r316}, [%rd28];
	// inline asm
	or.b32  	%r318, %r315, %r316;
	setp.eq.s32	%p18, %r318, 0;
	@%p18 bra 	BB3_26;

	and.b32  	%r319, %r315, 131040;
	and.b32  	%r320, %r315, 31;
	mov.u32 	%r321, 1;
	shl.b32 	%r322, %r321, %r320;
	shr.u32 	%r323, %r319, 3;
	add.s32 	%r325, %r218, %r323;
	atom.shared.or.b32 	%r326, [%r325], %r322;

BB3_26:
	add.s32 	%r33, %r27, 3072;
	setp.ge.s32	%p19, %r33, %r2;
	@%p19 bra 	BB3_29;

	add.s32 	%r331, %r33, %r14;
	mul.wide.s32 	%rd31, %r331, 8;
	add.s64 	%rd30, %rd2, %rd31;
	// inline asm
	ld.global.cs.v2.u32 {%r329,%r330}, [%rd30];
	// inline asm
	or.b32  	%r332, %r329, %r330;
	setp.eq.s32	%p20, %r332, 0;
	@%p20 bra 	BB3_29;

	and.b32  	%r333, %r329, 131040;
	and.b32  	%r334, %r329, 31;
	mov.u32 	%r335, 1;
	shl.b32 	%r336, %r335, %r334;
	shr.u32 	%r337, %r333, 3;
	add.s32 	%r339, %r218, %r337;
	atom.shared.or.b32 	%r340, [%r339], %r336;

BB3_29:
	add.s32 	%r1269, %r1269, 4;
	setp.lt.s32	%p21, %r1269, %r7;
	@%p21 bra 	BB3_17;

BB3_30:
	setp.lt.s32	%p22, %r8, 1024;
	@%p22 bra 	BB3_60;

	mov.u32 	%r345, 1;
	max.s32 	%r36, %r9, %r345;
	and.b32  	%r344, %r36, 3;
	mov.u32 	%r1273, 0;
	setp.eq.s32	%p23, %r344, 0;
	@%p23 bra 	BB3_46;

	setp.eq.s32	%p24, %r344, 1;
	@%p24 bra 	BB3_42;

	setp.eq.s32	%p25, %r344, 2;
	@%p25 bra 	BB3_38;

	setp.ge.s32	%p26, %r1, %r3;
	@%p26 bra 	BB3_35;

	add.s32 	%r350, %r1, %r14;
	mul.wide.s32 	%rd33, %r350, 8;
	add.s64 	%rd32, %rd3, %rd33;
	// inline asm
	ld.global.cs.v2.u32 {%r347,%r348}, [%rd32];
	// inline asm
	or.b32  	%r351, %r347, %r348;
	setp.eq.s32	%p27, %r351, 0;
	mov.u32 	%r1273, %r345;
	@%p27 bra 	BB3_38;

	and.b32  	%r353, %r347, 131040;
	and.b32  	%r354, %r347, 31;
	mov.u32 	%r1273, 1;
	shl.b32 	%r355, %r1273, %r354;
	shr.u32 	%r356, %r353, 3;
	add.s32 	%r358, %r218, %r356;
	atom.shared.or.b32 	%r359, [%r358], %r355;
	bra.uni 	BB3_38;

BB3_35:
	mov.u32 	%r1273, %r345;

BB3_38:
	shl.b32 	%r360, %r1273, 10;
	add.s32 	%r39, %r360, %r1;
	setp.ge.s32	%p28, %r39, %r3;
	@%p28 bra 	BB3_41;

	add.s32 	%r363, %r39, %r14;
	mul.wide.s32 	%rd35, %r363, 8;
	add.s64 	%rd34, %rd3, %rd35;
	// inline asm
	ld.global.cs.v2.u32 {%r361,%r362}, [%rd34];
	// inline asm
	or.b32  	%r364, %r361, %r362;
	setp.eq.s32	%p29, %r364, 0;
	@%p29 bra 	BB3_41;

	and.b32  	%r365, %r361, 131040;
	and.b32  	%r366, %r361, 31;
	mov.u32 	%r367, 1;
	shl.b32 	%r368, %r367, %r366;
	shr.u32 	%r369, %r365, 3;
	add.s32 	%r371, %r218, %r369;
	atom.shared.or.b32 	%r372, [%r371], %r368;

BB3_41:
	add.s32 	%r1273, %r1273, 1;

BB3_42:
	shl.b32 	%r373, %r1273, 10;
	add.s32 	%r43, %r373, %r1;
	setp.ge.s32	%p30, %r43, %r3;
	@%p30 bra 	BB3_45;

	add.s32 	%r376, %r43, %r14;
	mul.wide.s32 	%rd37, %r376, 8;
	add.s64 	%rd36, %rd3, %rd37;
	// inline asm
	ld.global.cs.v2.u32 {%r374,%r375}, [%rd36];
	// inline asm
	or.b32  	%r377, %r374, %r375;
	setp.eq.s32	%p31, %r377, 0;
	@%p31 bra 	BB3_45;

	and.b32  	%r378, %r374, 131040;
	and.b32  	%r379, %r374, 31;
	mov.u32 	%r380, 1;
	shl.b32 	%r381, %r380, %r379;
	shr.u32 	%r382, %r378, 3;
	add.s32 	%r384, %r218, %r382;
	atom.shared.or.b32 	%r385, [%r384], %r381;

BB3_45:
	add.s32 	%r1273, %r1273, 1;

BB3_46:
	setp.lt.u32	%p32, %r36, 4;
	@%p32 bra 	BB3_60;

BB3_47:
	shl.b32 	%r386, %r1273, 10;
	add.s32 	%r48, %r386, %r1;
	setp.ge.s32	%p33, %r48, %r3;
	@%p33 bra 	BB3_50;

	add.s32 	%r389, %r48, %r14;
	mul.wide.s32 	%rd39, %r389, 8;
	add.s64 	%rd38, %rd3, %rd39;
	// inline asm
	ld.global.cs.v2.u32 {%r387,%r388}, [%rd38];
	// inline asm
	or.b32  	%r390, %r387, %r388;
	setp.eq.s32	%p34, %r390, 0;
	@%p34 bra 	BB3_50;

	and.b32  	%r391, %r387, 131040;
	and.b32  	%r392, %r387, 31;
	mov.u32 	%r393, 1;
	shl.b32 	%r394, %r393, %r392;
	shr.u32 	%r395, %r391, 3;
	add.s32 	%r397, %r218, %r395;
	atom.shared.or.b32 	%r398, [%r397], %r394;

BB3_50:
	add.s32 	%r50, %r48, 1024;
	setp.ge.s32	%p35, %r50, %r3;
	@%p35 bra 	BB3_53;

	add.s32 	%r403, %r50, %r14;
	mul.wide.s32 	%rd41, %r403, 8;
	add.s64 	%rd40, %rd3, %rd41;
	// inline asm
	ld.global.cs.v2.u32 {%r401,%r402}, [%rd40];
	// inline asm
	or.b32  	%r404, %r401, %r402;
	setp.eq.s32	%p36, %r404, 0;
	@%p36 bra 	BB3_53;

	and.b32  	%r405, %r401, 131040;
	and.b32  	%r406, %r401, 31;
	mov.u32 	%r407, 1;
	shl.b32 	%r408, %r407, %r406;
	shr.u32 	%r409, %r405, 3;
	add.s32 	%r411, %r218, %r409;
	atom.shared.or.b32 	%r412, [%r411], %r408;

BB3_53:
	add.s32 	%r52, %r48, 2048;
	setp.ge.s32	%p37, %r52, %r3;
	@%p37 bra 	BB3_56;

	add.s32 	%r417, %r52, %r14;
	mul.wide.s32 	%rd43, %r417, 8;
	add.s64 	%rd42, %rd3, %rd43;
	// inline asm
	ld.global.cs.v2.u32 {%r415,%r416}, [%rd42];
	// inline asm
	or.b32  	%r418, %r415, %r416;
	setp.eq.s32	%p38, %r418, 0;
	@%p38 bra 	BB3_56;

	and.b32  	%r419, %r415, 131040;
	and.b32  	%r420, %r415, 31;
	mov.u32 	%r421, 1;
	shl.b32 	%r422, %r421, %r420;
	shr.u32 	%r423, %r419, 3;
	add.s32 	%r425, %r218, %r423;
	atom.shared.or.b32 	%r426, [%r425], %r422;

BB3_56:
	add.s32 	%r54, %r48, 3072;
	setp.ge.s32	%p39, %r54, %r3;
	@%p39 bra 	BB3_59;

	add.s32 	%r431, %r54, %r14;
	mul.wide.s32 	%rd45, %r431, 8;
	add.s64 	%rd44, %rd3, %rd45;
	// inline asm
	ld.global.cs.v2.u32 {%r429,%r430}, [%rd44];
	// inline asm
	or.b32  	%r432, %r429, %r430;
	setp.eq.s32	%p40, %r432, 0;
	@%p40 bra 	BB3_59;

	and.b32  	%r433, %r429, 131040;
	and.b32  	%r434, %r429, 31;
	mov.u32 	%r435, 1;
	shl.b32 	%r436, %r435, %r434;
	shr.u32 	%r437, %r433, 3;
	add.s32 	%r439, %r218, %r437;
	atom.shared.or.b32 	%r440, [%r439], %r436;

BB3_59:
	add.s32 	%r1273, %r1273, 4;
	setp.lt.s32	%p41, %r1273, %r9;
	@%p41 bra 	BB3_47;

BB3_60:
	setp.lt.s32	%p42, %r10, 1024;
	@%p42 bra 	BB3_90;

	mov.u32 	%r445, 1;
	max.s32 	%r57, %r11, %r445;
	and.b32  	%r444, %r57, 3;
	mov.u32 	%r1277, 0;
	setp.eq.s32	%p43, %r444, 0;
	@%p43 bra 	BB3_76;

	setp.eq.s32	%p44, %r444, 1;
	@%p44 bra 	BB3_72;

	setp.eq.s32	%p45, %r444, 2;
	@%p45 bra 	BB3_68;

	setp.ge.s32	%p46, %r1, %r4;
	@%p46 bra 	BB3_65;

	add.s32 	%r450, %r1, %r14;
	mul.wide.s32 	%rd47, %r450, 8;
	add.s64 	%rd46, %rd4, %rd47;
	// inline asm
	ld.global.cs.v2.u32 {%r447,%r448}, [%rd46];
	// inline asm
	or.b32  	%r451, %r447, %r448;
	setp.eq.s32	%p47, %r451, 0;
	mov.u32 	%r1277, %r445;
	@%p47 bra 	BB3_68;

	and.b32  	%r453, %r447, 131040;
	and.b32  	%r454, %r447, 31;
	mov.u32 	%r1277, 1;
	shl.b32 	%r455, %r1277, %r454;
	shr.u32 	%r456, %r453, 3;
	add.s32 	%r458, %r218, %r456;
	atom.shared.or.b32 	%r459, [%r458], %r455;
	bra.uni 	BB3_68;

BB3_65:
	mov.u32 	%r1277, %r445;

BB3_68:
	shl.b32 	%r460, %r1277, 10;
	add.s32 	%r60, %r460, %r1;
	setp.ge.s32	%p48, %r60, %r4;
	@%p48 bra 	BB3_71;

	add.s32 	%r463, %r60, %r14;
	mul.wide.s32 	%rd49, %r463, 8;
	add.s64 	%rd48, %rd4, %rd49;
	// inline asm
	ld.global.cs.v2.u32 {%r461,%r462}, [%rd48];
	// inline asm
	or.b32  	%r464, %r461, %r462;
	setp.eq.s32	%p49, %r464, 0;
	@%p49 bra 	BB3_71;

	and.b32  	%r465, %r461, 131040;
	and.b32  	%r466, %r461, 31;
	mov.u32 	%r467, 1;
	shl.b32 	%r468, %r467, %r466;
	shr.u32 	%r469, %r465, 3;
	add.s32 	%r471, %r218, %r469;
	atom.shared.or.b32 	%r472, [%r471], %r468;

BB3_71:
	add.s32 	%r1277, %r1277, 1;

BB3_72:
	shl.b32 	%r473, %r1277, 10;
	add.s32 	%r64, %r473, %r1;
	setp.ge.s32	%p50, %r64, %r4;
	@%p50 bra 	BB3_75;

	add.s32 	%r476, %r64, %r14;
	mul.wide.s32 	%rd51, %r476, 8;
	add.s64 	%rd50, %rd4, %rd51;
	// inline asm
	ld.global.cs.v2.u32 {%r474,%r475}, [%rd50];
	// inline asm
	or.b32  	%r477, %r474, %r475;
	setp.eq.s32	%p51, %r477, 0;
	@%p51 bra 	BB3_75;

	and.b32  	%r478, %r474, 131040;
	and.b32  	%r479, %r474, 31;
	mov.u32 	%r480, 1;
	shl.b32 	%r481, %r480, %r479;
	shr.u32 	%r482, %r478, 3;
	add.s32 	%r484, %r218, %r482;
	atom.shared.or.b32 	%r485, [%r484], %r481;

BB3_75:
	add.s32 	%r1277, %r1277, 1;

BB3_76:
	setp.lt.u32	%p52, %r57, 4;
	@%p52 bra 	BB3_90;

BB3_77:
	shl.b32 	%r486, %r1277, 10;
	add.s32 	%r69, %r486, %r1;
	setp.ge.s32	%p53, %r69, %r4;
	@%p53 bra 	BB3_80;

	add.s32 	%r489, %r69, %r14;
	mul.wide.s32 	%rd53, %r489, 8;
	add.s64 	%rd52, %rd4, %rd53;
	// inline asm
	ld.global.cs.v2.u32 {%r487,%r488}, [%rd52];
	// inline asm
	or.b32  	%r490, %r487, %r488;
	setp.eq.s32	%p54, %r490, 0;
	@%p54 bra 	BB3_80;

	and.b32  	%r491, %r487, 131040;
	and.b32  	%r492, %r487, 31;
	mov.u32 	%r493, 1;
	shl.b32 	%r494, %r493, %r492;
	shr.u32 	%r495, %r491, 3;
	add.s32 	%r497, %r218, %r495;
	atom.shared.or.b32 	%r498, [%r497], %r494;

BB3_80:
	add.s32 	%r71, %r69, 1024;
	setp.ge.s32	%p55, %r71, %r4;
	@%p55 bra 	BB3_83;

	add.s32 	%r503, %r71, %r14;
	mul.wide.s32 	%rd55, %r503, 8;
	add.s64 	%rd54, %rd4, %rd55;
	// inline asm
	ld.global.cs.v2.u32 {%r501,%r502}, [%rd54];
	// inline asm
	or.b32  	%r504, %r501, %r502;
	setp.eq.s32	%p56, %r504, 0;
	@%p56 bra 	BB3_83;

	and.b32  	%r505, %r501, 131040;
	and.b32  	%r506, %r501, 31;
	mov.u32 	%r507, 1;
	shl.b32 	%r508, %r507, %r506;
	shr.u32 	%r509, %r505, 3;
	add.s32 	%r511, %r218, %r509;
	atom.shared.or.b32 	%r512, [%r511], %r508;

BB3_83:
	add.s32 	%r73, %r69, 2048;
	setp.ge.s32	%p57, %r73, %r4;
	@%p57 bra 	BB3_86;

	add.s32 	%r517, %r73, %r14;
	mul.wide.s32 	%rd57, %r517, 8;
	add.s64 	%rd56, %rd4, %rd57;
	// inline asm
	ld.global.cs.v2.u32 {%r515,%r516}, [%rd56];
	// inline asm
	or.b32  	%r518, %r515, %r516;
	setp.eq.s32	%p58, %r518, 0;
	@%p58 bra 	BB3_86;

	and.b32  	%r519, %r515, 131040;
	and.b32  	%r520, %r515, 31;
	mov.u32 	%r521, 1;
	shl.b32 	%r522, %r521, %r520;
	shr.u32 	%r523, %r519, 3;
	add.s32 	%r525, %r218, %r523;
	atom.shared.or.b32 	%r526, [%r525], %r522;

BB3_86:
	add.s32 	%r75, %r69, 3072;
	setp.ge.s32	%p59, %r75, %r4;
	@%p59 bra 	BB3_89;

	add.s32 	%r531, %r75, %r14;
	mul.wide.s32 	%rd59, %r531, 8;
	add.s64 	%rd58, %rd4, %rd59;
	// inline asm
	ld.global.cs.v2.u32 {%r529,%r530}, [%rd58];
	// inline asm
	or.b32  	%r532, %r529, %r530;
	setp.eq.s32	%p60, %r532, 0;
	@%p60 bra 	BB3_89;

	and.b32  	%r533, %r529, 131040;
	and.b32  	%r534, %r529, 31;
	mov.u32 	%r535, 1;
	shl.b32 	%r536, %r535, %r534;
	shr.u32 	%r537, %r533, 3;
	add.s32 	%r539, %r218, %r537;
	atom.shared.or.b32 	%r540, [%r539], %r536;

BB3_89:
	add.s32 	%r1277, %r1277, 4;
	setp.lt.s32	%p61, %r1277, %r11;
	@%p61 bra 	BB3_77;

BB3_90:
	setp.lt.s32	%p62, %r12, 1024;
	@%p62 bra 	BB3_120;

	mov.u32 	%r545, 1;
	max.s32 	%r78, %r13, %r545;
	and.b32  	%r544, %r78, 3;
	mov.u32 	%r1281, 0;
	setp.eq.s32	%p63, %r544, 0;
	@%p63 bra 	BB3_106;

	setp.eq.s32	%p64, %r544, 1;
	@%p64 bra 	BB3_102;

	setp.eq.s32	%p65, %r544, 2;
	@%p65 bra 	BB3_98;

	setp.ge.s32	%p66, %r1, %r5;
	@%p66 bra 	BB3_95;

	add.s32 	%r550, %r1, %r14;
	mul.wide.s32 	%rd61, %r550, 8;
	add.s64 	%rd60, %rd5, %rd61;
	// inline asm
	ld.global.cs.v2.u32 {%r547,%r548}, [%rd60];
	// inline asm
	or.b32  	%r551, %r547, %r548;
	setp.eq.s32	%p67, %r551, 0;
	mov.u32 	%r1281, %r545;
	@%p67 bra 	BB3_98;

	and.b32  	%r553, %r547, 131040;
	and.b32  	%r554, %r547, 31;
	mov.u32 	%r1281, 1;
	shl.b32 	%r555, %r1281, %r554;
	shr.u32 	%r556, %r553, 3;
	add.s32 	%r558, %r218, %r556;
	atom.shared.or.b32 	%r559, [%r558], %r555;
	bra.uni 	BB3_98;

BB3_95:
	mov.u32 	%r1281, %r545;

BB3_98:
	shl.b32 	%r560, %r1281, 10;
	add.s32 	%r81, %r560, %r1;
	setp.ge.s32	%p68, %r81, %r5;
	@%p68 bra 	BB3_101;

	add.s32 	%r563, %r81, %r14;
	mul.wide.s32 	%rd63, %r563, 8;
	add.s64 	%rd62, %rd5, %rd63;
	// inline asm
	ld.global.cs.v2.u32 {%r561,%r562}, [%rd62];
	// inline asm
	or.b32  	%r564, %r561, %r562;
	setp.eq.s32	%p69, %r564, 0;
	@%p69 bra 	BB3_101;

	and.b32  	%r565, %r561, 131040;
	and.b32  	%r566, %r561, 31;
	mov.u32 	%r567, 1;
	shl.b32 	%r568, %r567, %r566;
	shr.u32 	%r569, %r565, 3;
	add.s32 	%r571, %r218, %r569;
	atom.shared.or.b32 	%r572, [%r571], %r568;

BB3_101:
	add.s32 	%r1281, %r1281, 1;

BB3_102:
	shl.b32 	%r573, %r1281, 10;
	add.s32 	%r85, %r573, %r1;
	setp.ge.s32	%p70, %r85, %r5;
	@%p70 bra 	BB3_105;

	add.s32 	%r576, %r85, %r14;
	mul.wide.s32 	%rd65, %r576, 8;
	add.s64 	%rd64, %rd5, %rd65;
	// inline asm
	ld.global.cs.v2.u32 {%r574,%r575}, [%rd64];
	// inline asm
	or.b32  	%r577, %r574, %r575;
	setp.eq.s32	%p71, %r577, 0;
	@%p71 bra 	BB3_105;

	and.b32  	%r578, %r574, 131040;
	and.b32  	%r579, %r574, 31;
	mov.u32 	%r580, 1;
	shl.b32 	%r581, %r580, %r579;
	shr.u32 	%r582, %r578, 3;
	add.s32 	%r584, %r218, %r582;
	atom.shared.or.b32 	%r585, [%r584], %r581;

BB3_105:
	add.s32 	%r1281, %r1281, 1;

BB3_106:
	setp.lt.u32	%p72, %r78, 4;
	@%p72 bra 	BB3_120;

BB3_107:
	shl.b32 	%r586, %r1281, 10;
	add.s32 	%r90, %r586, %r1;
	setp.ge.s32	%p73, %r90, %r5;
	@%p73 bra 	BB3_110;

	add.s32 	%r589, %r90, %r14;
	mul.wide.s32 	%rd67, %r589, 8;
	add.s64 	%rd66, %rd5, %rd67;
	// inline asm
	ld.global.cs.v2.u32 {%r587,%r588}, [%rd66];
	// inline asm
	or.b32  	%r590, %r587, %r588;
	setp.eq.s32	%p74, %r590, 0;
	@%p74 bra 	BB3_110;

	and.b32  	%r591, %r587, 131040;
	and.b32  	%r592, %r587, 31;
	mov.u32 	%r593, 1;
	shl.b32 	%r594, %r593, %r592;
	shr.u32 	%r595, %r591, 3;
	add.s32 	%r597, %r218, %r595;
	atom.shared.or.b32 	%r598, [%r597], %r594;

BB3_110:
	add.s32 	%r92, %r90, 1024;
	setp.ge.s32	%p75, %r92, %r5;
	@%p75 bra 	BB3_113;

	add.s32 	%r603, %r92, %r14;
	mul.wide.s32 	%rd69, %r603, 8;
	add.s64 	%rd68, %rd5, %rd69;
	// inline asm
	ld.global.cs.v2.u32 {%r601,%r602}, [%rd68];
	// inline asm
	or.b32  	%r604, %r601, %r602;
	setp.eq.s32	%p76, %r604, 0;
	@%p76 bra 	BB3_113;

	and.b32  	%r605, %r601, 131040;
	and.b32  	%r606, %r601, 31;
	mov.u32 	%r607, 1;
	shl.b32 	%r608, %r607, %r606;
	shr.u32 	%r609, %r605, 3;
	add.s32 	%r611, %r218, %r609;
	atom.shared.or.b32 	%r612, [%r611], %r608;

BB3_113:
	add.s32 	%r94, %r90, 2048;
	setp.ge.s32	%p77, %r94, %r5;
	@%p77 bra 	BB3_116;

	add.s32 	%r617, %r94, %r14;
	mul.wide.s32 	%rd71, %r617, 8;
	add.s64 	%rd70, %rd5, %rd71;
	// inline asm
	ld.global.cs.v2.u32 {%r615,%r616}, [%rd70];
	// inline asm
	or.b32  	%r618, %r615, %r616;
	setp.eq.s32	%p78, %r618, 0;
	@%p78 bra 	BB3_116;

	and.b32  	%r619, %r615, 131040;
	and.b32  	%r620, %r615, 31;
	mov.u32 	%r621, 1;
	shl.b32 	%r622, %r621, %r620;
	shr.u32 	%r623, %r619, 3;
	add.s32 	%r625, %r218, %r623;
	atom.shared.or.b32 	%r626, [%r625], %r622;

BB3_116:
	add.s32 	%r96, %r90, 3072;
	setp.ge.s32	%p79, %r96, %r5;
	@%p79 bra 	BB3_119;

	add.s32 	%r631, %r96, %r14;
	mul.wide.s32 	%rd73, %r631, 8;
	add.s64 	%rd72, %rd5, %rd73;
	// inline asm
	ld.global.cs.v2.u32 {%r629,%r630}, [%rd72];
	// inline asm
	or.b32  	%r632, %r629, %r630;
	setp.eq.s32	%p80, %r632, 0;
	@%p80 bra 	BB3_119;

	and.b32  	%r633, %r629, 131040;
	and.b32  	%r634, %r629, 31;
	mov.u32 	%r635, 1;
	shl.b32 	%r636, %r635, %r634;
	shr.u32 	%r637, %r633, 3;
	add.s32 	%r639, %r218, %r637;
	atom.shared.or.b32 	%r640, [%r639], %r636;

BB3_119:
	add.s32 	%r1281, %r1281, 4;
	setp.lt.s32	%p81, %r1281, %r13;
	@%p81 bra 	BB3_107;

BB3_120:
	setp.gt.s32	%p1, %r6, 1023;
	bar.sync 	0;
	@!%p1 bra 	BB3_157;
	bra.uni 	BB3_121;

BB3_121:
	add.s32 	%r99, %r215, -2;
	mov.u32 	%r645, 1;
	max.s32 	%r100, %r7, %r645;
	and.b32  	%r644, %r100, 3;
	mov.u32 	%r1285, 0;
	setp.eq.s32	%p82, %r644, 0;
	@%p82 bra 	BB3_139;

	setp.eq.s32	%p83, %r644, 1;
	@%p83 bra 	BB3_134;

	setp.eq.s32	%p84, %r644, 2;
	@%p84 bra 	BB3_129;

	setp.ge.s32	%p85, %r1, %r2;
	@%p85 bra 	BB3_125;

	add.s32 	%r650, %r1, %r14;
	mul.wide.s32 	%rd75, %r650, 8;
	add.s64 	%rd74, %rd2, %rd75;
	// inline asm
	ld.global.cs.v2.u32 {%r647,%r648}, [%rd74];
	// inline asm
	or.b32  	%r651, %r647, %r648;
	setp.eq.s32	%p86, %r651, 0;
	mov.u32 	%r1285, %r645;
	@%p86 bra 	BB3_129;

	and.b32  	%r653, %r647, 131040;
	and.b32  	%r654, %r647, 31;
	xor.b32  	%r655, %r654, 1;
	shr.u32 	%r656, %r653, 3;
	add.s32 	%r658, %r218, %r656;
	mov.u32 	%r1285, 1;
	shl.b32 	%r659, %r1285, %r655;
	ld.shared.u32 	%r660, [%r658];
	and.b32  	%r661, %r660, %r659;
	setp.eq.s32	%p87, %r661, 0;
	@%p87 bra 	BB3_129;

	bfe.u32 	%r665, %r648, 17, 12;
	mul.wide.u32 	%rd77, %r665, 4;
	add.s64 	%rd78, %rd1, %rd77;
	atom.global.add.u32 	%r666, [%rd78], 1;
	min.s32 	%r667, %r666, %r99;
	mad.lo.s32 	%r668, %r665, %r215, %r667;
	mul.wide.s32 	%rd79, %r668, 8;
	add.s64 	%rd76, %rd6, %rd79;
	// inline asm
	st.global.cg.v2.u32 [%rd76], {%r648, %r647};
	// inline asm
	bra.uni 	BB3_129;

BB3_125:
	mov.u32 	%r1285, %r645;

BB3_129:
	shl.b32 	%r669, %r1285, 10;
	add.s32 	%r104, %r669, %r1;
	setp.ge.s32	%p88, %r104, %r2;
	@%p88 bra 	BB3_133;

	add.s32 	%r672, %r104, %r14;
	mul.wide.s32 	%rd81, %r672, 8;
	add.s64 	%rd80, %rd2, %rd81;
	// inline asm
	ld.global.cs.v2.u32 {%r670,%r671}, [%rd80];
	// inline asm
	or.b32  	%r673, %r670, %r671;
	setp.eq.s32	%p89, %r673, 0;
	@%p89 bra 	BB3_133;

	and.b32  	%r674, %r670, 131040;
	and.b32  	%r675, %r670, 31;
	xor.b32  	%r676, %r675, 1;
	shr.u32 	%r677, %r674, 3;
	add.s32 	%r679, %r218, %r677;
	mov.u32 	%r680, 1;
	shl.b32 	%r681, %r680, %r676;
	ld.shared.u32 	%r682, [%r679];
	and.b32  	%r683, %r682, %r681;
	setp.eq.s32	%p90, %r683, 0;
	@%p90 bra 	BB3_133;

	bfe.u32 	%r686, %r671, 17, 12;
	mul.wide.u32 	%rd83, %r686, 4;
	add.s64 	%rd84, %rd1, %rd83;
	atom.global.add.u32 	%r687, [%rd84], 1;
	min.s32 	%r688, %r687, %r99;
	mad.lo.s32 	%r689, %r686, %r215, %r688;
	mul.wide.s32 	%rd85, %r689, 8;
	add.s64 	%rd82, %rd6, %rd85;
	// inline asm
	st.global.cg.v2.u32 [%rd82], {%r671, %r670};
	// inline asm

BB3_133:
	add.s32 	%r1285, %r1285, 1;

BB3_134:
	shl.b32 	%r690, %r1285, 10;
	add.s32 	%r109, %r690, %r1;
	setp.ge.s32	%p91, %r109, %r2;
	@%p91 bra 	BB3_138;

	add.s32 	%r693, %r109, %r14;
	mul.wide.s32 	%rd87, %r693, 8;
	add.s64 	%rd86, %rd2, %rd87;
	// inline asm
	ld.global.cs.v2.u32 {%r691,%r692}, [%rd86];
	// inline asm
	or.b32  	%r694, %r691, %r692;
	setp.eq.s32	%p92, %r694, 0;
	@%p92 bra 	BB3_138;

	and.b32  	%r695, %r691, 131040;
	and.b32  	%r696, %r691, 31;
	xor.b32  	%r697, %r696, 1;
	shr.u32 	%r698, %r695, 3;
	add.s32 	%r700, %r218, %r698;
	mov.u32 	%r701, 1;
	shl.b32 	%r702, %r701, %r697;
	ld.shared.u32 	%r703, [%r700];
	and.b32  	%r704, %r703, %r702;
	setp.eq.s32	%p93, %r704, 0;
	@%p93 bra 	BB3_138;

	bfe.u32 	%r707, %r692, 17, 12;
	mul.wide.u32 	%rd89, %r707, 4;
	add.s64 	%rd90, %rd1, %rd89;
	atom.global.add.u32 	%r708, [%rd90], 1;
	min.s32 	%r709, %r708, %r99;
	mad.lo.s32 	%r710, %r707, %r215, %r709;
	mul.wide.s32 	%rd91, %r710, 8;
	add.s64 	%rd88, %rd6, %rd91;
	// inline asm
	st.global.cg.v2.u32 [%rd88], {%r692, %r691};
	// inline asm

BB3_138:
	add.s32 	%r1285, %r1285, 1;

BB3_139:
	setp.lt.u32	%p94, %r100, 4;
	@%p94 bra 	BB3_157;

BB3_140:
	shl.b32 	%r711, %r1285, 10;
	add.s32 	%r115, %r711, %r1;
	setp.ge.s32	%p95, %r115, %r2;
	@%p95 bra 	BB3_144;

	add.s32 	%r714, %r115, %r14;
	mul.wide.s32 	%rd93, %r714, 8;
	add.s64 	%rd92, %rd2, %rd93;
	// inline asm
	ld.global.cs.v2.u32 {%r712,%r713}, [%rd92];
	// inline asm
	or.b32  	%r715, %r712, %r713;
	setp.eq.s32	%p96, %r715, 0;
	@%p96 bra 	BB3_144;

	and.b32  	%r716, %r712, 131040;
	and.b32  	%r717, %r712, 31;
	xor.b32  	%r718, %r717, 1;
	shr.u32 	%r719, %r716, 3;
	add.s32 	%r721, %r218, %r719;
	mov.u32 	%r722, 1;
	shl.b32 	%r723, %r722, %r718;
	ld.shared.u32 	%r724, [%r721];
	and.b32  	%r725, %r724, %r723;
	setp.eq.s32	%p97, %r725, 0;
	@%p97 bra 	BB3_144;

	bfe.u32 	%r728, %r713, 17, 12;
	mul.wide.u32 	%rd95, %r728, 4;
	add.s64 	%rd96, %rd1, %rd95;
	atom.global.add.u32 	%r729, [%rd96], 1;
	min.s32 	%r730, %r729, %r99;
	mad.lo.s32 	%r731, %r728, %r215, %r730;
	mul.wide.s32 	%rd97, %r731, 8;
	add.s64 	%rd94, %rd6, %rd97;
	// inline asm
	st.global.cg.v2.u32 [%rd94], {%r713, %r712};
	// inline asm

BB3_144:
	add.s32 	%r118, %r115, 1024;
	setp.ge.s32	%p98, %r118, %r2;
	@%p98 bra 	BB3_148;

	add.s32 	%r736, %r118, %r14;
	mul.wide.s32 	%rd99, %r736, 8;
	add.s64 	%rd98, %rd2, %rd99;
	// inline asm
	ld.global.cs.v2.u32 {%r734,%r735}, [%rd98];
	// inline asm
	or.b32  	%r737, %r734, %r735;
	setp.eq.s32	%p99, %r737, 0;
	@%p99 bra 	BB3_148;

	and.b32  	%r738, %r734, 131040;
	and.b32  	%r739, %r734, 31;
	xor.b32  	%r740, %r739, 1;
	shr.u32 	%r741, %r738, 3;
	add.s32 	%r743, %r218, %r741;
	mov.u32 	%r744, 1;
	shl.b32 	%r745, %r744, %r740;
	ld.shared.u32 	%r746, [%r743];
	and.b32  	%r747, %r746, %r745;
	setp.eq.s32	%p100, %r747, 0;
	@%p100 bra 	BB3_148;

	bfe.u32 	%r750, %r735, 17, 12;
	mul.wide.u32 	%rd101, %r750, 4;
	add.s64 	%rd102, %rd1, %rd101;
	atom.global.add.u32 	%r751, [%rd102], 1;
	min.s32 	%r752, %r751, %r99;
	mad.lo.s32 	%r753, %r750, %r215, %r752;
	mul.wide.s32 	%rd103, %r753, 8;
	add.s64 	%rd100, %rd6, %rd103;
	// inline asm
	st.global.cg.v2.u32 [%rd100], {%r735, %r734};
	// inline asm

BB3_148:
	add.s32 	%r121, %r115, 2048;
	setp.ge.s32	%p101, %r121, %r2;
	@%p101 bra 	BB3_152;

	add.s32 	%r758, %r121, %r14;
	mul.wide.s32 	%rd105, %r758, 8;
	add.s64 	%rd104, %rd2, %rd105;
	// inline asm
	ld.global.cs.v2.u32 {%r756,%r757}, [%rd104];
	// inline asm
	or.b32  	%r759, %r756, %r757;
	setp.eq.s32	%p102, %r759, 0;
	@%p102 bra 	BB3_152;

	and.b32  	%r760, %r756, 131040;
	and.b32  	%r761, %r756, 31;
	xor.b32  	%r762, %r761, 1;
	shr.u32 	%r763, %r760, 3;
	add.s32 	%r765, %r218, %r763;
	mov.u32 	%r766, 1;
	shl.b32 	%r767, %r766, %r762;
	ld.shared.u32 	%r768, [%r765];
	and.b32  	%r769, %r768, %r767;
	setp.eq.s32	%p103, %r769, 0;
	@%p103 bra 	BB3_152;

	bfe.u32 	%r772, %r757, 17, 12;
	mul.wide.u32 	%rd107, %r772, 4;
	add.s64 	%rd108, %rd1, %rd107;
	atom.global.add.u32 	%r773, [%rd108], 1;
	min.s32 	%r774, %r773, %r99;
	mad.lo.s32 	%r775, %r772, %r215, %r774;
	mul.wide.s32 	%rd109, %r775, 8;
	add.s64 	%rd106, %rd6, %rd109;
	// inline asm
	st.global.cg.v2.u32 [%rd106], {%r757, %r756};
	// inline asm

BB3_152:
	add.s32 	%r124, %r115, 3072;
	setp.ge.s32	%p104, %r124, %r2;
	@%p104 bra 	BB3_156;

	add.s32 	%r780, %r124, %r14;
	mul.wide.s32 	%rd111, %r780, 8;
	add.s64 	%rd110, %rd2, %rd111;
	// inline asm
	ld.global.cs.v2.u32 {%r778,%r779}, [%rd110];
	// inline asm
	or.b32  	%r781, %r778, %r779;
	setp.eq.s32	%p105, %r781, 0;
	@%p105 bra 	BB3_156;

	and.b32  	%r782, %r778, 131040;
	and.b32  	%r783, %r778, 31;
	xor.b32  	%r784, %r783, 1;
	shr.u32 	%r785, %r782, 3;
	add.s32 	%r787, %r218, %r785;
	mov.u32 	%r788, 1;
	shl.b32 	%r789, %r788, %r784;
	ld.shared.u32 	%r790, [%r787];
	and.b32  	%r791, %r790, %r789;
	setp.eq.s32	%p106, %r791, 0;
	@%p106 bra 	BB3_156;

	bfe.u32 	%r794, %r779, 17, 12;
	mul.wide.u32 	%rd113, %r794, 4;
	add.s64 	%rd114, %rd1, %rd113;
	atom.global.add.u32 	%r795, [%rd114], 1;
	min.s32 	%r796, %r795, %r99;
	mad.lo.s32 	%r797, %r794, %r215, %r796;
	mul.wide.s32 	%rd115, %r797, 8;
	add.s64 	%rd112, %rd6, %rd115;
	// inline asm
	st.global.cg.v2.u32 [%rd112], {%r779, %r778};
	// inline asm

BB3_156:
	add.s32 	%r1285, %r1285, 4;
	setp.lt.s32	%p107, %r1285, %r7;
	@%p107 bra 	BB3_140;

BB3_157:
	@%p22 bra 	BB3_194;

	add.s32 	%r128, %r215, -2;
	mov.u32 	%r802, 1;
	max.s32 	%r129, %r9, %r802;
	and.b32  	%r801, %r129, 3;
	mov.u32 	%r1289, 0;
	setp.eq.s32	%p109, %r801, 0;
	@%p109 bra 	BB3_176;

	setp.eq.s32	%p110, %r801, 1;
	@%p110 bra 	BB3_171;

	setp.eq.s32	%p111, %r801, 2;
	@%p111 bra 	BB3_166;

	setp.ge.s32	%p112, %r1, %r3;
	@%p112 bra 	BB3_162;

	add.s32 	%r807, %r1, %r14;
	mul.wide.s32 	%rd117, %r807, 8;
	add.s64 	%rd116, %rd3, %rd117;
	// inline asm
	ld.global.cs.v2.u32 {%r804,%r805}, [%rd116];
	// inline asm
	or.b32  	%r808, %r804, %r805;
	setp.eq.s32	%p113, %r808, 0;
	mov.u32 	%r1289, %r802;
	@%p113 bra 	BB3_166;

	and.b32  	%r810, %r804, 131040;
	and.b32  	%r811, %r804, 31;
	xor.b32  	%r812, %r811, 1;
	shr.u32 	%r813, %r810, 3;
	add.s32 	%r815, %r218, %r813;
	mov.u32 	%r1289, 1;
	shl.b32 	%r816, %r1289, %r812;
	ld.shared.u32 	%r817, [%r815];
	and.b32  	%r818, %r817, %r816;
	setp.eq.s32	%p114, %r818, 0;
	@%p114 bra 	BB3_166;

	bfe.u32 	%r822, %r805, 17, 12;
	mul.wide.u32 	%rd119, %r822, 4;
	add.s64 	%rd120, %rd1, %rd119;
	atom.global.add.u32 	%r823, [%rd120], 1;
	min.s32 	%r824, %r823, %r128;
	mad.lo.s32 	%r825, %r822, %r215, %r824;
	mul.wide.s32 	%rd121, %r825, 8;
	add.s64 	%rd118, %rd6, %rd121;
	// inline asm
	st.global.cg.v2.u32 [%rd118], {%r805, %r804};
	// inline asm
	bra.uni 	BB3_166;

BB3_162:
	mov.u32 	%r1289, %r802;

BB3_166:
	shl.b32 	%r826, %r1289, 10;
	add.s32 	%r133, %r826, %r1;
	setp.ge.s32	%p115, %r133, %r3;
	@%p115 bra 	BB3_170;

	add.s32 	%r829, %r133, %r14;
	mul.wide.s32 	%rd123, %r829, 8;
	add.s64 	%rd122, %rd3, %rd123;
	// inline asm
	ld.global.cs.v2.u32 {%r827,%r828}, [%rd122];
	// inline asm
	or.b32  	%r830, %r827, %r828;
	setp.eq.s32	%p116, %r830, 0;
	@%p116 bra 	BB3_170;

	and.b32  	%r831, %r827, 131040;
	and.b32  	%r832, %r827, 31;
	xor.b32  	%r833, %r832, 1;
	shr.u32 	%r834, %r831, 3;
	add.s32 	%r836, %r218, %r834;
	mov.u32 	%r837, 1;
	shl.b32 	%r838, %r837, %r833;
	ld.shared.u32 	%r839, [%r836];
	and.b32  	%r840, %r839, %r838;
	setp.eq.s32	%p117, %r840, 0;
	@%p117 bra 	BB3_170;

	bfe.u32 	%r843, %r828, 17, 12;
	mul.wide.u32 	%rd125, %r843, 4;
	add.s64 	%rd126, %rd1, %rd125;
	atom.global.add.u32 	%r844, [%rd126], 1;
	min.s32 	%r845, %r844, %r128;
	mad.lo.s32 	%r846, %r843, %r215, %r845;
	mul.wide.s32 	%rd127, %r846, 8;
	add.s64 	%rd124, %rd6, %rd127;
	// inline asm
	st.global.cg.v2.u32 [%rd124], {%r828, %r827};
	// inline asm

BB3_170:
	add.s32 	%r1289, %r1289, 1;

BB3_171:
	shl.b32 	%r847, %r1289, 10;
	add.s32 	%r138, %r847, %r1;
	setp.ge.s32	%p118, %r138, %r3;
	@%p118 bra 	BB3_175;

	add.s32 	%r850, %r138, %r14;
	mul.wide.s32 	%rd129, %r850, 8;
	add.s64 	%rd128, %rd3, %rd129;
	// inline asm
	ld.global.cs.v2.u32 {%r848,%r849}, [%rd128];
	// inline asm
	or.b32  	%r851, %r848, %r849;
	setp.eq.s32	%p119, %r851, 0;
	@%p119 bra 	BB3_175;

	and.b32  	%r852, %r848, 131040;
	and.b32  	%r853, %r848, 31;
	xor.b32  	%r854, %r853, 1;
	shr.u32 	%r855, %r852, 3;
	add.s32 	%r857, %r218, %r855;
	mov.u32 	%r858, 1;
	shl.b32 	%r859, %r858, %r854;
	ld.shared.u32 	%r860, [%r857];
	and.b32  	%r861, %r860, %r859;
	setp.eq.s32	%p120, %r861, 0;
	@%p120 bra 	BB3_175;

	bfe.u32 	%r864, %r849, 17, 12;
	mul.wide.u32 	%rd131, %r864, 4;
	add.s64 	%rd132, %rd1, %rd131;
	atom.global.add.u32 	%r865, [%rd132], 1;
	min.s32 	%r866, %r865, %r128;
	mad.lo.s32 	%r867, %r864, %r215, %r866;
	mul.wide.s32 	%rd133, %r867, 8;
	add.s64 	%rd130, %rd6, %rd133;
	// inline asm
	st.global.cg.v2.u32 [%rd130], {%r849, %r848};
	// inline asm

BB3_175:
	add.s32 	%r1289, %r1289, 1;

BB3_176:
	setp.lt.u32	%p121, %r129, 4;
	@%p121 bra 	BB3_194;

BB3_177:
	shl.b32 	%r868, %r1289, 10;
	add.s32 	%r144, %r868, %r1;
	setp.ge.s32	%p122, %r144, %r3;
	@%p122 bra 	BB3_181;

	add.s32 	%r871, %r144, %r14;
	mul.wide.s32 	%rd135, %r871, 8;
	add.s64 	%rd134, %rd3, %rd135;
	// inline asm
	ld.global.cs.v2.u32 {%r869,%r870}, [%rd134];
	// inline asm
	or.b32  	%r872, %r869, %r870;
	setp.eq.s32	%p123, %r872, 0;
	@%p123 bra 	BB3_181;

	and.b32  	%r873, %r869, 131040;
	and.b32  	%r874, %r869, 31;
	xor.b32  	%r875, %r874, 1;
	shr.u32 	%r876, %r873, 3;
	add.s32 	%r878, %r218, %r876;
	mov.u32 	%r879, 1;
	shl.b32 	%r880, %r879, %r875;
	ld.shared.u32 	%r881, [%r878];
	and.b32  	%r882, %r881, %r880;
	setp.eq.s32	%p124, %r882, 0;
	@%p124 bra 	BB3_181;

	bfe.u32 	%r885, %r870, 17, 12;
	mul.wide.u32 	%rd137, %r885, 4;
	add.s64 	%rd138, %rd1, %rd137;
	atom.global.add.u32 	%r886, [%rd138], 1;
	min.s32 	%r887, %r886, %r128;
	mad.lo.s32 	%r888, %r885, %r215, %r887;
	mul.wide.s32 	%rd139, %r888, 8;
	add.s64 	%rd136, %rd6, %rd139;
	// inline asm
	st.global.cg.v2.u32 [%rd136], {%r870, %r869};
	// inline asm

BB3_181:
	add.s32 	%r147, %r144, 1024;
	setp.ge.s32	%p125, %r147, %r3;
	@%p125 bra 	BB3_185;

	add.s32 	%r893, %r147, %r14;
	mul.wide.s32 	%rd141, %r893, 8;
	add.s64 	%rd140, %rd3, %rd141;
	// inline asm
	ld.global.cs.v2.u32 {%r891,%r892}, [%rd140];
	// inline asm
	or.b32  	%r894, %r891, %r892;
	setp.eq.s32	%p126, %r894, 0;
	@%p126 bra 	BB3_185;

	and.b32  	%r895, %r891, 131040;
	and.b32  	%r896, %r891, 31;
	xor.b32  	%r897, %r896, 1;
	shr.u32 	%r898, %r895, 3;
	add.s32 	%r900, %r218, %r898;
	mov.u32 	%r901, 1;
	shl.b32 	%r902, %r901, %r897;
	ld.shared.u32 	%r903, [%r900];
	and.b32  	%r904, %r903, %r902;
	setp.eq.s32	%p127, %r904, 0;
	@%p127 bra 	BB3_185;

	bfe.u32 	%r907, %r892, 17, 12;
	mul.wide.u32 	%rd143, %r907, 4;
	add.s64 	%rd144, %rd1, %rd143;
	atom.global.add.u32 	%r908, [%rd144], 1;
	min.s32 	%r909, %r908, %r128;
	mad.lo.s32 	%r910, %r907, %r215, %r909;
	mul.wide.s32 	%rd145, %r910, 8;
	add.s64 	%rd142, %rd6, %rd145;
	// inline asm
	st.global.cg.v2.u32 [%rd142], {%r892, %r891};
	// inline asm

BB3_185:
	add.s32 	%r150, %r144, 2048;
	setp.ge.s32	%p128, %r150, %r3;
	@%p128 bra 	BB3_189;

	add.s32 	%r915, %r150, %r14;
	mul.wide.s32 	%rd147, %r915, 8;
	add.s64 	%rd146, %rd3, %rd147;
	// inline asm
	ld.global.cs.v2.u32 {%r913,%r914}, [%rd146];
	// inline asm
	or.b32  	%r916, %r913, %r914;
	setp.eq.s32	%p129, %r916, 0;
	@%p129 bra 	BB3_189;

	and.b32  	%r917, %r913, 131040;
	and.b32  	%r918, %r913, 31;
	xor.b32  	%r919, %r918, 1;
	shr.u32 	%r920, %r917, 3;
	add.s32 	%r922, %r218, %r920;
	mov.u32 	%r923, 1;
	shl.b32 	%r924, %r923, %r919;
	ld.shared.u32 	%r925, [%r922];
	and.b32  	%r926, %r925, %r924;
	setp.eq.s32	%p130, %r926, 0;
	@%p130 bra 	BB3_189;

	bfe.u32 	%r929, %r914, 17, 12;
	mul.wide.u32 	%rd149, %r929, 4;
	add.s64 	%rd150, %rd1, %rd149;
	atom.global.add.u32 	%r930, [%rd150], 1;
	min.s32 	%r931, %r930, %r128;
	mad.lo.s32 	%r932, %r929, %r215, %r931;
	mul.wide.s32 	%rd151, %r932, 8;
	add.s64 	%rd148, %rd6, %rd151;
	// inline asm
	st.global.cg.v2.u32 [%rd148], {%r914, %r913};
	// inline asm

BB3_189:
	add.s32 	%r153, %r144, 3072;
	setp.ge.s32	%p131, %r153, %r3;
	@%p131 bra 	BB3_193;

	add.s32 	%r937, %r153, %r14;
	mul.wide.s32 	%rd153, %r937, 8;
	add.s64 	%rd152, %rd3, %rd153;
	// inline asm
	ld.global.cs.v2.u32 {%r935,%r936}, [%rd152];
	// inline asm
	or.b32  	%r938, %r935, %r936;
	setp.eq.s32	%p132, %r938, 0;
	@%p132 bra 	BB3_193;

	and.b32  	%r939, %r935, 131040;
	and.b32  	%r940, %r935, 31;
	xor.b32  	%r941, %r940, 1;
	shr.u32 	%r942, %r939, 3;
	add.s32 	%r944, %r218, %r942;
	mov.u32 	%r945, 1;
	shl.b32 	%r946, %r945, %r941;
	ld.shared.u32 	%r947, [%r944];
	and.b32  	%r948, %r947, %r946;
	setp.eq.s32	%p133, %r948, 0;
	@%p133 bra 	BB3_193;

	bfe.u32 	%r951, %r936, 17, 12;
	mul.wide.u32 	%rd155, %r951, 4;
	add.s64 	%rd156, %rd1, %rd155;
	atom.global.add.u32 	%r952, [%rd156], 1;
	min.s32 	%r953, %r952, %r128;
	mad.lo.s32 	%r954, %r951, %r215, %r953;
	mul.wide.s32 	%rd157, %r954, 8;
	add.s64 	%rd154, %rd6, %rd157;
	// inline asm
	st.global.cg.v2.u32 [%rd154], {%r936, %r935};
	// inline asm

BB3_193:
	add.s32 	%r1289, %r1289, 4;
	setp.lt.s32	%p134, %r1289, %r9;
	@%p134 bra 	BB3_177;

BB3_194:
	@%p42 bra 	BB3_231;

	add.s32 	%r157, %r215, -2;
	mov.u32 	%r959, 1;
	max.s32 	%r158, %r11, %r959;
	and.b32  	%r958, %r158, 3;
	mov.u32 	%r1293, 0;
	setp.eq.s32	%p136, %r958, 0;
	@%p136 bra 	BB3_213;

	setp.eq.s32	%p137, %r958, 1;
	@%p137 bra 	BB3_208;

	setp.eq.s32	%p138, %r958, 2;
	@%p138 bra 	BB3_203;

	setp.ge.s32	%p139, %r1, %r4;
	@%p139 bra 	BB3_199;

	add.s32 	%r964, %r1, %r14;
	mul.wide.s32 	%rd159, %r964, 8;
	add.s64 	%rd158, %rd4, %rd159;
	// inline asm
	ld.global.cs.v2.u32 {%r961,%r962}, [%rd158];
	// inline asm
	or.b32  	%r965, %r961, %r962;
	setp.eq.s32	%p140, %r965, 0;
	mov.u32 	%r1293, %r959;
	@%p140 bra 	BB3_203;

	and.b32  	%r967, %r961, 131040;
	and.b32  	%r968, %r961, 31;
	xor.b32  	%r969, %r968, 1;
	shr.u32 	%r970, %r967, 3;
	add.s32 	%r972, %r218, %r970;
	mov.u32 	%r1293, 1;
	shl.b32 	%r973, %r1293, %r969;
	ld.shared.u32 	%r974, [%r972];
	and.b32  	%r975, %r974, %r973;
	setp.eq.s32	%p141, %r975, 0;
	@%p141 bra 	BB3_203;

	bfe.u32 	%r979, %r962, 17, 12;
	mul.wide.u32 	%rd161, %r979, 4;
	add.s64 	%rd162, %rd1, %rd161;
	atom.global.add.u32 	%r980, [%rd162], 1;
	min.s32 	%r981, %r980, %r157;
	mad.lo.s32 	%r982, %r979, %r215, %r981;
	mul.wide.s32 	%rd163, %r982, 8;
	add.s64 	%rd160, %rd6, %rd163;
	// inline asm
	st.global.cg.v2.u32 [%rd160], {%r962, %r961};
	// inline asm
	bra.uni 	BB3_203;

BB3_199:
	mov.u32 	%r1293, %r959;

BB3_203:
	shl.b32 	%r983, %r1293, 10;
	add.s32 	%r162, %r983, %r1;
	setp.ge.s32	%p142, %r162, %r4;
	@%p142 bra 	BB3_207;

	add.s32 	%r986, %r162, %r14;
	mul.wide.s32 	%rd165, %r986, 8;
	add.s64 	%rd164, %rd4, %rd165;
	// inline asm
	ld.global.cs.v2.u32 {%r984,%r985}, [%rd164];
	// inline asm
	or.b32  	%r987, %r984, %r985;
	setp.eq.s32	%p143, %r987, 0;
	@%p143 bra 	BB3_207;

	and.b32  	%r988, %r984, 131040;
	and.b32  	%r989, %r984, 31;
	xor.b32  	%r990, %r989, 1;
	shr.u32 	%r991, %r988, 3;
	add.s32 	%r993, %r218, %r991;
	mov.u32 	%r994, 1;
	shl.b32 	%r995, %r994, %r990;
	ld.shared.u32 	%r996, [%r993];
	and.b32  	%r997, %r996, %r995;
	setp.eq.s32	%p144, %r997, 0;
	@%p144 bra 	BB3_207;

	bfe.u32 	%r1000, %r985, 17, 12;
	mul.wide.u32 	%rd167, %r1000, 4;
	add.s64 	%rd168, %rd1, %rd167;
	atom.global.add.u32 	%r1001, [%rd168], 1;
	min.s32 	%r1002, %r1001, %r157;
	mad.lo.s32 	%r1003, %r1000, %r215, %r1002;
	mul.wide.s32 	%rd169, %r1003, 8;
	add.s64 	%rd166, %rd6, %rd169;
	// inline asm
	st.global.cg.v2.u32 [%rd166], {%r985, %r984};
	// inline asm

BB3_207:
	add.s32 	%r1293, %r1293, 1;

BB3_208:
	shl.b32 	%r1004, %r1293, 10;
	add.s32 	%r167, %r1004, %r1;
	setp.ge.s32	%p145, %r167, %r4;
	@%p145 bra 	BB3_212;

	add.s32 	%r1007, %r167, %r14;
	mul.wide.s32 	%rd171, %r1007, 8;
	add.s64 	%rd170, %rd4, %rd171;
	// inline asm
	ld.global.cs.v2.u32 {%r1005,%r1006}, [%rd170];
	// inline asm
	or.b32  	%r1008, %r1005, %r1006;
	setp.eq.s32	%p146, %r1008, 0;
	@%p146 bra 	BB3_212;

	and.b32  	%r1009, %r1005, 131040;
	and.b32  	%r1010, %r1005, 31;
	xor.b32  	%r1011, %r1010, 1;
	shr.u32 	%r1012, %r1009, 3;
	add.s32 	%r1014, %r218, %r1012;
	mov.u32 	%r1015, 1;
	shl.b32 	%r1016, %r1015, %r1011;
	ld.shared.u32 	%r1017, [%r1014];
	and.b32  	%r1018, %r1017, %r1016;
	setp.eq.s32	%p147, %r1018, 0;
	@%p147 bra 	BB3_212;

	bfe.u32 	%r1021, %r1006, 17, 12;
	mul.wide.u32 	%rd173, %r1021, 4;
	add.s64 	%rd174, %rd1, %rd173;
	atom.global.add.u32 	%r1022, [%rd174], 1;
	min.s32 	%r1023, %r1022, %r157;
	mad.lo.s32 	%r1024, %r1021, %r215, %r1023;
	mul.wide.s32 	%rd175, %r1024, 8;
	add.s64 	%rd172, %rd6, %rd175;
	// inline asm
	st.global.cg.v2.u32 [%rd172], {%r1006, %r1005};
	// inline asm

BB3_212:
	add.s32 	%r1293, %r1293, 1;

BB3_213:
	setp.lt.u32	%p148, %r158, 4;
	@%p148 bra 	BB3_231;

BB3_214:
	shl.b32 	%r1025, %r1293, 10;
	add.s32 	%r173, %r1025, %r1;
	setp.ge.s32	%p149, %r173, %r4;
	@%p149 bra 	BB3_218;

	add.s32 	%r1028, %r173, %r14;
	mul.wide.s32 	%rd177, %r1028, 8;
	add.s64 	%rd176, %rd4, %rd177;
	// inline asm
	ld.global.cs.v2.u32 {%r1026,%r1027}, [%rd176];
	// inline asm
	or.b32  	%r1029, %r1026, %r1027;
	setp.eq.s32	%p150, %r1029, 0;
	@%p150 bra 	BB3_218;

	and.b32  	%r1030, %r1026, 131040;
	and.b32  	%r1031, %r1026, 31;
	xor.b32  	%r1032, %r1031, 1;
	shr.u32 	%r1033, %r1030, 3;
	add.s32 	%r1035, %r218, %r1033;
	mov.u32 	%r1036, 1;
	shl.b32 	%r1037, %r1036, %r1032;
	ld.shared.u32 	%r1038, [%r1035];
	and.b32  	%r1039, %r1038, %r1037;
	setp.eq.s32	%p151, %r1039, 0;
	@%p151 bra 	BB3_218;

	bfe.u32 	%r1042, %r1027, 17, 12;
	mul.wide.u32 	%rd179, %r1042, 4;
	add.s64 	%rd180, %rd1, %rd179;
	atom.global.add.u32 	%r1043, [%rd180], 1;
	min.s32 	%r1044, %r1043, %r157;
	mad.lo.s32 	%r1045, %r1042, %r215, %r1044;
	mul.wide.s32 	%rd181, %r1045, 8;
	add.s64 	%rd178, %rd6, %rd181;
	// inline asm
	st.global.cg.v2.u32 [%rd178], {%r1027, %r1026};
	// inline asm

BB3_218:
	add.s32 	%r176, %r173, 1024;
	setp.ge.s32	%p152, %r176, %r4;
	@%p152 bra 	BB3_222;

	add.s32 	%r1050, %r176, %r14;
	mul.wide.s32 	%rd183, %r1050, 8;
	add.s64 	%rd182, %rd4, %rd183;
	// inline asm
	ld.global.cs.v2.u32 {%r1048,%r1049}, [%rd182];
	// inline asm
	or.b32  	%r1051, %r1048, %r1049;
	setp.eq.s32	%p153, %r1051, 0;
	@%p153 bra 	BB3_222;

	and.b32  	%r1052, %r1048, 131040;
	and.b32  	%r1053, %r1048, 31;
	xor.b32  	%r1054, %r1053, 1;
	shr.u32 	%r1055, %r1052, 3;
	add.s32 	%r1057, %r218, %r1055;
	mov.u32 	%r1058, 1;
	shl.b32 	%r1059, %r1058, %r1054;
	ld.shared.u32 	%r1060, [%r1057];
	and.b32  	%r1061, %r1060, %r1059;
	setp.eq.s32	%p154, %r1061, 0;
	@%p154 bra 	BB3_222;

	bfe.u32 	%r1064, %r1049, 17, 12;
	mul.wide.u32 	%rd185, %r1064, 4;
	add.s64 	%rd186, %rd1, %rd185;
	atom.global.add.u32 	%r1065, [%rd186], 1;
	min.s32 	%r1066, %r1065, %r157;
	mad.lo.s32 	%r1067, %r1064, %r215, %r1066;
	mul.wide.s32 	%rd187, %r1067, 8;
	add.s64 	%rd184, %rd6, %rd187;
	// inline asm
	st.global.cg.v2.u32 [%rd184], {%r1049, %r1048};
	// inline asm

BB3_222:
	add.s32 	%r179, %r173, 2048;
	setp.ge.s32	%p155, %r179, %r4;
	@%p155 bra 	BB3_226;

	add.s32 	%r1072, %r179, %r14;
	mul.wide.s32 	%rd189, %r1072, 8;
	add.s64 	%rd188, %rd4, %rd189;
	// inline asm
	ld.global.cs.v2.u32 {%r1070,%r1071}, [%rd188];
	// inline asm
	or.b32  	%r1073, %r1070, %r1071;
	setp.eq.s32	%p156, %r1073, 0;
	@%p156 bra 	BB3_226;

	and.b32  	%r1074, %r1070, 131040;
	and.b32  	%r1075, %r1070, 31;
	xor.b32  	%r1076, %r1075, 1;
	shr.u32 	%r1077, %r1074, 3;
	add.s32 	%r1079, %r218, %r1077;
	mov.u32 	%r1080, 1;
	shl.b32 	%r1081, %r1080, %r1076;
	ld.shared.u32 	%r1082, [%r1079];
	and.b32  	%r1083, %r1082, %r1081;
	setp.eq.s32	%p157, %r1083, 0;
	@%p157 bra 	BB3_226;

	bfe.u32 	%r1086, %r1071, 17, 12;
	mul.wide.u32 	%rd191, %r1086, 4;
	add.s64 	%rd192, %rd1, %rd191;
	atom.global.add.u32 	%r1087, [%rd192], 1;
	min.s32 	%r1088, %r1087, %r157;
	mad.lo.s32 	%r1089, %r1086, %r215, %r1088;
	mul.wide.s32 	%rd193, %r1089, 8;
	add.s64 	%rd190, %rd6, %rd193;
	// inline asm
	st.global.cg.v2.u32 [%rd190], {%r1071, %r1070};
	// inline asm

BB3_226:
	add.s32 	%r182, %r173, 3072;
	setp.ge.s32	%p158, %r182, %r4;
	@%p158 bra 	BB3_230;

	add.s32 	%r1094, %r182, %r14;
	mul.wide.s32 	%rd195, %r1094, 8;
	add.s64 	%rd194, %rd4, %rd195;
	// inline asm
	ld.global.cs.v2.u32 {%r1092,%r1093}, [%rd194];
	// inline asm
	or.b32  	%r1095, %r1092, %r1093;
	setp.eq.s32	%p159, %r1095, 0;
	@%p159 bra 	BB3_230;

	and.b32  	%r1096, %r1092, 131040;
	and.b32  	%r1097, %r1092, 31;
	xor.b32  	%r1098, %r1097, 1;
	shr.u32 	%r1099, %r1096, 3;
	add.s32 	%r1101, %r218, %r1099;
	mov.u32 	%r1102, 1;
	shl.b32 	%r1103, %r1102, %r1098;
	ld.shared.u32 	%r1104, [%r1101];
	and.b32  	%r1105, %r1104, %r1103;
	setp.eq.s32	%p160, %r1105, 0;
	@%p160 bra 	BB3_230;

	bfe.u32 	%r1108, %r1093, 17, 12;
	mul.wide.u32 	%rd197, %r1108, 4;
	add.s64 	%rd198, %rd1, %rd197;
	atom.global.add.u32 	%r1109, [%rd198], 1;
	min.s32 	%r1110, %r1109, %r157;
	mad.lo.s32 	%r1111, %r1108, %r215, %r1110;
	mul.wide.s32 	%rd199, %r1111, 8;
	add.s64 	%rd196, %rd6, %rd199;
	// inline asm
	st.global.cg.v2.u32 [%rd196], {%r1093, %r1092};
	// inline asm

BB3_230:
	add.s32 	%r1293, %r1293, 4;
	setp.lt.s32	%p161, %r1293, %r11;
	@%p161 bra 	BB3_214;

BB3_231:
	@%p62 bra 	BB3_268;

	add.s32 	%r186, %r215, -2;
	mov.u32 	%r1116, 1;
	max.s32 	%r187, %r13, %r1116;
	and.b32  	%r1115, %r187, 3;
	setp.eq.s32	%p163, %r1115, 0;
	@%p163 bra 	BB3_250;

	setp.eq.s32	%p164, %r1115, 1;
	@%p164 bra 	BB3_245;

	setp.eq.s32	%p165, %r1115, 2;
	@%p165 bra 	BB3_240;

	setp.ge.s32	%p166, %r1, %r5;
	@%p166 bra 	BB3_236;

	add.s32 	%r1121, %r1, %r14;
	mul.wide.s32 	%rd201, %r1121, 8;
	add.s64 	%rd200, %rd5, %rd201;
	// inline asm
	ld.global.cs.v2.u32 {%r1118,%r1119}, [%rd200];
	// inline asm
	or.b32  	%r1122, %r1118, %r1119;
	setp.eq.s32	%p167, %r1122, 0;
	mov.u32 	%r1297, %r1116;
	@%p167 bra 	BB3_240;

	and.b32  	%r1124, %r1118, 131040;
	and.b32  	%r1125, %r1118, 31;
	xor.b32  	%r1126, %r1125, 1;
	shr.u32 	%r1127, %r1124, 3;
	add.s32 	%r1129, %r218, %r1127;
	mov.u32 	%r1297, 1;
	shl.b32 	%r1130, %r1297, %r1126;
	ld.shared.u32 	%r1131, [%r1129];
	and.b32  	%r1132, %r1131, %r1130;
	setp.eq.s32	%p168, %r1132, 0;
	@%p168 bra 	BB3_240;

	bfe.u32 	%r1136, %r1119, 17, 12;
	mul.wide.u32 	%rd203, %r1136, 4;
	add.s64 	%rd204, %rd1, %rd203;
	atom.global.add.u32 	%r1137, [%rd204], 1;
	min.s32 	%r1138, %r1137, %r186;
	mad.lo.s32 	%r1139, %r1136, %r215, %r1138;
	mul.wide.s32 	%rd205, %r1139, 8;
	add.s64 	%rd202, %rd6, %rd205;
	// inline asm
	st.global.cg.v2.u32 [%rd202], {%r1119, %r1118};
	// inline asm
	bra.uni 	BB3_240;

BB3_236:
	mov.u32 	%r1297, %r1116;

BB3_240:
	shl.b32 	%r1140, %r1297, 10;
	add.s32 	%r191, %r1140, %r1;
	setp.ge.s32	%p169, %r191, %r5;
	@%p169 bra 	BB3_244;

	add.s32 	%r1143, %r191, %r14;
	mul.wide.s32 	%rd207, %r1143, 8;
	add.s64 	%rd206, %rd5, %rd207;
	// inline asm
	ld.global.cs.v2.u32 {%r1141,%r1142}, [%rd206];
	// inline asm
	or.b32  	%r1144, %r1141, %r1142;
	setp.eq.s32	%p170, %r1144, 0;
	@%p170 bra 	BB3_244;

	and.b32  	%r1145, %r1141, 131040;
	and.b32  	%r1146, %r1141, 31;
	xor.b32  	%r1147, %r1146, 1;
	shr.u32 	%r1148, %r1145, 3;
	add.s32 	%r1150, %r218, %r1148;
	mov.u32 	%r1151, 1;
	shl.b32 	%r1152, %r1151, %r1147;
	ld.shared.u32 	%r1153, [%r1150];
	and.b32  	%r1154, %r1153, %r1152;
	setp.eq.s32	%p171, %r1154, 0;
	@%p171 bra 	BB3_244;

	bfe.u32 	%r1157, %r1142, 17, 12;
	mul.wide.u32 	%rd209, %r1157, 4;
	add.s64 	%rd210, %rd1, %rd209;
	atom.global.add.u32 	%r1158, [%rd210], 1;
	min.s32 	%r1159, %r1158, %r186;
	mad.lo.s32 	%r1160, %r1157, %r215, %r1159;
	mul.wide.s32 	%rd211, %r1160, 8;
	add.s64 	%rd208, %rd6, %rd211;
	// inline asm
	st.global.cg.v2.u32 [%rd208], {%r1142, %r1141};
	// inline asm

BB3_244:
	add.s32 	%r1297, %r1297, 1;

BB3_245:
	shl.b32 	%r1161, %r1297, 10;
	add.s32 	%r196, %r1161, %r1;
	setp.ge.s32	%p172, %r196, %r5;
	@%p172 bra 	BB3_249;

	add.s32 	%r1164, %r196, %r14;
	mul.wide.s32 	%rd213, %r1164, 8;
	add.s64 	%rd212, %rd5, %rd213;
	// inline asm
	ld.global.cs.v2.u32 {%r1162,%r1163}, [%rd212];
	// inline asm
	or.b32  	%r1165, %r1162, %r1163;
	setp.eq.s32	%p173, %r1165, 0;
	@%p173 bra 	BB3_249;

	and.b32  	%r1166, %r1162, 131040;
	and.b32  	%r1167, %r1162, 31;
	xor.b32  	%r1168, %r1167, 1;
	shr.u32 	%r1169, %r1166, 3;
	add.s32 	%r1171, %r218, %r1169;
	mov.u32 	%r1172, 1;
	shl.b32 	%r1173, %r1172, %r1168;
	ld.shared.u32 	%r1174, [%r1171];
	and.b32  	%r1175, %r1174, %r1173;
	setp.eq.s32	%p174, %r1175, 0;
	@%p174 bra 	BB3_249;

	bfe.u32 	%r1178, %r1163, 17, 12;
	mul.wide.u32 	%rd215, %r1178, 4;
	add.s64 	%rd216, %rd1, %rd215;
	atom.global.add.u32 	%r1179, [%rd216], 1;
	min.s32 	%r1180, %r1179, %r186;
	mad.lo.s32 	%r1181, %r1178, %r215, %r1180;
	mul.wide.s32 	%rd217, %r1181, 8;
	add.s64 	%rd214, %rd6, %rd217;
	// inline asm
	st.global.cg.v2.u32 [%rd214], {%r1163, %r1162};
	// inline asm

BB3_249:
	add.s32 	%r1297, %r1297, 1;

BB3_250:
	setp.lt.u32	%p175, %r187, 4;
	@%p175 bra 	BB3_268;

BB3_251:
	shl.b32 	%r1182, %r1297, 10;
	add.s32 	%r202, %r1182, %r1;
	setp.ge.s32	%p176, %r202, %r5;
	@%p176 bra 	BB3_255;

	add.s32 	%r1185, %r202, %r14;
	mul.wide.s32 	%rd219, %r1185, 8;
	add.s64 	%rd218, %rd5, %rd219;
	// inline asm
	ld.global.cs.v2.u32 {%r1183,%r1184}, [%rd218];
	// inline asm
	or.b32  	%r1186, %r1183, %r1184;
	setp.eq.s32	%p177, %r1186, 0;
	@%p177 bra 	BB3_255;

	and.b32  	%r1187, %r1183, 131040;
	and.b32  	%r1188, %r1183, 31;
	xor.b32  	%r1189, %r1188, 1;
	shr.u32 	%r1190, %r1187, 3;
	add.s32 	%r1192, %r218, %r1190;
	mov.u32 	%r1193, 1;
	shl.b32 	%r1194, %r1193, %r1189;
	ld.shared.u32 	%r1195, [%r1192];
	and.b32  	%r1196, %r1195, %r1194;
	setp.eq.s32	%p178, %r1196, 0;
	@%p178 bra 	BB3_255;

	bfe.u32 	%r1199, %r1184, 17, 12;
	mul.wide.u32 	%rd221, %r1199, 4;
	add.s64 	%rd222, %rd1, %rd221;
	atom.global.add.u32 	%r1200, [%rd222], 1;
	min.s32 	%r1201, %r1200, %r186;
	mad.lo.s32 	%r1202, %r1199, %r215, %r1201;
	mul.wide.s32 	%rd223, %r1202, 8;
	add.s64 	%rd220, %rd6, %rd223;
	// inline asm
	st.global.cg.v2.u32 [%rd220], {%r1184, %r1183};
	// inline asm

BB3_255:
	add.s32 	%r205, %r202, 1024;
	setp.ge.s32	%p179, %r205, %r5;
	@%p179 bra 	BB3_259;

	add.s32 	%r1207, %r205, %r14;
	mul.wide.s32 	%rd225, %r1207, 8;
	add.s64 	%rd224, %rd5, %rd225;
	// inline asm
	ld.global.cs.v2.u32 {%r1205,%r1206}, [%rd224];
	// inline asm
	or.b32  	%r1208, %r1205, %r1206;
	setp.eq.s32	%p180, %r1208, 0;
	@%p180 bra 	BB3_259;

	and.b32  	%r1209, %r1205, 131040;
	and.b32  	%r1210, %r1205, 31;
	xor.b32  	%r1211, %r1210, 1;
	shr.u32 	%r1212, %r1209, 3;
	add.s32 	%r1214, %r218, %r1212;
	mov.u32 	%r1215, 1;
	shl.b32 	%r1216, %r1215, %r1211;
	ld.shared.u32 	%r1217, [%r1214];
	and.b32  	%r1218, %r1217, %r1216;
	setp.eq.s32	%p181, %r1218, 0;
	@%p181 bra 	BB3_259;

	bfe.u32 	%r1221, %r1206, 17, 12;
	mul.wide.u32 	%rd227, %r1221, 4;
	add.s64 	%rd228, %rd1, %rd227;
	atom.global.add.u32 	%r1222, [%rd228], 1;
	min.s32 	%r1223, %r1222, %r186;
	mad.lo.s32 	%r1224, %r1221, %r215, %r1223;
	mul.wide.s32 	%rd229, %r1224, 8;
	add.s64 	%rd226, %rd6, %rd229;
	// inline asm
	st.global.cg.v2.u32 [%rd226], {%r1206, %r1205};
	// inline asm

BB3_259:
	add.s32 	%r208, %r202, 2048;
	setp.ge.s32	%p182, %r208, %r5;
	@%p182 bra 	BB3_263;

	add.s32 	%r1229, %r208, %r14;
	mul.wide.s32 	%rd231, %r1229, 8;
	add.s64 	%rd230, %rd5, %rd231;
	// inline asm
	ld.global.cs.v2.u32 {%r1227,%r1228}, [%rd230];
	// inline asm
	or.b32  	%r1230, %r1227, %r1228;
	setp.eq.s32	%p183, %r1230, 0;
	@%p183 bra 	BB3_263;

	and.b32  	%r1231, %r1227, 131040;
	and.b32  	%r1232, %r1227, 31;
	xor.b32  	%r1233, %r1232, 1;
	shr.u32 	%r1234, %r1231, 3;
	add.s32 	%r1236, %r218, %r1234;
	mov.u32 	%r1237, 1;
	shl.b32 	%r1238, %r1237, %r1233;
	ld.shared.u32 	%r1239, [%r1236];
	and.b32  	%r1240, %r1239, %r1238;
	setp.eq.s32	%p184, %r1240, 0;
	@%p184 bra 	BB3_263;

	bfe.u32 	%r1243, %r1228, 17, 12;
	mul.wide.u32 	%rd233, %r1243, 4;
	add.s64 	%rd234, %rd1, %rd233;
	atom.global.add.u32 	%r1244, [%rd234], 1;
	min.s32 	%r1245, %r1244, %r186;
	mad.lo.s32 	%r1246, %r1243, %r215, %r1245;
	mul.wide.s32 	%rd235, %r1246, 8;
	add.s64 	%rd232, %rd6, %rd235;
	// inline asm
	st.global.cg.v2.u32 [%rd232], {%r1228, %r1227};
	// inline asm

BB3_263:
	add.s32 	%r211, %r202, 3072;
	setp.ge.s32	%p185, %r211, %r5;
	@%p185 bra 	BB3_267;

	add.s32 	%r1251, %r211, %r14;
	mul.wide.s32 	%rd237, %r1251, 8;
	add.s64 	%rd236, %rd5, %rd237;
	// inline asm
	ld.global.cs.v2.u32 {%r1249,%r1250}, [%rd236];
	// inline asm
	or.b32  	%r1252, %r1249, %r1250;
	setp.eq.s32	%p186, %r1252, 0;
	@%p186 bra 	BB3_267;

	and.b32  	%r1253, %r1249, 131040;
	and.b32  	%r1254, %r1249, 31;
	xor.b32  	%r1255, %r1254, 1;
	shr.u32 	%r1256, %r1253, 3;
	add.s32 	%r1258, %r218, %r1256;
	mov.u32 	%r1259, 1;
	shl.b32 	%r1260, %r1259, %r1255;
	ld.shared.u32 	%r1261, [%r1258];
	and.b32  	%r1262, %r1261, %r1260;
	setp.eq.s32	%p187, %r1262, 0;
	@%p187 bra 	BB3_267;

	bfe.u32 	%r1265, %r1250, 17, 12;
	mul.wide.u32 	%rd239, %r1265, 4;
	add.s64 	%rd240, %rd1, %rd239;
	atom.global.add.u32 	%r1266, [%rd240], 1;
	min.s32 	%r1267, %r1266, %r186;
	mad.lo.s32 	%r1268, %r1265, %r215, %r1267;
	mul.wide.s32 	%rd241, %r1268, 8;
	add.s64 	%rd238, %rd6, %rd241;
	// inline asm
	st.global.cg.v2.u32 [%rd238], {%r1250, %r1249};
	// inline asm

BB3_267:
	add.s32 	%r1297, %r1297, 4;
	setp.lt.s32	%p188, %r1297, %r13;
	@%p188 bra 	BB3_251;

BB3_268:
	ret;
}

	// .globl	FluffySeed4K_C0
.visible .entry FluffySeed4K_C0(
	.param .u64 FluffySeed4K_C0_param_0,
	.param .u64 FluffySeed4K_C0_param_1,
	.param .u64 FluffySeed4K_C0_param_2,
	.param .u64 FluffySeed4K_C0_param_3,
	.param .u64 FluffySeed4K_C0_param_4,
	.param .u64 FluffySeed4K_C0_param_5,
	.param .u32 FluffySeed4K_C0_param_6
)
{
	.local .align 16 .b8 	__local_depot4[480];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<16>;
	.reg .b32 	%r<1140>;
	.reg .b64 	%rd<969>;
	// demoted variable
	.shared .align 8 .b8 FluffySeed4K_C0$__cuda_local_var_207638_30_non_const_magazine[32768];

	mov.u64 	%rd968, __local_depot4;
	cvta.local.u64 	%SP, %rd968;
	ld.param.u64 	%rd69, [FluffySeed4K_C0_param_4];
	ld.param.u64 	%rd70, [FluffySeed4K_C0_param_5];
	ld.param.u32 	%r43, [FluffySeed4K_C0_param_6];
	cvta.to.global.u64 	%rd1, %rd69;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r44, %r1, 3;
	mov.u32 	%r45, FluffySeed4K_C0$__cuda_local_var_207638_30_non_const_magazine;
	add.s32 	%r2, %r45, %r44;
	mov.u64 	%rd71, 0;
	st.shared.u64 	[%r2], %rd71;
	st.shared.u64 	[%r2+8192], %rd71;
	st.shared.u64 	[%r2+16384], %rd71;
	st.shared.u64 	[%r2+24576], %rd71;
	mov.u32 	%r46, %ctaid.x;
	mov.u32 	%r47, %ntid.x;
	mad.lo.s32 	%r9, %r46, %r47, %r1;
	cvta.to.global.u64 	%rd2, %rd70;
	add.u64 	%rd72, %SP, 0;
	cvta.to.local.u64 	%rd3, %rd72;
	bar.sync 	0;
	shl.b32 	%r48, %r9, 8;
	add.s32 	%r10, %r48, %r43;
	add.s64 	%rd4, %rd3, 448;
	mov.u16 	%rs13, 0;

BB4_1:
	mov.u16 	%rs14, 0;
	ld.param.u64 	%rd966, [FluffySeed4K_C0_param_3];
	ld.param.u64 	%rd965, [FluffySeed4K_C0_param_2];
	ld.param.u64 	%rd964, [FluffySeed4K_C0_param_1];
	ld.param.u64 	%rd963, [FluffySeed4K_C0_param_0];
	cvt.s32.s16	%r49, %rs13;
	add.s32 	%r50, %r10, %r49;
	cvt.s64.s32	%rd5, %r50;

BB4_2:
	add.u64 	%rd962, %SP, 0;
	cvta.to.local.u64 	%rd961, %rd962;
	cvt.s64.s16	%rd73, %rs14;
	add.s64 	%rd74, %rd73, %rd5;
	xor.b64  	%rd75, %rd74, %rd966;
	add.s64 	%rd76, %rd965, %rd75;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r51}, %rd75;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r52,%dummy}, %rd75;
	}
	shf.l.wrap.b32 	%r53, %r52, %r51, 16;
	shf.l.wrap.b32 	%r54, %r51, %r52, 16;
	mov.b64 	%rd77, {%r54, %r53};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r55}, %rd964;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r56,%dummy}, %rd964;
	}
	shf.l.wrap.b32 	%r57, %r56, %r55, 13;
	shf.l.wrap.b32 	%r58, %r55, %r56, 13;
	mov.b64 	%rd78, {%r58, %r57};
	add.s64 	%rd79, %rd963, %rd964;
	xor.b64  	%rd80, %rd78, %rd79;
	xor.b64  	%rd81, %rd77, %rd76;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd79, 32;
	shr.b64 	%rhs, %rd79, 32;
	add.u64 	%rd82, %lhs, %rhs;
	}
	add.s64 	%rd83, %rd80, %rd76;
	add.s64 	%rd84, %rd81, %rd82;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r59}, %rd80;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r60,%dummy}, %rd80;
	}
	shf.l.wrap.b32 	%r61, %r60, %r59, 17;
	shf.l.wrap.b32 	%r62, %r59, %r60, 17;
	mov.b64 	%rd85, {%r62, %r61};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r63}, %rd81;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r64,%dummy}, %rd81;
	}
	shf.l.wrap.b32 	%r65, %r64, %r63, 25;
	shf.l.wrap.b32 	%r66, %r63, %r64, 25;
	mov.b64 	%rd86, {%r66, %r65};
	xor.b64  	%rd87, %rd85, %rd83;
	xor.b64  	%rd88, %rd86, %rd84;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd83, 32;
	shr.b64 	%rhs, %rd83, 32;
	add.u64 	%rd89, %lhs, %rhs;
	}
	add.s64 	%rd90, %rd84, %rd87;
	add.s64 	%rd91, %rd89, %rd88;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r67}, %rd87;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r68,%dummy}, %rd87;
	}
	shf.l.wrap.b32 	%r69, %r68, %r67, 13;
	shf.l.wrap.b32 	%r70, %r67, %r68, 13;
	mov.b64 	%rd92, {%r70, %r69};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r71}, %rd88;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r72,%dummy}, %rd88;
	}
	shf.l.wrap.b32 	%r73, %r72, %r71, 16;
	shf.l.wrap.b32 	%r74, %r71, %r72, 16;
	mov.b64 	%rd93, {%r74, %r73};
	xor.b64  	%rd94, %rd92, %rd90;
	xor.b64  	%rd95, %rd93, %rd91;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd90, 32;
	shr.b64 	%rhs, %rd90, 32;
	add.u64 	%rd96, %lhs, %rhs;
	}
	add.s64 	%rd97, %rd94, %rd91;
	add.s64 	%rd98, %rd95, %rd96;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r75}, %rd94;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r76,%dummy}, %rd94;
	}
	shf.l.wrap.b32 	%r77, %r76, %r75, 17;
	shf.l.wrap.b32 	%r78, %r75, %r76, 17;
	mov.b64 	%rd99, {%r78, %r77};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r79}, %rd95;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r80,%dummy}, %rd95;
	}
	shf.l.wrap.b32 	%r81, %r80, %r79, 25;
	shf.l.wrap.b32 	%r82, %r79, %r80, 25;
	mov.b64 	%rd100, {%r82, %r81};
	xor.b64  	%rd101, %rd99, %rd97;
	xor.b64  	%rd102, %rd100, %rd98;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd97, 32;
	shr.b64 	%rhs, %rd97, 32;
	add.u64 	%rd103, %lhs, %rhs;
	}
	xor.b64  	%rd104, %rd98, %rd74;
	xor.b64  	%rd105, %rd103, 255;
	add.s64 	%rd106, %rd104, %rd101;
	add.s64 	%rd107, %rd105, %rd102;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r83}, %rd101;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r84,%dummy}, %rd101;
	}
	shf.l.wrap.b32 	%r85, %r84, %r83, 13;
	shf.l.wrap.b32 	%r86, %r83, %r84, 13;
	mov.b64 	%rd108, {%r86, %r85};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r87}, %rd102;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r88,%dummy}, %rd102;
	}
	shf.l.wrap.b32 	%r89, %r88, %r87, 16;
	shf.l.wrap.b32 	%r90, %r87, %r88, 16;
	mov.b64 	%rd109, {%r90, %r89};
	xor.b64  	%rd110, %rd108, %rd106;
	xor.b64  	%rd111, %rd109, %rd107;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd106, 32;
	shr.b64 	%rhs, %rd106, 32;
	add.u64 	%rd112, %lhs, %rhs;
	}
	add.s64 	%rd113, %rd110, %rd107;
	add.s64 	%rd114, %rd111, %rd112;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r91}, %rd110;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r92,%dummy}, %rd110;
	}
	shf.l.wrap.b32 	%r93, %r92, %r91, 17;
	shf.l.wrap.b32 	%r94, %r91, %r92, 17;
	mov.b64 	%rd115, {%r94, %r93};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r95}, %rd111;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r96,%dummy}, %rd111;
	}
	shf.l.wrap.b32 	%r97, %r96, %r95, 25;
	shf.l.wrap.b32 	%r98, %r95, %r96, 25;
	mov.b64 	%rd116, {%r98, %r97};
	xor.b64  	%rd117, %rd115, %rd113;
	xor.b64  	%rd118, %rd116, %rd114;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd113, 32;
	shr.b64 	%rhs, %rd113, 32;
	add.u64 	%rd119, %lhs, %rhs;
	}
	add.s64 	%rd120, %rd114, %rd117;
	add.s64 	%rd121, %rd119, %rd118;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r99}, %rd117;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r100,%dummy}, %rd117;
	}
	shf.l.wrap.b32 	%r101, %r100, %r99, 13;
	shf.l.wrap.b32 	%r102, %r99, %r100, 13;
	mov.b64 	%rd122, {%r102, %r101};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r103}, %rd118;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r104,%dummy}, %rd118;
	}
	shf.l.wrap.b32 	%r105, %r104, %r103, 16;
	shf.l.wrap.b32 	%r106, %r103, %r104, 16;
	mov.b64 	%rd123, {%r106, %r105};
	xor.b64  	%rd124, %rd122, %rd120;
	xor.b64  	%rd125, %rd123, %rd121;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd120, 32;
	shr.b64 	%rhs, %rd120, 32;
	add.u64 	%rd126, %lhs, %rhs;
	}
	add.s64 	%rd127, %rd124, %rd121;
	add.s64 	%rd128, %rd125, %rd126;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r107}, %rd124;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r108,%dummy}, %rd124;
	}
	shf.l.wrap.b32 	%r109, %r108, %r107, 17;
	shf.l.wrap.b32 	%r110, %r107, %r108, 17;
	mov.b64 	%rd129, {%r110, %r109};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r111}, %rd125;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r112,%dummy}, %rd125;
	}
	shf.l.wrap.b32 	%r113, %r112, %r111, 25;
	shf.l.wrap.b32 	%r114, %r111, %r112, 25;
	mov.b64 	%rd130, {%r114, %r113};
	xor.b64  	%rd131, %rd129, %rd127;
	xor.b64  	%rd132, %rd130, %rd128;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd127, 32;
	shr.b64 	%rhs, %rd127, 32;
	add.u64 	%rd133, %lhs, %rhs;
	}
	add.s64 	%rd134, %rd128, %rd131;
	add.s64 	%rd135, %rd133, %rd132;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r115}, %rd131;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r116,%dummy}, %rd131;
	}
	shf.l.wrap.b32 	%r117, %r116, %r115, 13;
	shf.l.wrap.b32 	%r118, %r115, %r116, 13;
	mov.b64 	%rd136, {%r118, %r117};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r119}, %rd132;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r120,%dummy}, %rd132;
	}
	shf.l.wrap.b32 	%r121, %r120, %r119, 16;
	shf.l.wrap.b32 	%r122, %r119, %r120, 16;
	mov.b64 	%rd137, {%r122, %r121};
	xor.b64  	%rd138, %rd136, %rd134;
	xor.b64  	%rd139, %rd137, %rd135;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd134, 32;
	shr.b64 	%rhs, %rd134, 32;
	add.u64 	%rd140, %lhs, %rhs;
	}
	add.s64 	%rd141, %rd138, %rd135;
	add.s64 	%rd142, %rd139, %rd140;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r123}, %rd138;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r124,%dummy}, %rd138;
	}
	shf.l.wrap.b32 	%r125, %r124, %r123, 17;
	shf.l.wrap.b32 	%r126, %r123, %r124, 17;
	mov.b64 	%rd143, {%r126, %r125};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r127}, %rd139;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r128,%dummy}, %rd139;
	}
	shf.l.wrap.b32 	%r129, %r128, %r127, 25;
	shf.l.wrap.b32 	%r130, %r127, %r128, 25;
	mov.b64 	%rd144, {%r130, %r129};
	xor.b64  	%rd145, %rd143, %rd141;
	xor.b64  	%rd146, %rd144, %rd142;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd141, 32;
	shr.b64 	%rhs, %rd141, 32;
	add.u64 	%rd147, %lhs, %rhs;
	}
	add.s64 	%rd148, %rd142, %rd145;
	add.s64 	%rd149, %rd147, %rd146;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r131}, %rd145;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r132,%dummy}, %rd145;
	}
	shf.l.wrap.b32 	%r133, %r132, %r131, 13;
	shf.l.wrap.b32 	%r134, %r131, %r132, 13;
	mov.b64 	%rd150, {%r134, %r133};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r135}, %rd146;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r136,%dummy}, %rd146;
	}
	shf.l.wrap.b32 	%r137, %r136, %r135, 16;
	shf.l.wrap.b32 	%r138, %r135, %r136, 16;
	mov.b64 	%rd151, {%r138, %r137};
	xor.b64  	%rd152, %rd150, %rd148;
	xor.b64  	%rd153, %rd151, %rd149;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd148, 32;
	shr.b64 	%rhs, %rd148, 32;
	add.u64 	%rd154, %lhs, %rhs;
	}
	add.s64 	%rd155, %rd152, %rd149;
	add.s64 	%rd156, %rd153, %rd154;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r139}, %rd152;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r140,%dummy}, %rd152;
	}
	shf.l.wrap.b32 	%r141, %r140, %r139, 17;
	shf.l.wrap.b32 	%r142, %r139, %r140, 17;
	mov.b64 	%rd157, {%r142, %r141};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r143}, %rd153;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r144,%dummy}, %rd153;
	}
	shf.l.wrap.b32 	%r145, %r144, %r143, 25;
	shf.l.wrap.b32 	%r146, %r143, %r144, 25;
	mov.b64 	%rd158, {%r146, %r145};
	xor.b64  	%rd159, %rd157, %rd155;
	xor.b64  	%rd160, %rd158, %rd156;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd155, 32;
	shr.b64 	%rhs, %rd155, 32;
	add.u64 	%rd161, %lhs, %rhs;
	}
	xor.b64  	%rd162, %rd161, %rd160;
	xor.b64  	%rd163, %rd162, %rd159;
	add.s64 	%rd164, %rd74, 1;
	xor.b64  	%rd165, %rd160, %rd164;
	add.s64 	%rd166, %rd156, %rd159;
	add.s64 	%rd167, %rd161, %rd165;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r147}, %rd159;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r148,%dummy}, %rd159;
	}
	shf.l.wrap.b32 	%r149, %r148, %r147, 13;
	shf.l.wrap.b32 	%r150, %r147, %r148, 13;
	mov.b64 	%rd168, {%r150, %r149};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r151}, %rd165;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r152,%dummy}, %rd165;
	}
	shf.l.wrap.b32 	%r153, %r152, %r151, 16;
	shf.l.wrap.b32 	%r154, %r151, %r152, 16;
	mov.b64 	%rd169, {%r154, %r153};
	xor.b64  	%rd170, %rd168, %rd166;
	xor.b64  	%rd171, %rd169, %rd167;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd166, 32;
	shr.b64 	%rhs, %rd166, 32;
	add.u64 	%rd172, %lhs, %rhs;
	}
	add.s64 	%rd173, %rd170, %rd167;
	add.s64 	%rd174, %rd171, %rd172;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r155}, %rd170;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r156,%dummy}, %rd170;
	}
	shf.l.wrap.b32 	%r157, %r156, %r155, 17;
	shf.l.wrap.b32 	%r158, %r155, %r156, 17;
	mov.b64 	%rd175, {%r158, %r157};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r159}, %rd171;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r160,%dummy}, %rd171;
	}
	shf.l.wrap.b32 	%r161, %r160, %r159, 25;
	shf.l.wrap.b32 	%r162, %r159, %r160, 25;
	mov.b64 	%rd176, {%r162, %r161};
	xor.b64  	%rd177, %rd175, %rd173;
	xor.b64  	%rd178, %rd176, %rd174;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd173, 32;
	shr.b64 	%rhs, %rd173, 32;
	add.u64 	%rd179, %lhs, %rhs;
	}
	add.s64 	%rd180, %rd174, %rd177;
	add.s64 	%rd181, %rd179, %rd178;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r163}, %rd177;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r164,%dummy}, %rd177;
	}
	shf.l.wrap.b32 	%r165, %r164, %r163, 13;
	shf.l.wrap.b32 	%r166, %r163, %r164, 13;
	mov.b64 	%rd182, {%r166, %r165};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r167}, %rd178;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r168,%dummy}, %rd178;
	}
	shf.l.wrap.b32 	%r169, %r168, %r167, 16;
	shf.l.wrap.b32 	%r170, %r167, %r168, 16;
	mov.b64 	%rd183, {%r170, %r169};
	xor.b64  	%rd184, %rd182, %rd180;
	xor.b64  	%rd185, %rd183, %rd181;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd180, 32;
	shr.b64 	%rhs, %rd180, 32;
	add.u64 	%rd186, %lhs, %rhs;
	}
	add.s64 	%rd187, %rd184, %rd181;
	add.s64 	%rd188, %rd185, %rd186;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r171}, %rd184;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r172,%dummy}, %rd184;
	}
	shf.l.wrap.b32 	%r173, %r172, %r171, 17;
	shf.l.wrap.b32 	%r174, %r171, %r172, 17;
	mov.b64 	%rd189, {%r174, %r173};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r175}, %rd185;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r176,%dummy}, %rd185;
	}
	shf.l.wrap.b32 	%r177, %r176, %r175, 25;
	shf.l.wrap.b32 	%r178, %r175, %r176, 25;
	mov.b64 	%rd190, {%r178, %r177};
	xor.b64  	%rd191, %rd189, %rd187;
	xor.b64  	%rd192, %rd190, %rd188;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd187, 32;
	shr.b64 	%rhs, %rd187, 32;
	add.u64 	%rd193, %lhs, %rhs;
	}
	xor.b64  	%rd194, %rd188, %rd164;
	xor.b64  	%rd195, %rd193, 255;
	add.s64 	%rd196, %rd194, %rd191;
	add.s64 	%rd197, %rd195, %rd192;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r179}, %rd191;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r180,%dummy}, %rd191;
	}
	shf.l.wrap.b32 	%r181, %r180, %r179, 13;
	shf.l.wrap.b32 	%r182, %r179, %r180, 13;
	mov.b64 	%rd198, {%r182, %r181};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r183}, %rd192;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r184,%dummy}, %rd192;
	}
	shf.l.wrap.b32 	%r185, %r184, %r183, 16;
	shf.l.wrap.b32 	%r186, %r183, %r184, 16;
	mov.b64 	%rd199, {%r186, %r185};
	xor.b64  	%rd200, %rd198, %rd196;
	xor.b64  	%rd201, %rd199, %rd197;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd196, 32;
	shr.b64 	%rhs, %rd196, 32;
	add.u64 	%rd202, %lhs, %rhs;
	}
	add.s64 	%rd203, %rd200, %rd197;
	add.s64 	%rd204, %rd201, %rd202;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r187}, %rd200;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r188,%dummy}, %rd200;
	}
	shf.l.wrap.b32 	%r189, %r188, %r187, 17;
	shf.l.wrap.b32 	%r190, %r187, %r188, 17;
	mov.b64 	%rd205, {%r190, %r189};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r191}, %rd201;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r192,%dummy}, %rd201;
	}
	shf.l.wrap.b32 	%r193, %r192, %r191, 25;
	shf.l.wrap.b32 	%r194, %r191, %r192, 25;
	mov.b64 	%rd206, {%r194, %r193};
	xor.b64  	%rd207, %rd205, %rd203;
	xor.b64  	%rd208, %rd206, %rd204;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd203, 32;
	shr.b64 	%rhs, %rd203, 32;
	add.u64 	%rd209, %lhs, %rhs;
	}
	add.s64 	%rd210, %rd204, %rd207;
	add.s64 	%rd211, %rd209, %rd208;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r195}, %rd207;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r196,%dummy}, %rd207;
	}
	shf.l.wrap.b32 	%r197, %r196, %r195, 13;
	shf.l.wrap.b32 	%r198, %r195, %r196, 13;
	mov.b64 	%rd212, {%r198, %r197};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r199}, %rd208;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r200,%dummy}, %rd208;
	}
	shf.l.wrap.b32 	%r201, %r200, %r199, 16;
	shf.l.wrap.b32 	%r202, %r199, %r200, 16;
	mov.b64 	%rd213, {%r202, %r201};
	xor.b64  	%rd214, %rd212, %rd210;
	xor.b64  	%rd215, %rd213, %rd211;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd210, 32;
	shr.b64 	%rhs, %rd210, 32;
	add.u64 	%rd216, %lhs, %rhs;
	}
	add.s64 	%rd217, %rd214, %rd211;
	add.s64 	%rd218, %rd215, %rd216;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r203}, %rd214;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r204,%dummy}, %rd214;
	}
	shf.l.wrap.b32 	%r205, %r204, %r203, 17;
	shf.l.wrap.b32 	%r206, %r203, %r204, 17;
	mov.b64 	%rd219, {%r206, %r205};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r207}, %rd215;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r208,%dummy}, %rd215;
	}
	shf.l.wrap.b32 	%r209, %r208, %r207, 25;
	shf.l.wrap.b32 	%r210, %r207, %r208, 25;
	mov.b64 	%rd220, {%r210, %r209};
	xor.b64  	%rd221, %rd219, %rd217;
	xor.b64  	%rd222, %rd220, %rd218;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd217, 32;
	shr.b64 	%rhs, %rd217, 32;
	add.u64 	%rd223, %lhs, %rhs;
	}
	add.s64 	%rd224, %rd218, %rd221;
	add.s64 	%rd225, %rd223, %rd222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r211}, %rd221;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r212,%dummy}, %rd221;
	}
	shf.l.wrap.b32 	%r213, %r212, %r211, 13;
	shf.l.wrap.b32 	%r214, %r211, %r212, 13;
	mov.b64 	%rd226, {%r214, %r213};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r215}, %rd222;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r216,%dummy}, %rd222;
	}
	shf.l.wrap.b32 	%r217, %r216, %r215, 16;
	shf.l.wrap.b32 	%r218, %r215, %r216, 16;
	mov.b64 	%rd227, {%r218, %r217};
	xor.b64  	%rd228, %rd226, %rd224;
	xor.b64  	%rd229, %rd227, %rd225;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd224, 32;
	shr.b64 	%rhs, %rd224, 32;
	add.u64 	%rd230, %lhs, %rhs;
	}
	add.s64 	%rd231, %rd228, %rd225;
	add.s64 	%rd232, %rd229, %rd230;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r219}, %rd228;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r220,%dummy}, %rd228;
	}
	shf.l.wrap.b32 	%r221, %r220, %r219, 17;
	shf.l.wrap.b32 	%r222, %r219, %r220, 17;
	mov.b64 	%rd233, {%r222, %r221};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r223}, %rd229;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r224,%dummy}, %rd229;
	}
	shf.l.wrap.b32 	%r225, %r224, %r223, 25;
	shf.l.wrap.b32 	%r226, %r223, %r224, 25;
	mov.b64 	%rd234, {%r226, %r225};
	xor.b64  	%rd235, %rd233, %rd231;
	xor.b64  	%rd236, %rd234, %rd232;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd231, 32;
	shr.b64 	%rhs, %rd231, 32;
	add.u64 	%rd237, %lhs, %rhs;
	}
	add.s64 	%rd238, %rd232, %rd235;
	add.s64 	%rd239, %rd237, %rd236;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r227}, %rd235;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r228,%dummy}, %rd235;
	}
	shf.l.wrap.b32 	%r229, %r228, %r227, 13;
	shf.l.wrap.b32 	%r230, %r227, %r228, 13;
	mov.b64 	%rd240, {%r230, %r229};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r231}, %rd236;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r232,%dummy}, %rd236;
	}
	shf.l.wrap.b32 	%r233, %r232, %r231, 16;
	shf.l.wrap.b32 	%r234, %r231, %r232, 16;
	mov.b64 	%rd241, {%r234, %r233};
	xor.b64  	%rd242, %rd240, %rd238;
	xor.b64  	%rd243, %rd241, %rd239;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd238, 32;
	shr.b64 	%rhs, %rd238, 32;
	add.u64 	%rd244, %lhs, %rhs;
	}
	add.s64 	%rd245, %rd242, %rd239;
	add.s64 	%rd246, %rd243, %rd244;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r235}, %rd242;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r236,%dummy}, %rd242;
	}
	shf.l.wrap.b32 	%r237, %r236, %r235, 17;
	shf.l.wrap.b32 	%r238, %r235, %r236, 17;
	mov.b64 	%rd247, {%r238, %r237};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r239}, %rd243;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r240,%dummy}, %rd243;
	}
	shf.l.wrap.b32 	%r241, %r240, %r239, 25;
	shf.l.wrap.b32 	%r242, %r239, %r240, 25;
	mov.b64 	%rd248, {%r242, %r241};
	xor.b64  	%rd249, %rd247, %rd245;
	xor.b64  	%rd250, %rd248, %rd246;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd245, 32;
	shr.b64 	%rhs, %rd245, 32;
	add.u64 	%rd251, %lhs, %rhs;
	}
	xor.b64  	%rd252, %rd251, %rd250;
	xor.b64  	%rd253, %rd252, %rd249;
	add.s64 	%rd254, %rd74, 2;
	xor.b64  	%rd255, %rd250, %rd254;
	add.s64 	%rd256, %rd246, %rd249;
	add.s64 	%rd257, %rd251, %rd255;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r243}, %rd249;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r244,%dummy}, %rd249;
	}
	shf.l.wrap.b32 	%r245, %r244, %r243, 13;
	shf.l.wrap.b32 	%r246, %r243, %r244, 13;
	mov.b64 	%rd258, {%r246, %r245};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r247}, %rd255;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r248,%dummy}, %rd255;
	}
	shf.l.wrap.b32 	%r249, %r248, %r247, 16;
	shf.l.wrap.b32 	%r250, %r247, %r248, 16;
	mov.b64 	%rd259, {%r250, %r249};
	xor.b64  	%rd260, %rd258, %rd256;
	xor.b64  	%rd261, %rd259, %rd257;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd256, 32;
	shr.b64 	%rhs, %rd256, 32;
	add.u64 	%rd262, %lhs, %rhs;
	}
	add.s64 	%rd263, %rd260, %rd257;
	add.s64 	%rd264, %rd261, %rd262;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r251}, %rd260;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r252,%dummy}, %rd260;
	}
	shf.l.wrap.b32 	%r253, %r252, %r251, 17;
	shf.l.wrap.b32 	%r254, %r251, %r252, 17;
	mov.b64 	%rd265, {%r254, %r253};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r255}, %rd261;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r256,%dummy}, %rd261;
	}
	shf.l.wrap.b32 	%r257, %r256, %r255, 25;
	shf.l.wrap.b32 	%r258, %r255, %r256, 25;
	mov.b64 	%rd266, {%r258, %r257};
	xor.b64  	%rd267, %rd265, %rd263;
	xor.b64  	%rd268, %rd266, %rd264;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd263, 32;
	shr.b64 	%rhs, %rd263, 32;
	add.u64 	%rd269, %lhs, %rhs;
	}
	add.s64 	%rd270, %rd264, %rd267;
	add.s64 	%rd271, %rd269, %rd268;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r259}, %rd267;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r260,%dummy}, %rd267;
	}
	shf.l.wrap.b32 	%r261, %r260, %r259, 13;
	shf.l.wrap.b32 	%r262, %r259, %r260, 13;
	mov.b64 	%rd272, {%r262, %r261};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r263}, %rd268;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r264,%dummy}, %rd268;
	}
	shf.l.wrap.b32 	%r265, %r264, %r263, 16;
	shf.l.wrap.b32 	%r266, %r263, %r264, 16;
	mov.b64 	%rd273, {%r266, %r265};
	xor.b64  	%rd274, %rd272, %rd270;
	xor.b64  	%rd275, %rd273, %rd271;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd270, 32;
	shr.b64 	%rhs, %rd270, 32;
	add.u64 	%rd276, %lhs, %rhs;
	}
	add.s64 	%rd277, %rd274, %rd271;
	add.s64 	%rd278, %rd275, %rd276;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r267}, %rd274;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r268,%dummy}, %rd274;
	}
	shf.l.wrap.b32 	%r269, %r268, %r267, 17;
	shf.l.wrap.b32 	%r270, %r267, %r268, 17;
	mov.b64 	%rd279, {%r270, %r269};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r271}, %rd275;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r272,%dummy}, %rd275;
	}
	shf.l.wrap.b32 	%r273, %r272, %r271, 25;
	shf.l.wrap.b32 	%r274, %r271, %r272, 25;
	mov.b64 	%rd280, {%r274, %r273};
	xor.b64  	%rd281, %rd279, %rd277;
	xor.b64  	%rd282, %rd280, %rd278;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd277, 32;
	shr.b64 	%rhs, %rd277, 32;
	add.u64 	%rd283, %lhs, %rhs;
	}
	xor.b64  	%rd284, %rd278, %rd254;
	xor.b64  	%rd285, %rd283, 255;
	add.s64 	%rd286, %rd284, %rd281;
	add.s64 	%rd287, %rd285, %rd282;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r275}, %rd281;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r276,%dummy}, %rd281;
	}
	shf.l.wrap.b32 	%r277, %r276, %r275, 13;
	shf.l.wrap.b32 	%r278, %r275, %r276, 13;
	mov.b64 	%rd288, {%r278, %r277};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r279}, %rd282;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r280,%dummy}, %rd282;
	}
	shf.l.wrap.b32 	%r281, %r280, %r279, 16;
	shf.l.wrap.b32 	%r282, %r279, %r280, 16;
	mov.b64 	%rd289, {%r282, %r281};
	xor.b64  	%rd290, %rd288, %rd286;
	xor.b64  	%rd291, %rd289, %rd287;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd286, 32;
	shr.b64 	%rhs, %rd286, 32;
	add.u64 	%rd292, %lhs, %rhs;
	}
	add.s64 	%rd293, %rd290, %rd287;
	add.s64 	%rd294, %rd291, %rd292;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r283}, %rd290;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r284,%dummy}, %rd290;
	}
	shf.l.wrap.b32 	%r285, %r284, %r283, 17;
	shf.l.wrap.b32 	%r286, %r283, %r284, 17;
	mov.b64 	%rd295, {%r286, %r285};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r287}, %rd291;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r288,%dummy}, %rd291;
	}
	shf.l.wrap.b32 	%r289, %r288, %r287, 25;
	shf.l.wrap.b32 	%r290, %r287, %r288, 25;
	mov.b64 	%rd296, {%r290, %r289};
	xor.b64  	%rd297, %rd295, %rd293;
	xor.b64  	%rd298, %rd296, %rd294;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd293, 32;
	shr.b64 	%rhs, %rd293, 32;
	add.u64 	%rd299, %lhs, %rhs;
	}
	add.s64 	%rd300, %rd294, %rd297;
	add.s64 	%rd301, %rd299, %rd298;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r291}, %rd297;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r292,%dummy}, %rd297;
	}
	shf.l.wrap.b32 	%r293, %r292, %r291, 13;
	shf.l.wrap.b32 	%r294, %r291, %r292, 13;
	mov.b64 	%rd302, {%r294, %r293};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r295}, %rd298;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r296,%dummy}, %rd298;
	}
	shf.l.wrap.b32 	%r297, %r296, %r295, 16;
	shf.l.wrap.b32 	%r298, %r295, %r296, 16;
	mov.b64 	%rd303, {%r298, %r297};
	xor.b64  	%rd304, %rd302, %rd300;
	xor.b64  	%rd305, %rd303, %rd301;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd300, 32;
	shr.b64 	%rhs, %rd300, 32;
	add.u64 	%rd306, %lhs, %rhs;
	}
	add.s64 	%rd307, %rd304, %rd301;
	add.s64 	%rd308, %rd305, %rd306;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r299}, %rd304;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r300,%dummy}, %rd304;
	}
	shf.l.wrap.b32 	%r301, %r300, %r299, 17;
	shf.l.wrap.b32 	%r302, %r299, %r300, 17;
	mov.b64 	%rd309, {%r302, %r301};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r303}, %rd305;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r304,%dummy}, %rd305;
	}
	shf.l.wrap.b32 	%r305, %r304, %r303, 25;
	shf.l.wrap.b32 	%r306, %r303, %r304, 25;
	mov.b64 	%rd310, {%r306, %r305};
	xor.b64  	%rd311, %rd309, %rd307;
	xor.b64  	%rd312, %rd310, %rd308;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd307, 32;
	shr.b64 	%rhs, %rd307, 32;
	add.u64 	%rd313, %lhs, %rhs;
	}
	add.s64 	%rd314, %rd308, %rd311;
	add.s64 	%rd315, %rd313, %rd312;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r307}, %rd311;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r308,%dummy}, %rd311;
	}
	shf.l.wrap.b32 	%r309, %r308, %r307, 13;
	shf.l.wrap.b32 	%r310, %r307, %r308, 13;
	mov.b64 	%rd316, {%r310, %r309};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r311}, %rd312;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r312,%dummy}, %rd312;
	}
	shf.l.wrap.b32 	%r313, %r312, %r311, 16;
	shf.l.wrap.b32 	%r314, %r311, %r312, 16;
	mov.b64 	%rd317, {%r314, %r313};
	xor.b64  	%rd318, %rd316, %rd314;
	xor.b64  	%rd319, %rd317, %rd315;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd314, 32;
	shr.b64 	%rhs, %rd314, 32;
	add.u64 	%rd320, %lhs, %rhs;
	}
	add.s64 	%rd321, %rd318, %rd315;
	add.s64 	%rd322, %rd319, %rd320;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r315}, %rd318;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r316,%dummy}, %rd318;
	}
	shf.l.wrap.b32 	%r317, %r316, %r315, 17;
	shf.l.wrap.b32 	%r318, %r315, %r316, 17;
	mov.b64 	%rd323, {%r318, %r317};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r319}, %rd319;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r320,%dummy}, %rd319;
	}
	shf.l.wrap.b32 	%r321, %r320, %r319, 25;
	shf.l.wrap.b32 	%r322, %r319, %r320, 25;
	mov.b64 	%rd324, {%r322, %r321};
	xor.b64  	%rd325, %rd323, %rd321;
	xor.b64  	%rd326, %rd324, %rd322;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd321, 32;
	shr.b64 	%rhs, %rd321, 32;
	add.u64 	%rd327, %lhs, %rhs;
	}
	add.s64 	%rd328, %rd322, %rd325;
	add.s64 	%rd329, %rd327, %rd326;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r323}, %rd325;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r324,%dummy}, %rd325;
	}
	shf.l.wrap.b32 	%r325, %r324, %r323, 13;
	shf.l.wrap.b32 	%r326, %r323, %r324, 13;
	mov.b64 	%rd330, {%r326, %r325};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r327}, %rd326;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r328,%dummy}, %rd326;
	}
	shf.l.wrap.b32 	%r329, %r328, %r327, 16;
	shf.l.wrap.b32 	%r330, %r327, %r328, 16;
	mov.b64 	%rd331, {%r330, %r329};
	xor.b64  	%rd332, %rd330, %rd328;
	xor.b64  	%rd333, %rd331, %rd329;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd328, 32;
	shr.b64 	%rhs, %rd328, 32;
	add.u64 	%rd334, %lhs, %rhs;
	}
	add.s64 	%rd335, %rd332, %rd329;
	add.s64 	%rd336, %rd333, %rd334;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r331}, %rd332;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r332,%dummy}, %rd332;
	}
	shf.l.wrap.b32 	%r333, %r332, %r331, 17;
	shf.l.wrap.b32 	%r334, %r331, %r332, 17;
	mov.b64 	%rd337, {%r334, %r333};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r335}, %rd333;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r336,%dummy}, %rd333;
	}
	shf.l.wrap.b32 	%r337, %r336, %r335, 25;
	shf.l.wrap.b32 	%r338, %r335, %r336, 25;
	mov.b64 	%rd338, {%r338, %r337};
	xor.b64  	%rd339, %rd337, %rd335;
	xor.b64  	%rd340, %rd338, %rd336;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd335, 32;
	shr.b64 	%rhs, %rd335, 32;
	add.u64 	%rd341, %lhs, %rhs;
	}
	xor.b64  	%rd342, %rd341, %rd340;
	xor.b64  	%rd343, %rd342, %rd339;
	add.s64 	%rd344, %rd74, 3;
	xor.b64  	%rd345, %rd340, %rd344;
	add.s64 	%rd346, %rd336, %rd339;
	add.s64 	%rd347, %rd341, %rd345;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r339}, %rd339;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r340,%dummy}, %rd339;
	}
	shf.l.wrap.b32 	%r341, %r340, %r339, 13;
	shf.l.wrap.b32 	%r342, %r339, %r340, 13;
	mov.b64 	%rd348, {%r342, %r341};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r343}, %rd345;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r344,%dummy}, %rd345;
	}
	shf.l.wrap.b32 	%r345, %r344, %r343, 16;
	shf.l.wrap.b32 	%r346, %r343, %r344, 16;
	mov.b64 	%rd349, {%r346, %r345};
	xor.b64  	%rd350, %rd348, %rd346;
	xor.b64  	%rd351, %rd349, %rd347;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd346, 32;
	shr.b64 	%rhs, %rd346, 32;
	add.u64 	%rd352, %lhs, %rhs;
	}
	add.s64 	%rd353, %rd350, %rd347;
	add.s64 	%rd354, %rd351, %rd352;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r347}, %rd350;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r348,%dummy}, %rd350;
	}
	shf.l.wrap.b32 	%r349, %r348, %r347, 17;
	shf.l.wrap.b32 	%r350, %r347, %r348, 17;
	mov.b64 	%rd355, {%r350, %r349};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r351}, %rd351;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r352,%dummy}, %rd351;
	}
	shf.l.wrap.b32 	%r353, %r352, %r351, 25;
	shf.l.wrap.b32 	%r354, %r351, %r352, 25;
	mov.b64 	%rd356, {%r354, %r353};
	xor.b64  	%rd357, %rd355, %rd353;
	xor.b64  	%rd358, %rd356, %rd354;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd353, 32;
	shr.b64 	%rhs, %rd353, 32;
	add.u64 	%rd359, %lhs, %rhs;
	}
	add.s64 	%rd360, %rd354, %rd357;
	add.s64 	%rd361, %rd359, %rd358;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r355}, %rd357;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r356,%dummy}, %rd357;
	}
	shf.l.wrap.b32 	%r357, %r356, %r355, 13;
	shf.l.wrap.b32 	%r358, %r355, %r356, 13;
	mov.b64 	%rd362, {%r358, %r357};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r359}, %rd358;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r360,%dummy}, %rd358;
	}
	shf.l.wrap.b32 	%r361, %r360, %r359, 16;
	shf.l.wrap.b32 	%r362, %r359, %r360, 16;
	mov.b64 	%rd363, {%r362, %r361};
	xor.b64  	%rd364, %rd362, %rd360;
	xor.b64  	%rd365, %rd363, %rd361;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd360, 32;
	shr.b64 	%rhs, %rd360, 32;
	add.u64 	%rd366, %lhs, %rhs;
	}
	add.s64 	%rd367, %rd364, %rd361;
	add.s64 	%rd368, %rd365, %rd366;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r363}, %rd364;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r364,%dummy}, %rd364;
	}
	shf.l.wrap.b32 	%r365, %r364, %r363, 17;
	shf.l.wrap.b32 	%r366, %r363, %r364, 17;
	mov.b64 	%rd369, {%r366, %r365};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r367}, %rd365;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r368,%dummy}, %rd365;
	}
	shf.l.wrap.b32 	%r369, %r368, %r367, 25;
	shf.l.wrap.b32 	%r370, %r367, %r368, 25;
	mov.b64 	%rd370, {%r370, %r369};
	xor.b64  	%rd371, %rd369, %rd367;
	xor.b64  	%rd372, %rd370, %rd368;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd367, 32;
	shr.b64 	%rhs, %rd367, 32;
	add.u64 	%rd373, %lhs, %rhs;
	}
	xor.b64  	%rd374, %rd368, %rd344;
	xor.b64  	%rd375, %rd373, 255;
	add.s64 	%rd376, %rd374, %rd371;
	add.s64 	%rd377, %rd375, %rd372;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r371}, %rd371;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r372,%dummy}, %rd371;
	}
	shf.l.wrap.b32 	%r373, %r372, %r371, 13;
	shf.l.wrap.b32 	%r374, %r371, %r372, 13;
	mov.b64 	%rd378, {%r374, %r373};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r375}, %rd372;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r376,%dummy}, %rd372;
	}
	shf.l.wrap.b32 	%r377, %r376, %r375, 16;
	shf.l.wrap.b32 	%r378, %r375, %r376, 16;
	mov.b64 	%rd379, {%r378, %r377};
	xor.b64  	%rd380, %rd378, %rd376;
	xor.b64  	%rd381, %rd379, %rd377;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd376, 32;
	shr.b64 	%rhs, %rd376, 32;
	add.u64 	%rd382, %lhs, %rhs;
	}
	add.s64 	%rd383, %rd380, %rd377;
	add.s64 	%rd384, %rd381, %rd382;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r379}, %rd380;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r380,%dummy}, %rd380;
	}
	shf.l.wrap.b32 	%r381, %r380, %r379, 17;
	shf.l.wrap.b32 	%r382, %r379, %r380, 17;
	mov.b64 	%rd385, {%r382, %r381};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r383}, %rd381;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r384,%dummy}, %rd381;
	}
	shf.l.wrap.b32 	%r385, %r384, %r383, 25;
	shf.l.wrap.b32 	%r386, %r383, %r384, 25;
	mov.b64 	%rd386, {%r386, %r385};
	xor.b64  	%rd387, %rd385, %rd383;
	xor.b64  	%rd388, %rd386, %rd384;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd383, 32;
	shr.b64 	%rhs, %rd383, 32;
	add.u64 	%rd389, %lhs, %rhs;
	}
	add.s64 	%rd390, %rd384, %rd387;
	add.s64 	%rd391, %rd389, %rd388;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r387}, %rd387;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r388,%dummy}, %rd387;
	}
	shf.l.wrap.b32 	%r389, %r388, %r387, 13;
	shf.l.wrap.b32 	%r390, %r387, %r388, 13;
	mov.b64 	%rd392, {%r390, %r389};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r391}, %rd388;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r392,%dummy}, %rd388;
	}
	shf.l.wrap.b32 	%r393, %r392, %r391, 16;
	shf.l.wrap.b32 	%r394, %r391, %r392, 16;
	mov.b64 	%rd393, {%r394, %r393};
	xor.b64  	%rd394, %rd392, %rd390;
	xor.b64  	%rd395, %rd393, %rd391;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd390, 32;
	shr.b64 	%rhs, %rd390, 32;
	add.u64 	%rd396, %lhs, %rhs;
	}
	add.s64 	%rd397, %rd394, %rd391;
	add.s64 	%rd398, %rd395, %rd396;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r395}, %rd394;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r396,%dummy}, %rd394;
	}
	shf.l.wrap.b32 	%r397, %r396, %r395, 17;
	shf.l.wrap.b32 	%r398, %r395, %r396, 17;
	mov.b64 	%rd399, {%r398, %r397};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r399}, %rd395;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r400,%dummy}, %rd395;
	}
	shf.l.wrap.b32 	%r401, %r400, %r399, 25;
	shf.l.wrap.b32 	%r402, %r399, %r400, 25;
	mov.b64 	%rd400, {%r402, %r401};
	xor.b64  	%rd401, %rd399, %rd397;
	xor.b64  	%rd402, %rd400, %rd398;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd397, 32;
	shr.b64 	%rhs, %rd397, 32;
	add.u64 	%rd403, %lhs, %rhs;
	}
	add.s64 	%rd404, %rd398, %rd401;
	add.s64 	%rd405, %rd403, %rd402;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r403}, %rd401;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r404,%dummy}, %rd401;
	}
	shf.l.wrap.b32 	%r405, %r404, %r403, 13;
	shf.l.wrap.b32 	%r406, %r403, %r404, 13;
	mov.b64 	%rd406, {%r406, %r405};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r407}, %rd402;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r408,%dummy}, %rd402;
	}
	shf.l.wrap.b32 	%r409, %r408, %r407, 16;
	shf.l.wrap.b32 	%r410, %r407, %r408, 16;
	mov.b64 	%rd407, {%r410, %r409};
	xor.b64  	%rd408, %rd406, %rd404;
	xor.b64  	%rd409, %rd407, %rd405;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd404, 32;
	shr.b64 	%rhs, %rd404, 32;
	add.u64 	%rd410, %lhs, %rhs;
	}
	add.s64 	%rd411, %rd408, %rd405;
	add.s64 	%rd412, %rd409, %rd410;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r411}, %rd408;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r412,%dummy}, %rd408;
	}
	shf.l.wrap.b32 	%r413, %r412, %r411, 17;
	shf.l.wrap.b32 	%r414, %r411, %r412, 17;
	mov.b64 	%rd413, {%r414, %r413};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r415}, %rd409;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r416,%dummy}, %rd409;
	}
	shf.l.wrap.b32 	%r417, %r416, %r415, 25;
	shf.l.wrap.b32 	%r418, %r415, %r416, 25;
	mov.b64 	%rd414, {%r418, %r417};
	xor.b64  	%rd415, %rd413, %rd411;
	xor.b64  	%rd416, %rd414, %rd412;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd411, 32;
	shr.b64 	%rhs, %rd411, 32;
	add.u64 	%rd417, %lhs, %rhs;
	}
	add.s64 	%rd418, %rd412, %rd415;
	add.s64 	%rd419, %rd417, %rd416;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r419}, %rd415;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r420,%dummy}, %rd415;
	}
	shf.l.wrap.b32 	%r421, %r420, %r419, 13;
	shf.l.wrap.b32 	%r422, %r419, %r420, 13;
	mov.b64 	%rd420, {%r422, %r421};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r423}, %rd416;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r424,%dummy}, %rd416;
	}
	shf.l.wrap.b32 	%r425, %r424, %r423, 16;
	shf.l.wrap.b32 	%r426, %r423, %r424, 16;
	mov.b64 	%rd421, {%r426, %r425};
	xor.b64  	%rd422, %rd420, %rd418;
	xor.b64  	%rd423, %rd421, %rd419;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd418, 32;
	shr.b64 	%rhs, %rd418, 32;
	add.u64 	%rd424, %lhs, %rhs;
	}
	add.s64 	%rd425, %rd422, %rd419;
	add.s64 	%rd963, %rd423, %rd424;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r427}, %rd422;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r428,%dummy}, %rd422;
	}
	shf.l.wrap.b32 	%r429, %r428, %r427, 17;
	shf.l.wrap.b32 	%r430, %r427, %r428, 17;
	mov.b64 	%rd426, {%r430, %r429};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r431}, %rd423;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r432,%dummy}, %rd423;
	}
	shf.l.wrap.b32 	%r433, %r432, %r431, 25;
	shf.l.wrap.b32 	%r434, %r431, %r432, 25;
	mov.b64 	%rd427, {%r434, %r433};
	xor.b64  	%rd964, %rd426, %rd425;
	xor.b64  	%rd966, %rd427, %rd963;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd425, 32;
	shr.b64 	%rhs, %rd425, 32;
	add.u64 	%rd965, %lhs, %rhs;
	}
	xor.b64  	%rd428, %rd965, %rd966;
	xor.b64  	%rd429, %rd428, %rd964;
	cvt.s32.s16	%r435, %rs14;
	shr.s32 	%r436, %r435, 31;
	shr.u32 	%r437, %r436, 30;
	add.s32 	%r438, %r435, %r437;
	shr.s32 	%r439, %r438, 2;
	mul.wide.s32 	%rd430, %r439, 32;
	add.s64 	%rd431, %rd961, %rd430;
	xor.b64  	%rd432, %rd253, %rd246;
	xor.b64  	%rd433, %rd163, %rd156;
	st.local.v2.u64 	[%rd431], {%rd433, %rd432};
	xor.b64  	%rd434, %rd429, %rd963;
	xor.b64  	%rd435, %rd343, %rd336;
	st.local.v2.u64 	[%rd431+16], {%rd435, %rd434};
	cvt.u32.u16	%r440, %rs14;
	add.s32 	%r441, %r440, 4;
	cvt.u16.u32	%rs14, %r441;
	setp.lt.s16	%p1, %rs14, 60;
	@%p1 bra 	BB4_2;

	add.s64 	%rd436, %rd5, 60;
	xor.b64  	%rd437, %rd966, %rd436;
	add.s64 	%rd438, %rd965, %rd437;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r442}, %rd437;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r443,%dummy}, %rd437;
	}
	shf.l.wrap.b32 	%r444, %r443, %r442, 16;
	shf.l.wrap.b32 	%r445, %r442, %r443, 16;
	mov.b64 	%rd439, {%r445, %r444};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r446}, %rd964;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r447,%dummy}, %rd964;
	}
	shf.l.wrap.b32 	%r448, %r447, %r446, 13;
	shf.l.wrap.b32 	%r449, %r446, %r447, 13;
	mov.b64 	%rd440, {%r449, %r448};
	add.s64 	%rd441, %rd963, %rd964;
	xor.b64  	%rd442, %rd440, %rd441;
	xor.b64  	%rd443, %rd439, %rd438;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd441, 32;
	shr.b64 	%rhs, %rd441, 32;
	add.u64 	%rd444, %lhs, %rhs;
	}
	add.s64 	%rd445, %rd442, %rd438;
	add.s64 	%rd446, %rd443, %rd444;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r450}, %rd442;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r451,%dummy}, %rd442;
	}
	shf.l.wrap.b32 	%r452, %r451, %r450, 17;
	shf.l.wrap.b32 	%r453, %r450, %r451, 17;
	mov.b64 	%rd447, {%r453, %r452};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r454}, %rd443;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r455,%dummy}, %rd443;
	}
	shf.l.wrap.b32 	%r456, %r455, %r454, 25;
	shf.l.wrap.b32 	%r457, %r454, %r455, 25;
	mov.b64 	%rd448, {%r457, %r456};
	xor.b64  	%rd449, %rd447, %rd445;
	xor.b64  	%rd450, %rd448, %rd446;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd445, 32;
	shr.b64 	%rhs, %rd445, 32;
	add.u64 	%rd451, %lhs, %rhs;
	}
	add.s64 	%rd452, %rd446, %rd449;
	add.s64 	%rd453, %rd451, %rd450;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r458}, %rd449;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r459,%dummy}, %rd449;
	}
	shf.l.wrap.b32 	%r460, %r459, %r458, 13;
	shf.l.wrap.b32 	%r461, %r458, %r459, 13;
	mov.b64 	%rd454, {%r461, %r460};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r462}, %rd450;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r463,%dummy}, %rd450;
	}
	shf.l.wrap.b32 	%r464, %r463, %r462, 16;
	shf.l.wrap.b32 	%r465, %r462, %r463, 16;
	mov.b64 	%rd455, {%r465, %r464};
	xor.b64  	%rd456, %rd454, %rd452;
	xor.b64  	%rd457, %rd455, %rd453;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd452, 32;
	shr.b64 	%rhs, %rd452, 32;
	add.u64 	%rd458, %lhs, %rhs;
	}
	add.s64 	%rd459, %rd456, %rd453;
	add.s64 	%rd460, %rd457, %rd458;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r466}, %rd456;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r467,%dummy}, %rd456;
	}
	shf.l.wrap.b32 	%r468, %r467, %r466, 17;
	shf.l.wrap.b32 	%r469, %r466, %r467, 17;
	mov.b64 	%rd461, {%r469, %r468};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r470}, %rd457;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r471,%dummy}, %rd457;
	}
	shf.l.wrap.b32 	%r472, %r471, %r470, 25;
	shf.l.wrap.b32 	%r473, %r470, %r471, 25;
	mov.b64 	%rd462, {%r473, %r472};
	xor.b64  	%rd463, %rd461, %rd459;
	xor.b64  	%rd464, %rd462, %rd460;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd459, 32;
	shr.b64 	%rhs, %rd459, 32;
	add.u64 	%rd465, %lhs, %rhs;
	}
	xor.b64  	%rd466, %rd460, %rd436;
	xor.b64  	%rd467, %rd465, 255;
	add.s64 	%rd468, %rd466, %rd463;
	add.s64 	%rd469, %rd467, %rd464;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r474}, %rd463;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r475,%dummy}, %rd463;
	}
	shf.l.wrap.b32 	%r476, %r475, %r474, 13;
	shf.l.wrap.b32 	%r477, %r474, %r475, 13;
	mov.b64 	%rd470, {%r477, %r476};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r478}, %rd464;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r479,%dummy}, %rd464;
	}
	shf.l.wrap.b32 	%r480, %r479, %r478, 16;
	shf.l.wrap.b32 	%r481, %r478, %r479, 16;
	mov.b64 	%rd471, {%r481, %r480};
	xor.b64  	%rd472, %rd470, %rd468;
	xor.b64  	%rd473, %rd471, %rd469;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd468, 32;
	shr.b64 	%rhs, %rd468, 32;
	add.u64 	%rd474, %lhs, %rhs;
	}
	add.s64 	%rd475, %rd472, %rd469;
	add.s64 	%rd476, %rd473, %rd474;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r482}, %rd472;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r483,%dummy}, %rd472;
	}
	shf.l.wrap.b32 	%r484, %r483, %r482, 17;
	shf.l.wrap.b32 	%r485, %r482, %r483, 17;
	mov.b64 	%rd477, {%r485, %r484};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r486}, %rd473;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r487,%dummy}, %rd473;
	}
	shf.l.wrap.b32 	%r488, %r487, %r486, 25;
	shf.l.wrap.b32 	%r489, %r486, %r487, 25;
	mov.b64 	%rd478, {%r489, %r488};
	xor.b64  	%rd479, %rd477, %rd475;
	xor.b64  	%rd480, %rd478, %rd476;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd475, 32;
	shr.b64 	%rhs, %rd475, 32;
	add.u64 	%rd481, %lhs, %rhs;
	}
	add.s64 	%rd482, %rd476, %rd479;
	add.s64 	%rd483, %rd481, %rd480;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r490}, %rd479;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r491,%dummy}, %rd479;
	}
	shf.l.wrap.b32 	%r492, %r491, %r490, 13;
	shf.l.wrap.b32 	%r493, %r490, %r491, 13;
	mov.b64 	%rd484, {%r493, %r492};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r494}, %rd480;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r495,%dummy}, %rd480;
	}
	shf.l.wrap.b32 	%r496, %r495, %r494, 16;
	shf.l.wrap.b32 	%r497, %r494, %r495, 16;
	mov.b64 	%rd485, {%r497, %r496};
	xor.b64  	%rd486, %rd484, %rd482;
	xor.b64  	%rd487, %rd485, %rd483;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd482, 32;
	shr.b64 	%rhs, %rd482, 32;
	add.u64 	%rd488, %lhs, %rhs;
	}
	add.s64 	%rd489, %rd486, %rd483;
	add.s64 	%rd490, %rd487, %rd488;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r498}, %rd486;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r499,%dummy}, %rd486;
	}
	shf.l.wrap.b32 	%r500, %r499, %r498, 17;
	shf.l.wrap.b32 	%r501, %r498, %r499, 17;
	mov.b64 	%rd491, {%r501, %r500};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r502}, %rd487;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r503,%dummy}, %rd487;
	}
	shf.l.wrap.b32 	%r504, %r503, %r502, 25;
	shf.l.wrap.b32 	%r505, %r502, %r503, 25;
	mov.b64 	%rd492, {%r505, %r504};
	xor.b64  	%rd493, %rd491, %rd489;
	xor.b64  	%rd494, %rd492, %rd490;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd489, 32;
	shr.b64 	%rhs, %rd489, 32;
	add.u64 	%rd495, %lhs, %rhs;
	}
	add.s64 	%rd496, %rd490, %rd493;
	add.s64 	%rd497, %rd495, %rd494;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r506}, %rd493;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r507,%dummy}, %rd493;
	}
	shf.l.wrap.b32 	%r508, %r507, %r506, 13;
	shf.l.wrap.b32 	%r509, %r506, %r507, 13;
	mov.b64 	%rd498, {%r509, %r508};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r510}, %rd494;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r511,%dummy}, %rd494;
	}
	shf.l.wrap.b32 	%r512, %r511, %r510, 16;
	shf.l.wrap.b32 	%r513, %r510, %r511, 16;
	mov.b64 	%rd499, {%r513, %r512};
	xor.b64  	%rd500, %rd498, %rd496;
	xor.b64  	%rd501, %rd499, %rd497;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd496, 32;
	shr.b64 	%rhs, %rd496, 32;
	add.u64 	%rd502, %lhs, %rhs;
	}
	add.s64 	%rd503, %rd500, %rd497;
	add.s64 	%rd504, %rd501, %rd502;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r514}, %rd500;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r515,%dummy}, %rd500;
	}
	shf.l.wrap.b32 	%r516, %r515, %r514, 17;
	shf.l.wrap.b32 	%r517, %r514, %r515, 17;
	mov.b64 	%rd505, {%r517, %r516};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r518}, %rd501;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r519,%dummy}, %rd501;
	}
	shf.l.wrap.b32 	%r520, %r519, %r518, 25;
	shf.l.wrap.b32 	%r521, %r518, %r519, 25;
	mov.b64 	%rd506, {%r521, %r520};
	xor.b64  	%rd507, %rd505, %rd503;
	xor.b64  	%rd508, %rd506, %rd504;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd503, 32;
	shr.b64 	%rhs, %rd503, 32;
	add.u64 	%rd509, %lhs, %rhs;
	}
	add.s64 	%rd510, %rd504, %rd507;
	add.s64 	%rd511, %rd509, %rd508;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r522}, %rd507;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r523,%dummy}, %rd507;
	}
	shf.l.wrap.b32 	%r524, %r523, %r522, 13;
	shf.l.wrap.b32 	%r525, %r522, %r523, 13;
	mov.b64 	%rd512, {%r525, %r524};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r526}, %rd508;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r527,%dummy}, %rd508;
	}
	shf.l.wrap.b32 	%r528, %r527, %r526, 16;
	shf.l.wrap.b32 	%r529, %r526, %r527, 16;
	mov.b64 	%rd513, {%r529, %r528};
	xor.b64  	%rd514, %rd512, %rd510;
	xor.b64  	%rd515, %rd513, %rd511;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd510, 32;
	shr.b64 	%rhs, %rd510, 32;
	add.u64 	%rd516, %lhs, %rhs;
	}
	add.s64 	%rd517, %rd514, %rd511;
	add.s64 	%rd14, %rd515, %rd516;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r530}, %rd514;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r531,%dummy}, %rd514;
	}
	shf.l.wrap.b32 	%r532, %r531, %r530, 17;
	shf.l.wrap.b32 	%r533, %r530, %r531, 17;
	mov.b64 	%rd518, {%r533, %r532};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r534}, %rd515;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r535,%dummy}, %rd515;
	}
	shf.l.wrap.b32 	%r536, %r535, %r534, 25;
	shf.l.wrap.b32 	%r537, %r534, %r535, 25;
	mov.b64 	%rd519, {%r537, %r536};
	xor.b64  	%rd15, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd14;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd517, 32;
	shr.b64 	%rhs, %rd517, 32;
	add.u64 	%rd521, %lhs, %rhs;
	}
	xor.b64  	%rd16, %rd521, %rd520;
	add.s64 	%rd522, %rd5, 61;
	xor.b64  	%rd523, %rd520, %rd522;
	add.s64 	%rd524, %rd14, %rd15;
	add.s64 	%rd525, %rd521, %rd523;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r538}, %rd15;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r539,%dummy}, %rd15;
	}
	shf.l.wrap.b32 	%r540, %r539, %r538, 13;
	shf.l.wrap.b32 	%r541, %r538, %r539, 13;
	mov.b64 	%rd526, {%r541, %r540};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r542}, %rd523;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r543,%dummy}, %rd523;
	}
	shf.l.wrap.b32 	%r544, %r543, %r542, 16;
	shf.l.wrap.b32 	%r545, %r542, %r543, 16;
	mov.b64 	%rd527, {%r545, %r544};
	xor.b64  	%rd528, %rd526, %rd524;
	xor.b64  	%rd529, %rd527, %rd525;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd524, 32;
	shr.b64 	%rhs, %rd524, 32;
	add.u64 	%rd530, %lhs, %rhs;
	}
	add.s64 	%rd531, %rd528, %rd525;
	add.s64 	%rd532, %rd529, %rd530;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r546}, %rd528;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r547,%dummy}, %rd528;
	}
	shf.l.wrap.b32 	%r548, %r547, %r546, 17;
	shf.l.wrap.b32 	%r549, %r546, %r547, 17;
	mov.b64 	%rd533, {%r549, %r548};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r550}, %rd529;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r551,%dummy}, %rd529;
	}
	shf.l.wrap.b32 	%r552, %r551, %r550, 25;
	shf.l.wrap.b32 	%r553, %r550, %r551, 25;
	mov.b64 	%rd534, {%r553, %r552};
	xor.b64  	%rd535, %rd533, %rd531;
	xor.b64  	%rd536, %rd534, %rd532;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd531, 32;
	shr.b64 	%rhs, %rd531, 32;
	add.u64 	%rd537, %lhs, %rhs;
	}
	add.s64 	%rd538, %rd532, %rd535;
	add.s64 	%rd539, %rd537, %rd536;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r554}, %rd535;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r555,%dummy}, %rd535;
	}
	shf.l.wrap.b32 	%r556, %r555, %r554, 13;
	shf.l.wrap.b32 	%r557, %r554, %r555, 13;
	mov.b64 	%rd540, {%r557, %r556};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r558}, %rd536;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r559,%dummy}, %rd536;
	}
	shf.l.wrap.b32 	%r560, %r559, %r558, 16;
	shf.l.wrap.b32 	%r561, %r558, %r559, 16;
	mov.b64 	%rd541, {%r561, %r560};
	xor.b64  	%rd542, %rd540, %rd538;
	xor.b64  	%rd543, %rd541, %rd539;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd538, 32;
	shr.b64 	%rhs, %rd538, 32;
	add.u64 	%rd544, %lhs, %rhs;
	}
	add.s64 	%rd545, %rd542, %rd539;
	add.s64 	%rd546, %rd543, %rd544;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r562}, %rd542;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r563,%dummy}, %rd542;
	}
	shf.l.wrap.b32 	%r564, %r563, %r562, 17;
	shf.l.wrap.b32 	%r565, %r562, %r563, 17;
	mov.b64 	%rd547, {%r565, %r564};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r566}, %rd543;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r567,%dummy}, %rd543;
	}
	shf.l.wrap.b32 	%r568, %r567, %r566, 25;
	shf.l.wrap.b32 	%r569, %r566, %r567, 25;
	mov.b64 	%rd548, {%r569, %r568};
	xor.b64  	%rd549, %rd547, %rd545;
	xor.b64  	%rd550, %rd548, %rd546;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd545, 32;
	shr.b64 	%rhs, %rd545, 32;
	add.u64 	%rd551, %lhs, %rhs;
	}
	xor.b64  	%rd552, %rd546, %rd522;
	xor.b64  	%rd553, %rd551, 255;
	add.s64 	%rd554, %rd552, %rd549;
	add.s64 	%rd555, %rd553, %rd550;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r570}, %rd549;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r571,%dummy}, %rd549;
	}
	shf.l.wrap.b32 	%r572, %r571, %r570, 13;
	shf.l.wrap.b32 	%r573, %r570, %r571, 13;
	mov.b64 	%rd556, {%r573, %r572};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r574}, %rd550;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r575,%dummy}, %rd550;
	}
	shf.l.wrap.b32 	%r576, %r575, %r574, 16;
	shf.l.wrap.b32 	%r577, %r574, %r575, 16;
	mov.b64 	%rd557, {%r577, %r576};
	xor.b64  	%rd558, %rd556, %rd554;
	xor.b64  	%rd559, %rd557, %rd555;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd554, 32;
	shr.b64 	%rhs, %rd554, 32;
	add.u64 	%rd560, %lhs, %rhs;
	}
	add.s64 	%rd561, %rd558, %rd555;
	add.s64 	%rd562, %rd559, %rd560;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r578}, %rd558;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r579,%dummy}, %rd558;
	}
	shf.l.wrap.b32 	%r580, %r579, %r578, 17;
	shf.l.wrap.b32 	%r581, %r578, %r579, 17;
	mov.b64 	%rd563, {%r581, %r580};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r582}, %rd559;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r583,%dummy}, %rd559;
	}
	shf.l.wrap.b32 	%r584, %r583, %r582, 25;
	shf.l.wrap.b32 	%r585, %r582, %r583, 25;
	mov.b64 	%rd564, {%r585, %r584};
	xor.b64  	%rd565, %rd563, %rd561;
	xor.b64  	%rd566, %rd564, %rd562;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd561, 32;
	shr.b64 	%rhs, %rd561, 32;
	add.u64 	%rd567, %lhs, %rhs;
	}
	add.s64 	%rd568, %rd562, %rd565;
	add.s64 	%rd569, %rd567, %rd566;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r586}, %rd565;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r587,%dummy}, %rd565;
	}
	shf.l.wrap.b32 	%r588, %r587, %r586, 13;
	shf.l.wrap.b32 	%r589, %r586, %r587, 13;
	mov.b64 	%rd570, {%r589, %r588};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r590}, %rd566;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r591,%dummy}, %rd566;
	}
	shf.l.wrap.b32 	%r592, %r591, %r590, 16;
	shf.l.wrap.b32 	%r593, %r590, %r591, 16;
	mov.b64 	%rd571, {%r593, %r592};
	xor.b64  	%rd572, %rd570, %rd568;
	xor.b64  	%rd573, %rd571, %rd569;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd568, 32;
	shr.b64 	%rhs, %rd568, 32;
	add.u64 	%rd574, %lhs, %rhs;
	}
	add.s64 	%rd575, %rd572, %rd569;
	add.s64 	%rd576, %rd573, %rd574;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r594}, %rd572;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r595,%dummy}, %rd572;
	}
	shf.l.wrap.b32 	%r596, %r595, %r594, 17;
	shf.l.wrap.b32 	%r597, %r594, %r595, 17;
	mov.b64 	%rd577, {%r597, %r596};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r598}, %rd573;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r599,%dummy}, %rd573;
	}
	shf.l.wrap.b32 	%r600, %r599, %r598, 25;
	shf.l.wrap.b32 	%r601, %r598, %r599, 25;
	mov.b64 	%rd578, {%r601, %r600};
	xor.b64  	%rd579, %rd577, %rd575;
	xor.b64  	%rd580, %rd578, %rd576;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd575, 32;
	shr.b64 	%rhs, %rd575, 32;
	add.u64 	%rd581, %lhs, %rhs;
	}
	add.s64 	%rd582, %rd576, %rd579;
	add.s64 	%rd583, %rd581, %rd580;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r602}, %rd579;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r603,%dummy}, %rd579;
	}
	shf.l.wrap.b32 	%r604, %r603, %r602, 13;
	shf.l.wrap.b32 	%r605, %r602, %r603, 13;
	mov.b64 	%rd584, {%r605, %r604};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r606}, %rd580;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r607,%dummy}, %rd580;
	}
	shf.l.wrap.b32 	%r608, %r607, %r606, 16;
	shf.l.wrap.b32 	%r609, %r606, %r607, 16;
	mov.b64 	%rd585, {%r609, %r608};
	xor.b64  	%rd586, %rd584, %rd582;
	xor.b64  	%rd587, %rd585, %rd583;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd582, 32;
	shr.b64 	%rhs, %rd582, 32;
	add.u64 	%rd588, %lhs, %rhs;
	}
	add.s64 	%rd589, %rd586, %rd583;
	add.s64 	%rd590, %rd587, %rd588;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r610}, %rd586;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r611,%dummy}, %rd586;
	}
	shf.l.wrap.b32 	%r612, %r611, %r610, 17;
	shf.l.wrap.b32 	%r613, %r610, %r611, 17;
	mov.b64 	%rd591, {%r613, %r612};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r614}, %rd587;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r615,%dummy}, %rd587;
	}
	shf.l.wrap.b32 	%r616, %r615, %r614, 25;
	shf.l.wrap.b32 	%r617, %r614, %r615, 25;
	mov.b64 	%rd592, {%r617, %r616};
	xor.b64  	%rd593, %rd591, %rd589;
	xor.b64  	%rd594, %rd592, %rd590;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd589, 32;
	shr.b64 	%rhs, %rd589, 32;
	add.u64 	%rd595, %lhs, %rhs;
	}
	add.s64 	%rd596, %rd590, %rd593;
	add.s64 	%rd597, %rd595, %rd594;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r618}, %rd593;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r619,%dummy}, %rd593;
	}
	shf.l.wrap.b32 	%r620, %r619, %r618, 13;
	shf.l.wrap.b32 	%r621, %r618, %r619, 13;
	mov.b64 	%rd598, {%r621, %r620};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r622}, %rd594;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r623,%dummy}, %rd594;
	}
	shf.l.wrap.b32 	%r624, %r623, %r622, 16;
	shf.l.wrap.b32 	%r625, %r622, %r623, 16;
	mov.b64 	%rd599, {%r625, %r624};
	xor.b64  	%rd600, %rd598, %rd596;
	xor.b64  	%rd601, %rd599, %rd597;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd596, 32;
	shr.b64 	%rhs, %rd596, 32;
	add.u64 	%rd602, %lhs, %rhs;
	}
	add.s64 	%rd603, %rd600, %rd597;
	add.s64 	%rd17, %rd601, %rd602;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r626}, %rd600;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r627,%dummy}, %rd600;
	}
	shf.l.wrap.b32 	%r628, %r627, %r626, 17;
	shf.l.wrap.b32 	%r629, %r626, %r627, 17;
	mov.b64 	%rd604, {%r629, %r628};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r630}, %rd601;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r631,%dummy}, %rd601;
	}
	shf.l.wrap.b32 	%r632, %r631, %r630, 25;
	shf.l.wrap.b32 	%r633, %r630, %r631, 25;
	mov.b64 	%rd605, {%r633, %r632};
	xor.b64  	%rd18, %rd604, %rd603;
	xor.b64  	%rd606, %rd605, %rd17;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd603, 32;
	shr.b64 	%rhs, %rd603, 32;
	add.u64 	%rd607, %lhs, %rhs;
	}
	xor.b64  	%rd19, %rd607, %rd606;
	add.s64 	%rd608, %rd5, 62;
	xor.b64  	%rd609, %rd606, %rd608;
	add.s64 	%rd610, %rd17, %rd18;
	add.s64 	%rd611, %rd607, %rd609;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r634}, %rd18;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r635,%dummy}, %rd18;
	}
	shf.l.wrap.b32 	%r636, %r635, %r634, 13;
	shf.l.wrap.b32 	%r637, %r634, %r635, 13;
	mov.b64 	%rd612, {%r637, %r636};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r638}, %rd609;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r639,%dummy}, %rd609;
	}
	shf.l.wrap.b32 	%r640, %r639, %r638, 16;
	shf.l.wrap.b32 	%r641, %r638, %r639, 16;
	mov.b64 	%rd613, {%r641, %r640};
	xor.b64  	%rd614, %rd612, %rd610;
	xor.b64  	%rd615, %rd613, %rd611;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd610, 32;
	shr.b64 	%rhs, %rd610, 32;
	add.u64 	%rd616, %lhs, %rhs;
	}
	add.s64 	%rd617, %rd614, %rd611;
	add.s64 	%rd618, %rd615, %rd616;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r642}, %rd614;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r643,%dummy}, %rd614;
	}
	shf.l.wrap.b32 	%r644, %r643, %r642, 17;
	shf.l.wrap.b32 	%r645, %r642, %r643, 17;
	mov.b64 	%rd619, {%r645, %r644};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r646}, %rd615;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r647,%dummy}, %rd615;
	}
	shf.l.wrap.b32 	%r648, %r647, %r646, 25;
	shf.l.wrap.b32 	%r649, %r646, %r647, 25;
	mov.b64 	%rd620, {%r649, %r648};
	xor.b64  	%rd621, %rd619, %rd617;
	xor.b64  	%rd622, %rd620, %rd618;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd617, 32;
	shr.b64 	%rhs, %rd617, 32;
	add.u64 	%rd623, %lhs, %rhs;
	}
	add.s64 	%rd624, %rd618, %rd621;
	add.s64 	%rd625, %rd623, %rd622;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r650}, %rd621;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r651,%dummy}, %rd621;
	}
	shf.l.wrap.b32 	%r652, %r651, %r650, 13;
	shf.l.wrap.b32 	%r653, %r650, %r651, 13;
	mov.b64 	%rd626, {%r653, %r652};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r654}, %rd622;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r655,%dummy}, %rd622;
	}
	shf.l.wrap.b32 	%r656, %r655, %r654, 16;
	shf.l.wrap.b32 	%r657, %r654, %r655, 16;
	mov.b64 	%rd627, {%r657, %r656};
	xor.b64  	%rd628, %rd626, %rd624;
	xor.b64  	%rd629, %rd627, %rd625;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd624, 32;
	shr.b64 	%rhs, %rd624, 32;
	add.u64 	%rd630, %lhs, %rhs;
	}
	add.s64 	%rd631, %rd628, %rd625;
	add.s64 	%rd632, %rd629, %rd630;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r658}, %rd628;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r659,%dummy}, %rd628;
	}
	shf.l.wrap.b32 	%r660, %r659, %r658, 17;
	shf.l.wrap.b32 	%r661, %r658, %r659, 17;
	mov.b64 	%rd633, {%r661, %r660};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r662}, %rd629;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r663,%dummy}, %rd629;
	}
	shf.l.wrap.b32 	%r664, %r663, %r662, 25;
	shf.l.wrap.b32 	%r665, %r662, %r663, 25;
	mov.b64 	%rd634, {%r665, %r664};
	xor.b64  	%rd635, %rd633, %rd631;
	xor.b64  	%rd636, %rd634, %rd632;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd631, 32;
	shr.b64 	%rhs, %rd631, 32;
	add.u64 	%rd637, %lhs, %rhs;
	}
	xor.b64  	%rd638, %rd632, %rd608;
	xor.b64  	%rd639, %rd637, 255;
	add.s64 	%rd640, %rd638, %rd635;
	add.s64 	%rd641, %rd639, %rd636;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r666}, %rd635;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r667,%dummy}, %rd635;
	}
	shf.l.wrap.b32 	%r668, %r667, %r666, 13;
	shf.l.wrap.b32 	%r669, %r666, %r667, 13;
	mov.b64 	%rd642, {%r669, %r668};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r670}, %rd636;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r671,%dummy}, %rd636;
	}
	shf.l.wrap.b32 	%r672, %r671, %r670, 16;
	shf.l.wrap.b32 	%r673, %r670, %r671, 16;
	mov.b64 	%rd643, {%r673, %r672};
	xor.b64  	%rd644, %rd642, %rd640;
	xor.b64  	%rd645, %rd643, %rd641;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd640, 32;
	shr.b64 	%rhs, %rd640, 32;
	add.u64 	%rd646, %lhs, %rhs;
	}
	add.s64 	%rd647, %rd644, %rd641;
	add.s64 	%rd648, %rd645, %rd646;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r674}, %rd644;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r675,%dummy}, %rd644;
	}
	shf.l.wrap.b32 	%r676, %r675, %r674, 17;
	shf.l.wrap.b32 	%r677, %r674, %r675, 17;
	mov.b64 	%rd649, {%r677, %r676};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r678}, %rd645;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r679,%dummy}, %rd645;
	}
	shf.l.wrap.b32 	%r680, %r679, %r678, 25;
	shf.l.wrap.b32 	%r681, %r678, %r679, 25;
	mov.b64 	%rd650, {%r681, %r680};
	xor.b64  	%rd651, %rd649, %rd647;
	xor.b64  	%rd652, %rd650, %rd648;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd647, 32;
	shr.b64 	%rhs, %rd647, 32;
	add.u64 	%rd653, %lhs, %rhs;
	}
	add.s64 	%rd654, %rd648, %rd651;
	add.s64 	%rd655, %rd653, %rd652;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r682}, %rd651;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r683,%dummy}, %rd651;
	}
	shf.l.wrap.b32 	%r684, %r683, %r682, 13;
	shf.l.wrap.b32 	%r685, %r682, %r683, 13;
	mov.b64 	%rd656, {%r685, %r684};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r686}, %rd652;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r687,%dummy}, %rd652;
	}
	shf.l.wrap.b32 	%r688, %r687, %r686, 16;
	shf.l.wrap.b32 	%r689, %r686, %r687, 16;
	mov.b64 	%rd657, {%r689, %r688};
	xor.b64  	%rd658, %rd656, %rd654;
	xor.b64  	%rd659, %rd657, %rd655;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd654, 32;
	shr.b64 	%rhs, %rd654, 32;
	add.u64 	%rd660, %lhs, %rhs;
	}
	add.s64 	%rd661, %rd658, %rd655;
	add.s64 	%rd662, %rd659, %rd660;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r690}, %rd658;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r691,%dummy}, %rd658;
	}
	shf.l.wrap.b32 	%r692, %r691, %r690, 17;
	shf.l.wrap.b32 	%r693, %r690, %r691, 17;
	mov.b64 	%rd663, {%r693, %r692};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r694}, %rd659;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r695,%dummy}, %rd659;
	}
	shf.l.wrap.b32 	%r696, %r695, %r694, 25;
	shf.l.wrap.b32 	%r697, %r694, %r695, 25;
	mov.b64 	%rd664, {%r697, %r696};
	xor.b64  	%rd665, %rd663, %rd661;
	xor.b64  	%rd666, %rd664, %rd662;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd661, 32;
	shr.b64 	%rhs, %rd661, 32;
	add.u64 	%rd667, %lhs, %rhs;
	}
	add.s64 	%rd668, %rd662, %rd665;
	add.s64 	%rd669, %rd667, %rd666;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r698}, %rd665;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r699,%dummy}, %rd665;
	}
	shf.l.wrap.b32 	%r700, %r699, %r698, 13;
	shf.l.wrap.b32 	%r701, %r698, %r699, 13;
	mov.b64 	%rd670, {%r701, %r700};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r702}, %rd666;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r703,%dummy}, %rd666;
	}
	shf.l.wrap.b32 	%r704, %r703, %r702, 16;
	shf.l.wrap.b32 	%r705, %r702, %r703, 16;
	mov.b64 	%rd671, {%r705, %r704};
	xor.b64  	%rd672, %rd670, %rd668;
	xor.b64  	%rd673, %rd671, %rd669;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd668, 32;
	shr.b64 	%rhs, %rd668, 32;
	add.u64 	%rd674, %lhs, %rhs;
	}
	add.s64 	%rd675, %rd672, %rd669;
	add.s64 	%rd676, %rd673, %rd674;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r706}, %rd672;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r707,%dummy}, %rd672;
	}
	shf.l.wrap.b32 	%r708, %r707, %r706, 17;
	shf.l.wrap.b32 	%r709, %r706, %r707, 17;
	mov.b64 	%rd677, {%r709, %r708};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r710}, %rd673;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r711,%dummy}, %rd673;
	}
	shf.l.wrap.b32 	%r712, %r711, %r710, 25;
	shf.l.wrap.b32 	%r713, %r710, %r711, 25;
	mov.b64 	%rd678, {%r713, %r712};
	xor.b64  	%rd679, %rd677, %rd675;
	xor.b64  	%rd680, %rd678, %rd676;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd675, 32;
	shr.b64 	%rhs, %rd675, 32;
	add.u64 	%rd681, %lhs, %rhs;
	}
	add.s64 	%rd682, %rd676, %rd679;
	add.s64 	%rd683, %rd681, %rd680;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r714}, %rd679;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r715,%dummy}, %rd679;
	}
	shf.l.wrap.b32 	%r716, %r715, %r714, 13;
	shf.l.wrap.b32 	%r717, %r714, %r715, 13;
	mov.b64 	%rd684, {%r717, %r716};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r718}, %rd680;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r719,%dummy}, %rd680;
	}
	shf.l.wrap.b32 	%r720, %r719, %r718, 16;
	shf.l.wrap.b32 	%r721, %r718, %r719, 16;
	mov.b64 	%rd685, {%r721, %r720};
	xor.b64  	%rd686, %rd684, %rd682;
	xor.b64  	%rd687, %rd685, %rd683;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd682, 32;
	shr.b64 	%rhs, %rd682, 32;
	add.u64 	%rd688, %lhs, %rhs;
	}
	add.s64 	%rd689, %rd686, %rd683;
	add.s64 	%rd20, %rd687, %rd688;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r722}, %rd686;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r723,%dummy}, %rd686;
	}
	shf.l.wrap.b32 	%r724, %r723, %r722, 17;
	shf.l.wrap.b32 	%r725, %r722, %r723, 17;
	mov.b64 	%rd690, {%r725, %r724};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r726}, %rd687;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r727,%dummy}, %rd687;
	}
	shf.l.wrap.b32 	%r728, %r727, %r726, 25;
	shf.l.wrap.b32 	%r729, %r726, %r727, 25;
	mov.b64 	%rd691, {%r729, %r728};
	xor.b64  	%rd21, %rd690, %rd689;
	xor.b64  	%rd692, %rd691, %rd20;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd689, 32;
	shr.b64 	%rhs, %rd689, 32;
	add.u64 	%rd693, %lhs, %rhs;
	}
	xor.b64  	%rd22, %rd693, %rd692;
	add.s64 	%rd694, %rd5, 63;
	xor.b64  	%rd695, %rd692, %rd694;
	add.s64 	%rd696, %rd20, %rd21;
	add.s64 	%rd697, %rd693, %rd695;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r730}, %rd21;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r731,%dummy}, %rd21;
	}
	shf.l.wrap.b32 	%r732, %r731, %r730, 13;
	shf.l.wrap.b32 	%r733, %r730, %r731, 13;
	mov.b64 	%rd698, {%r733, %r732};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r734}, %rd695;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r735,%dummy}, %rd695;
	}
	shf.l.wrap.b32 	%r736, %r735, %r734, 16;
	shf.l.wrap.b32 	%r737, %r734, %r735, 16;
	mov.b64 	%rd699, {%r737, %r736};
	xor.b64  	%rd700, %rd698, %rd696;
	xor.b64  	%rd701, %rd699, %rd697;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd696, 32;
	shr.b64 	%rhs, %rd696, 32;
	add.u64 	%rd702, %lhs, %rhs;
	}
	add.s64 	%rd703, %rd700, %rd697;
	add.s64 	%rd704, %rd701, %rd702;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r738}, %rd700;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r739,%dummy}, %rd700;
	}
	shf.l.wrap.b32 	%r740, %r739, %r738, 17;
	shf.l.wrap.b32 	%r741, %r738, %r739, 17;
	mov.b64 	%rd705, {%r741, %r740};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r742}, %rd701;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r743,%dummy}, %rd701;
	}
	shf.l.wrap.b32 	%r744, %r743, %r742, 25;
	shf.l.wrap.b32 	%r745, %r742, %r743, 25;
	mov.b64 	%rd706, {%r745, %r744};
	xor.b64  	%rd707, %rd705, %rd703;
	xor.b64  	%rd708, %rd706, %rd704;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd703, 32;
	shr.b64 	%rhs, %rd703, 32;
	add.u64 	%rd709, %lhs, %rhs;
	}
	add.s64 	%rd710, %rd704, %rd707;
	add.s64 	%rd711, %rd709, %rd708;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r746}, %rd707;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r747,%dummy}, %rd707;
	}
	shf.l.wrap.b32 	%r748, %r747, %r746, 13;
	shf.l.wrap.b32 	%r749, %r746, %r747, 13;
	mov.b64 	%rd712, {%r749, %r748};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r750}, %rd708;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r751,%dummy}, %rd708;
	}
	shf.l.wrap.b32 	%r752, %r751, %r750, 16;
	shf.l.wrap.b32 	%r753, %r750, %r751, 16;
	mov.b64 	%rd713, {%r753, %r752};
	xor.b64  	%rd714, %rd712, %rd710;
	xor.b64  	%rd715, %rd713, %rd711;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd710, 32;
	shr.b64 	%rhs, %rd710, 32;
	add.u64 	%rd716, %lhs, %rhs;
	}
	add.s64 	%rd717, %rd714, %rd711;
	add.s64 	%rd718, %rd715, %rd716;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r754}, %rd714;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r755,%dummy}, %rd714;
	}
	shf.l.wrap.b32 	%r756, %r755, %r754, 17;
	shf.l.wrap.b32 	%r757, %r754, %r755, 17;
	mov.b64 	%rd719, {%r757, %r756};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r758}, %rd715;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r759,%dummy}, %rd715;
	}
	shf.l.wrap.b32 	%r760, %r759, %r758, 25;
	shf.l.wrap.b32 	%r761, %r758, %r759, 25;
	mov.b64 	%rd720, {%r761, %r760};
	xor.b64  	%rd721, %rd719, %rd717;
	xor.b64  	%rd722, %rd720, %rd718;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd717, 32;
	shr.b64 	%rhs, %rd717, 32;
	add.u64 	%rd723, %lhs, %rhs;
	}
	xor.b64  	%rd724, %rd718, %rd694;
	xor.b64  	%rd725, %rd723, 255;
	add.s64 	%rd726, %rd724, %rd721;
	add.s64 	%rd727, %rd725, %rd722;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r762}, %rd721;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r763,%dummy}, %rd721;
	}
	shf.l.wrap.b32 	%r764, %r763, %r762, 13;
	shf.l.wrap.b32 	%r765, %r762, %r763, 13;
	mov.b64 	%rd728, {%r765, %r764};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r766}, %rd722;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r767,%dummy}, %rd722;
	}
	shf.l.wrap.b32 	%r768, %r767, %r766, 16;
	shf.l.wrap.b32 	%r769, %r766, %r767, 16;
	mov.b64 	%rd729, {%r769, %r768};
	xor.b64  	%rd730, %rd728, %rd726;
	xor.b64  	%rd731, %rd729, %rd727;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd726, 32;
	shr.b64 	%rhs, %rd726, 32;
	add.u64 	%rd732, %lhs, %rhs;
	}
	add.s64 	%rd733, %rd730, %rd727;
	add.s64 	%rd734, %rd731, %rd732;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r770}, %rd730;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r771,%dummy}, %rd730;
	}
	shf.l.wrap.b32 	%r772, %r771, %r770, 17;
	shf.l.wrap.b32 	%r773, %r770, %r771, 17;
	mov.b64 	%rd735, {%r773, %r772};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r774}, %rd731;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r775,%dummy}, %rd731;
	}
	shf.l.wrap.b32 	%r776, %r775, %r774, 25;
	shf.l.wrap.b32 	%r777, %r774, %r775, 25;
	mov.b64 	%rd736, {%r777, %r776};
	xor.b64  	%rd737, %rd735, %rd733;
	xor.b64  	%rd738, %rd736, %rd734;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd733, 32;
	shr.b64 	%rhs, %rd733, 32;
	add.u64 	%rd739, %lhs, %rhs;
	}
	add.s64 	%rd740, %rd734, %rd737;
	add.s64 	%rd741, %rd739, %rd738;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r778}, %rd737;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r779,%dummy}, %rd737;
	}
	shf.l.wrap.b32 	%r780, %r779, %r778, 13;
	shf.l.wrap.b32 	%r781, %r778, %r779, 13;
	mov.b64 	%rd742, {%r781, %r780};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r782}, %rd738;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r783,%dummy}, %rd738;
	}
	shf.l.wrap.b32 	%r784, %r783, %r782, 16;
	shf.l.wrap.b32 	%r785, %r782, %r783, 16;
	mov.b64 	%rd743, {%r785, %r784};
	xor.b64  	%rd744, %rd742, %rd740;
	xor.b64  	%rd745, %rd743, %rd741;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd740, 32;
	shr.b64 	%rhs, %rd740, 32;
	add.u64 	%rd746, %lhs, %rhs;
	}
	add.s64 	%rd747, %rd744, %rd741;
	add.s64 	%rd748, %rd745, %rd746;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r786}, %rd744;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r787,%dummy}, %rd744;
	}
	shf.l.wrap.b32 	%r788, %r787, %r786, 17;
	shf.l.wrap.b32 	%r789, %r786, %r787, 17;
	mov.b64 	%rd749, {%r789, %r788};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r790}, %rd745;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r791,%dummy}, %rd745;
	}
	shf.l.wrap.b32 	%r792, %r791, %r790, 25;
	shf.l.wrap.b32 	%r793, %r790, %r791, 25;
	mov.b64 	%rd750, {%r793, %r792};
	xor.b64  	%rd751, %rd749, %rd747;
	xor.b64  	%rd752, %rd750, %rd748;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd747, 32;
	shr.b64 	%rhs, %rd747, 32;
	add.u64 	%rd753, %lhs, %rhs;
	}
	add.s64 	%rd754, %rd748, %rd751;
	add.s64 	%rd755, %rd753, %rd752;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r794}, %rd751;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r795,%dummy}, %rd751;
	}
	shf.l.wrap.b32 	%r796, %r795, %r794, 13;
	shf.l.wrap.b32 	%r797, %r794, %r795, 13;
	mov.b64 	%rd756, {%r797, %r796};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r798}, %rd752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r799,%dummy}, %rd752;
	}
	shf.l.wrap.b32 	%r800, %r799, %r798, 16;
	shf.l.wrap.b32 	%r801, %r798, %r799, 16;
	mov.b64 	%rd757, {%r801, %r800};
	xor.b64  	%rd758, %rd756, %rd754;
	xor.b64  	%rd759, %rd757, %rd755;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd754, 32;
	shr.b64 	%rhs, %rd754, 32;
	add.u64 	%rd760, %lhs, %rhs;
	}
	add.s64 	%rd761, %rd758, %rd755;
	add.s64 	%rd762, %rd759, %rd760;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r802}, %rd758;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r803,%dummy}, %rd758;
	}
	shf.l.wrap.b32 	%r804, %r803, %r802, 17;
	shf.l.wrap.b32 	%r805, %r802, %r803, 17;
	mov.b64 	%rd763, {%r805, %r804};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r806}, %rd759;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r807,%dummy}, %rd759;
	}
	shf.l.wrap.b32 	%r808, %r807, %r806, 25;
	shf.l.wrap.b32 	%r809, %r806, %r807, 25;
	mov.b64 	%rd764, {%r809, %r808};
	xor.b64  	%rd765, %rd763, %rd761;
	xor.b64  	%rd766, %rd764, %rd762;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd761, 32;
	shr.b64 	%rhs, %rd761, 32;
	add.u64 	%rd767, %lhs, %rhs;
	}
	add.s64 	%rd768, %rd762, %rd765;
	add.s64 	%rd769, %rd767, %rd766;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r810}, %rd765;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r811,%dummy}, %rd765;
	}
	shf.l.wrap.b32 	%r812, %r811, %r810, 13;
	shf.l.wrap.b32 	%r813, %r810, %r811, 13;
	mov.b64 	%rd770, {%r813, %r812};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r814}, %rd766;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r815,%dummy}, %rd766;
	}
	shf.l.wrap.b32 	%r816, %r815, %r814, 16;
	shf.l.wrap.b32 	%r817, %r814, %r815, 16;
	mov.b64 	%rd771, {%r817, %r816};
	xor.b64  	%rd772, %rd770, %rd768;
	xor.b64  	%rd773, %rd771, %rd769;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd768, 32;
	shr.b64 	%rhs, %rd768, 32;
	add.u64 	%rd774, %lhs, %rhs;
	}
	add.s64 	%rd775, %rd772, %rd769;
	add.s64 	%rd776, %rd773, %rd774;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r818}, %rd772;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r819,%dummy}, %rd772;
	}
	shf.l.wrap.b32 	%r820, %r819, %r818, 17;
	shf.l.wrap.b32 	%r821, %r818, %r819, 17;
	mov.b64 	%rd777, {%r821, %r820};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r822}, %rd773;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r823,%dummy}, %rd773;
	}
	shf.l.wrap.b32 	%r824, %r823, %r822, 25;
	shf.l.wrap.b32 	%r825, %r822, %r823, 25;
	mov.b64 	%rd778, {%r825, %r824};
	xor.b64  	%rd779, %rd777, %rd775;
	xor.b64  	%rd780, %rd778, %rd776;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd775, 32;
	shr.b64 	%rhs, %rd775, 32;
	add.u64 	%rd781, %lhs, %rhs;
	}
	xor.b64  	%rd782, %rd781, %rd780;
	xor.b64  	%rd783, %rd782, %rd779;
	xor.b64  	%rd23, %rd783, %rd776;
	and.b64  	%rd784, %rd23, 268435455;
	shl.b64 	%rd785, %rd784, 1;
	or.b64  	%rd24, %rd785, 1;
	cvt.u32.u64	%r826, %rd785;
	or.b32  	%r11, %r826, 1;
	shr.u64 	%rd786, %rd23, 31;
	and.b64  	%rd787, %rd786, 536870910;
	or.b64  	%rd25, %rd787, 1;
	cvt.u32.u64	%r827, %rd786;
	and.b32  	%r828, %r827, 536870910;
	or.b32  	%r12, %r828, 1;
	shr.u32 	%r13, %r11, 17;
	shl.b32 	%r829, %r13, 3;
	add.s32 	%r14, %r45, %r829;
	atom.shared.exch.b64 	%rd26, [%r14], 0;
	setp.eq.s64	%p2, %rd26, 0;
	@%p2 bra 	BB4_5;
	bra.uni 	BB4_4;

BB4_5:
	shl.b64 	%rd793, %rd25, 32;
	or.b64  	%rd794, %rd793, %rd24;
	atom.shared.cas.b64 	%rd796, [%r14], %rd71, %rd794;
	setp.eq.s64	%p3, %rd796, 0;
	@%p3 bra 	BB4_7;

	shr.u32 	%r1133, %r11, 17;
	shr.u32 	%r844, %r11, 27;
	mul.lo.s32 	%r845, %r844, 140509184;
	mul.wide.s32 	%rd797, %r1133, 4;
	add.s64 	%rd798, %rd2, %rd797;
	atom.global.add.u32 	%r846, [%rd798], 2;
	mov.u32 	%r847, 34300;
	min.s32 	%r848, %r847, %r846;
	and.b32  	%r849, %r1133, 1023;
	mad.lo.s32 	%r850, %r849, 34304, %r845;
	add.s32 	%r851, %r850, %r848;
	shr.u32 	%r852, %r851, 31;
	add.s32 	%r853, %r851, %r852;
	shr.s32 	%r854, %r853, 1;
	mul.wide.s32 	%rd799, %r854, 16;
	add.s64 	%rd800, %rd1, %rd799;
	mov.u32 	%r855, 0;
	st.global.v4.u32 	[%rd800], {%r11, %r12, %r855, %r855};
	bra.uni 	BB4_7;

BB4_4:
	shr.u32 	%r1131, %r11, 17;
	shr.u32 	%r831, %r11, 27;
	mul.lo.s32 	%r832, %r831, 140509184;
	mul.wide.s32 	%rd788, %r1131, 4;
	add.s64 	%rd789, %rd2, %rd788;
	atom.global.add.u32 	%r833, [%rd789], 2;
	mov.u32 	%r834, 34300;
	min.s32 	%r835, %r834, %r833;
	and.b32  	%r836, %r1131, 1023;
	mad.lo.s32 	%r837, %r836, 34304, %r832;
	add.s32 	%r838, %r837, %r835;
	shr.u32 	%r839, %r838, 31;
	add.s32 	%r840, %r838, %r839;
	shr.s32 	%r841, %r840, 1;
	shr.u64 	%rd790, %rd26, 32;
	mul.wide.s32 	%rd791, %r841, 16;
	add.s64 	%rd792, %rd1, %rd791;
	cvt.u32.u64	%r842, %rd26;
	cvt.u32.u64	%r843, %rd790;
	st.global.v4.u32 	[%rd792], {%r842, %r843, %r11, %r12};

BB4_7:
	xor.b64  	%rd801, %rd16, %rd15;
	xor.b64  	%rd802, %rd801, %rd14;
	xor.b64  	%rd803, %rd802, %rd23;
	and.b64  	%rd27, %rd803, 268435455;
	shl.b64 	%rd28, %rd27, 1;
	cvt.u32.u64	%r15, %rd28;
	shr.u64 	%rd804, %rd803, 31;
	cvt.u32.u64	%r856, %rd804;
	and.b32  	%r16, %r856, 536870910;
	bfe.u64 	%rd29, %rd803, 16, 12;
	cvt.u32.u64	%r17, %rd29;
	shl.b32 	%r857, %r17, 3;
	add.s32 	%r18, %r45, %r857;
	atom.shared.exch.b64 	%rd30, [%r18], 0;
	setp.eq.s64	%p4, %rd30, 0;
	@%p4 bra 	BB4_9;
	bra.uni 	BB4_8;

BB4_9:
	cvt.u64.u32	%rd811, %r16;
	shl.b64 	%rd812, %rd811, 32;
	or.b64  	%rd813, %rd812, %rd28;
	atom.shared.cas.b64 	%rd815, [%r18], %rd71, %rd813;
	setp.eq.s64	%p5, %rd815, 0;
	@%p5 bra 	BB4_11;

	shr.u64 	%rd816, %rd27, 26;
	cvt.u32.u64	%r872, %rd816;
	mul.lo.s32 	%r873, %r872, 140509184;
	shl.b64 	%rd817, %rd29, 2;
	add.s64 	%rd818, %rd2, %rd817;
	atom.global.add.u32 	%r874, [%rd818], 2;
	mov.u32 	%r875, 34300;
	min.s32 	%r876, %r875, %r874;
	and.b32  	%r877, %r17, 1023;
	mad.lo.s32 	%r878, %r877, 34304, %r873;
	add.s32 	%r879, %r878, %r876;
	shr.u32 	%r880, %r879, 31;
	add.s32 	%r881, %r879, %r880;
	shr.s32 	%r882, %r881, 1;
	mul.wide.s32 	%rd819, %r882, 16;
	add.s64 	%rd820, %rd1, %rd819;
	mov.u32 	%r883, 0;
	st.global.v4.u32 	[%rd820], {%r15, %r16, %r883, %r883};
	bra.uni 	BB4_11;

BB4_8:
	shr.u64 	%rd805, %rd27, 26;
	cvt.u32.u64	%r859, %rd805;
	mul.lo.s32 	%r860, %r859, 140509184;
	shl.b64 	%rd806, %rd29, 2;
	add.s64 	%rd807, %rd2, %rd806;
	atom.global.add.u32 	%r861, [%rd807], 2;
	mov.u32 	%r862, 34300;
	min.s32 	%r863, %r862, %r861;
	and.b32  	%r864, %r17, 1023;
	mad.lo.s32 	%r865, %r864, 34304, %r860;
	add.s32 	%r866, %r865, %r863;
	shr.u32 	%r867, %r866, 31;
	add.s32 	%r868, %r866, %r867;
	shr.s32 	%r869, %r868, 1;
	shr.u64 	%rd808, %rd30, 32;
	mul.wide.s32 	%rd809, %r869, 16;
	add.s64 	%rd810, %rd1, %rd809;
	cvt.u32.u64	%r870, %rd30;
	cvt.u32.u64	%r871, %rd808;
	st.global.v4.u32 	[%rd810], {%r870, %r871, %r15, %r16};

BB4_11:
	xor.b64  	%rd821, %rd19, %rd18;
	xor.b64  	%rd822, %rd821, %rd17;
	xor.b64  	%rd823, %rd822, %rd23;
	and.b64  	%rd824, %rd823, 268435455;
	shl.b64 	%rd825, %rd824, 1;
	or.b64  	%rd31, %rd825, 1;
	cvt.u32.u64	%r884, %rd825;
	or.b32  	%r19, %r884, 1;
	shr.u64 	%rd826, %rd823, 31;
	and.b64  	%rd827, %rd826, 536870910;
	or.b64  	%rd32, %rd827, 1;
	cvt.u32.u64	%r885, %rd826;
	and.b32  	%r886, %r885, 536870910;
	or.b32  	%r20, %r886, 1;
	shr.u32 	%r21, %r19, 17;
	shl.b32 	%r887, %r21, 3;
	add.s32 	%r22, %r45, %r887;
	atom.shared.exch.b64 	%rd33, [%r22], 0;
	setp.eq.s64	%p6, %rd33, 0;
	@%p6 bra 	BB4_13;
	bra.uni 	BB4_12;

BB4_13:
	shl.b64 	%rd833, %rd32, 32;
	or.b64  	%rd834, %rd833, %rd31;
	atom.shared.cas.b64 	%rd836, [%r22], %rd71, %rd834;
	setp.eq.s64	%p7, %rd836, 0;
	@%p7 bra 	BB4_15;

	shr.u32 	%r902, %r19, 27;
	mul.lo.s32 	%r903, %r902, 140509184;
	mul.wide.s32 	%rd837, %r21, 4;
	add.s64 	%rd838, %rd2, %rd837;
	atom.global.add.u32 	%r904, [%rd838], 2;
	mov.u32 	%r905, 34300;
	min.s32 	%r906, %r905, %r904;
	and.b32  	%r907, %r21, 1023;
	mad.lo.s32 	%r908, %r907, 34304, %r903;
	add.s32 	%r909, %r908, %r906;
	shr.u32 	%r910, %r909, 31;
	add.s32 	%r911, %r909, %r910;
	shr.s32 	%r912, %r911, 1;
	mul.wide.s32 	%rd839, %r912, 16;
	add.s64 	%rd840, %rd1, %rd839;
	mov.u32 	%r913, 0;
	st.global.v4.u32 	[%rd840], {%r19, %r20, %r913, %r913};
	bra.uni 	BB4_15;

BB4_12:
	shr.u32 	%r889, %r19, 27;
	mul.lo.s32 	%r890, %r889, 140509184;
	mul.wide.s32 	%rd828, %r21, 4;
	add.s64 	%rd829, %rd2, %rd828;
	atom.global.add.u32 	%r891, [%rd829], 2;
	mov.u32 	%r892, 34300;
	min.s32 	%r893, %r892, %r891;
	and.b32  	%r894, %r21, 1023;
	mad.lo.s32 	%r895, %r894, 34304, %r890;
	add.s32 	%r896, %r895, %r893;
	shr.u32 	%r897, %r896, 31;
	add.s32 	%r898, %r896, %r897;
	shr.s32 	%r899, %r898, 1;
	shr.u64 	%rd830, %rd33, 32;
	mul.wide.s32 	%rd831, %r899, 16;
	add.s64 	%rd832, %rd1, %rd831;
	cvt.u32.u64	%r900, %rd33;
	cvt.u32.u64	%r901, %rd830;
	st.global.v4.u32 	[%rd832], {%r900, %r901, %r19, %r20};

BB4_15:
	xor.b64  	%rd841, %rd22, %rd21;
	xor.b64  	%rd842, %rd841, %rd20;
	xor.b64  	%rd843, %rd842, %rd23;
	and.b64  	%rd34, %rd843, 268435455;
	shl.b64 	%rd35, %rd34, 1;
	cvt.u32.u64	%r23, %rd35;
	shr.u64 	%rd844, %rd843, 31;
	cvt.u32.u64	%r914, %rd844;
	and.b32  	%r24, %r914, 536870910;
	bfe.u64 	%rd36, %rd843, 16, 12;
	cvt.u32.u64	%r25, %rd36;
	shl.b32 	%r915, %r25, 3;
	add.s32 	%r26, %r45, %r915;
	atom.shared.exch.b64 	%rd37, [%r26], 0;
	setp.eq.s64	%p8, %rd37, 0;
	@%p8 bra 	BB4_17;
	bra.uni 	BB4_16;

BB4_17:
	cvt.u64.u32	%rd851, %r24;
	shl.b64 	%rd852, %rd851, 32;
	or.b64  	%rd853, %rd852, %rd35;
	atom.shared.cas.b64 	%rd855, [%r26], %rd71, %rd853;
	setp.eq.s64	%p9, %rd855, 0;
	mov.u16 	%rs15, 15;
	@%p9 bra 	BB4_19;

	shr.u64 	%rd856, %rd34, 26;
	cvt.u32.u64	%r930, %rd856;
	mul.lo.s32 	%r931, %r930, 140509184;
	shl.b64 	%rd857, %rd36, 2;
	add.s64 	%rd858, %rd2, %rd857;
	atom.global.add.u32 	%r932, [%rd858], 2;
	mov.u32 	%r933, 34300;
	min.s32 	%r934, %r933, %r932;
	and.b32  	%r935, %r25, 1023;
	mad.lo.s32 	%r936, %r935, 34304, %r931;
	add.s32 	%r937, %r936, %r934;
	shr.u32 	%r938, %r937, 31;
	add.s32 	%r939, %r937, %r938;
	shr.s32 	%r940, %r939, 1;
	mul.wide.s32 	%rd859, %r940, 16;
	add.s64 	%rd860, %rd1, %rd859;
	mov.u32 	%r941, 0;
	st.global.v4.u32 	[%rd860], {%r23, %r24, %r941, %r941};
	bra.uni 	BB4_19;

BB4_16:
	shr.u64 	%rd845, %rd34, 26;
	cvt.u32.u64	%r917, %rd845;
	mul.lo.s32 	%r918, %r917, 140509184;
	shl.b64 	%rd846, %rd36, 2;
	add.s64 	%rd847, %rd2, %rd846;
	atom.global.add.u32 	%r919, [%rd847], 2;
	mov.u32 	%r920, 34300;
	min.s32 	%r921, %r920, %r919;
	and.b32  	%r922, %r25, 1023;
	mad.lo.s32 	%r923, %r922, 34304, %r918;
	add.s32 	%r924, %r923, %r921;
	shr.u32 	%r925, %r924, 31;
	add.s32 	%r926, %r924, %r925;
	shr.s32 	%r927, %r926, 1;
	shr.u64 	%rd848, %rd37, 32;
	mul.wide.s32 	%rd849, %r927, 16;
	add.s64 	%rd850, %rd1, %rd849;
	cvt.u32.u64	%r928, %rd37;
	cvt.u32.u64	%r929, %rd848;
	st.global.v4.u32 	[%rd850], {%r928, %r929, %r23, %r24};
	mov.u16 	%rs15, 15;

BB4_19:
	mov.u64 	%rd967, %rd4;

BB4_20:
	ld.local.v2.u64 	{%rd861, %rd862}, [%rd967+16];
	ld.local.v2.u64 	{%rd863, %rd864}, [%rd967];
	xor.b64  	%rd865, %rd863, %rd23;
	and.b64  	%rd46, %rd865, 268435455;
	shl.b64 	%rd47, %rd46, 1;
	cvt.u32.u64	%r27, %rd47;
	shr.u64 	%rd866, %rd865, 31;
	cvt.u32.u64	%r942, %rd866;
	and.b32  	%r28, %r942, 536870910;
	bfe.u64 	%rd48, %rd865, 16, 12;
	cvt.u32.u64	%r29, %rd48;
	shl.b32 	%r943, %r29, 3;
	add.s32 	%r30, %r45, %r943;
	atom.shared.exch.b64 	%rd49, [%r30], 0;
	setp.eq.s64	%p10, %rd49, 0;
	@%p10 bra 	BB4_22;
	bra.uni 	BB4_21;

BB4_22:
	cvt.u64.u32	%rd873, %r28;
	shl.b64 	%rd874, %rd873, 32;
	or.b64  	%rd875, %rd874, %rd47;
	atom.shared.cas.b64 	%rd877, [%r30], %rd71, %rd875;
	setp.eq.s64	%p11, %rd877, 0;
	@%p11 bra 	BB4_24;

	shr.u64 	%rd878, %rd46, 26;
	cvt.u32.u64	%r958, %rd878;
	mul.lo.s32 	%r959, %r958, 140509184;
	shl.b64 	%rd879, %rd48, 2;
	add.s64 	%rd880, %rd2, %rd879;
	atom.global.add.u32 	%r960, [%rd880], 2;
	mov.u32 	%r961, 34300;
	min.s32 	%r962, %r961, %r960;
	and.b32  	%r963, %r29, 1023;
	mad.lo.s32 	%r964, %r963, 34304, %r959;
	add.s32 	%r965, %r964, %r962;
	shr.u32 	%r966, %r965, 31;
	add.s32 	%r967, %r965, %r966;
	shr.s32 	%r968, %r967, 1;
	mul.wide.s32 	%rd881, %r968, 16;
	add.s64 	%rd882, %rd1, %rd881;
	mov.u32 	%r969, 0;
	st.global.v4.u32 	[%rd882], {%r27, %r28, %r969, %r969};
	bra.uni 	BB4_24;

BB4_21:
	shr.u64 	%rd867, %rd46, 26;
	cvt.u32.u64	%r945, %rd867;
	mul.lo.s32 	%r946, %r945, 140509184;
	shl.b64 	%rd868, %rd48, 2;
	add.s64 	%rd869, %rd2, %rd868;
	atom.global.add.u32 	%r947, [%rd869], 2;
	mov.u32 	%r948, 34300;
	min.s32 	%r949, %r948, %r947;
	and.b32  	%r950, %r29, 1023;
	mad.lo.s32 	%r951, %r950, 34304, %r946;
	add.s32 	%r952, %r951, %r949;
	shr.u32 	%r953, %r952, 31;
	add.s32 	%r954, %r952, %r953;
	shr.s32 	%r955, %r954, 1;
	shr.u64 	%rd870, %rd49, 32;
	mul.wide.s32 	%rd871, %r955, 16;
	add.s64 	%rd872, %rd1, %rd871;
	cvt.u32.u64	%r956, %rd49;
	cvt.u32.u64	%r957, %rd870;
	st.global.v4.u32 	[%rd872], {%r956, %r957, %r27, %r28};

BB4_24:
	xor.b64  	%rd883, %rd864, %rd23;
	and.b64  	%rd884, %rd883, 268435455;
	shl.b64 	%rd885, %rd884, 1;
	or.b64  	%rd50, %rd885, 1;
	cvt.u32.u64	%r970, %rd885;
	or.b32  	%r31, %r970, 1;
	shr.u64 	%rd886, %rd883, 31;
	and.b64  	%rd887, %rd886, 536870910;
	or.b64  	%rd51, %rd887, 1;
	cvt.u32.u64	%r971, %rd886;
	and.b32  	%r972, %r971, 536870910;
	or.b32  	%r32, %r972, 1;
	shr.u32 	%r33, %r31, 17;
	shl.b32 	%r973, %r33, 3;
	add.s32 	%r34, %r45, %r973;
	atom.shared.exch.b64 	%rd52, [%r34], 0;
	setp.eq.s64	%p12, %rd52, 0;
	@%p12 bra 	BB4_26;
	bra.uni 	BB4_25;

BB4_26:
	shl.b64 	%rd893, %rd51, 32;
	or.b64  	%rd894, %rd893, %rd50;
	atom.shared.cas.b64 	%rd896, [%r34], %rd71, %rd894;
	setp.eq.s64	%p13, %rd896, 0;
	@%p13 bra 	BB4_28;

	shr.u32 	%r988, %r31, 27;
	mul.lo.s32 	%r989, %r988, 140509184;
	mul.wide.s32 	%rd897, %r33, 4;
	add.s64 	%rd898, %rd2, %rd897;
	atom.global.add.u32 	%r990, [%rd898], 2;
	mov.u32 	%r991, 34300;
	min.s32 	%r992, %r991, %r990;
	and.b32  	%r993, %r33, 1023;
	mad.lo.s32 	%r994, %r993, 34304, %r989;
	add.s32 	%r995, %r994, %r992;
	shr.u32 	%r996, %r995, 31;
	add.s32 	%r997, %r995, %r996;
	shr.s32 	%r998, %r997, 1;
	mul.wide.s32 	%rd899, %r998, 16;
	add.s64 	%rd900, %rd1, %rd899;
	mov.u32 	%r999, 0;
	st.global.v4.u32 	[%rd900], {%r31, %r32, %r999, %r999};
	bra.uni 	BB4_28;

BB4_25:
	shr.u32 	%r975, %r31, 27;
	mul.lo.s32 	%r976, %r975, 140509184;
	mul.wide.s32 	%rd888, %r33, 4;
	add.s64 	%rd889, %rd2, %rd888;
	atom.global.add.u32 	%r977, [%rd889], 2;
	mov.u32 	%r978, 34300;
	min.s32 	%r979, %r978, %r977;
	and.b32  	%r980, %r33, 1023;
	mad.lo.s32 	%r981, %r980, 34304, %r976;
	add.s32 	%r982, %r981, %r979;
	shr.u32 	%r983, %r982, 31;
	add.s32 	%r984, %r982, %r983;
	shr.s32 	%r985, %r984, 1;
	shr.u64 	%rd890, %rd52, 32;
	mul.wide.s32 	%rd891, %r985, 16;
	add.s64 	%rd892, %rd1, %rd891;
	cvt.u32.u64	%r986, %rd52;
	cvt.u32.u64	%r987, %rd890;
	st.global.v4.u32 	[%rd892], {%r986, %r987, %r31, %r32};

BB4_28:
	xor.b64  	%rd901, %rd861, %rd23;
	and.b64  	%rd53, %rd901, 268435455;
	shl.b64 	%rd54, %rd53, 1;
	cvt.u32.u64	%r35, %rd54;
	shr.u64 	%rd902, %rd901, 31;
	cvt.u32.u64	%r1000, %rd902;
	and.b32  	%r36, %r1000, 536870910;
	bfe.u64 	%rd55, %rd901, 16, 12;
	cvt.u32.u64	%r37, %rd55;
	shl.b32 	%r1001, %r37, 3;
	add.s32 	%r38, %r45, %r1001;
	atom.shared.exch.b64 	%rd56, [%r38], 0;
	setp.eq.s64	%p14, %rd56, 0;
	@%p14 bra 	BB4_30;
	bra.uni 	BB4_29;

BB4_30:
	cvt.u64.u32	%rd909, %r36;
	shl.b64 	%rd910, %rd909, 32;
	or.b64  	%rd911, %rd910, %rd54;
	atom.shared.cas.b64 	%rd913, [%r38], %rd71, %rd911;
	setp.eq.s64	%p15, %rd913, 0;
	@%p15 bra 	BB4_32;

	shr.u64 	%rd914, %rd53, 26;
	cvt.u32.u64	%r1016, %rd914;
	mul.lo.s32 	%r1017, %r1016, 140509184;
	shl.b64 	%rd915, %rd55, 2;
	add.s64 	%rd916, %rd2, %rd915;
	atom.global.add.u32 	%r1018, [%rd916], 2;
	mov.u32 	%r1019, 34300;
	min.s32 	%r1020, %r1019, %r1018;
	and.b32  	%r1021, %r37, 1023;
	mad.lo.s32 	%r1022, %r1021, 34304, %r1017;
	add.s32 	%r1023, %r1022, %r1020;
	shr.u32 	%r1024, %r1023, 31;
	add.s32 	%r1025, %r1023, %r1024;
	shr.s32 	%r1026, %r1025, 1;
	mul.wide.s32 	%rd917, %r1026, 16;
	add.s64 	%rd918, %rd1, %rd917;
	mov.u32 	%r1027, 0;
	st.global.v4.u32 	[%rd918], {%r35, %r36, %r1027, %r1027};
	bra.uni 	BB4_32;

BB4_29:
	shr.u64 	%rd903, %rd53, 26;
	cvt.u32.u64	%r1003, %rd903;
	mul.lo.s32 	%r1004, %r1003, 140509184;
	shl.b64 	%rd904, %rd55, 2;
	add.s64 	%rd905, %rd2, %rd904;
	atom.global.add.u32 	%r1005, [%rd905], 2;
	mov.u32 	%r1006, 34300;
	min.s32 	%r1007, %r1006, %r1005;
	and.b32  	%r1008, %r37, 1023;
	mad.lo.s32 	%r1009, %r1008, 34304, %r1004;
	add.s32 	%r1010, %r1009, %r1007;
	shr.u32 	%r1011, %r1010, 31;
	add.s32 	%r1012, %r1010, %r1011;
	shr.s32 	%r1013, %r1012, 1;
	shr.u64 	%rd906, %rd56, 32;
	mul.wide.s32 	%rd907, %r1013, 16;
	add.s64 	%rd908, %rd1, %rd907;
	cvt.u32.u64	%r1014, %rd56;
	cvt.u32.u64	%r1015, %rd906;
	st.global.v4.u32 	[%rd908], {%r1014, %r1015, %r35, %r36};

BB4_32:
	xor.b64  	%rd919, %rd862, %rd23;
	and.b64  	%rd920, %rd919, 268435455;
	shl.b64 	%rd921, %rd920, 1;
	or.b64  	%rd57, %rd921, 1;
	cvt.u32.u64	%r1028, %rd921;
	or.b32  	%r39, %r1028, 1;
	shr.u64 	%rd922, %rd919, 31;
	and.b64  	%rd923, %rd922, 536870910;
	or.b64  	%rd58, %rd923, 1;
	cvt.u32.u64	%r1029, %rd922;
	and.b32  	%r1030, %r1029, 536870910;
	or.b32  	%r40, %r1030, 1;
	shr.u32 	%r41, %r39, 17;
	shl.b32 	%r1031, %r41, 3;
	add.s32 	%r42, %r45, %r1031;
	atom.shared.exch.b64 	%rd59, [%r42], 0;
	setp.eq.s64	%p16, %rd59, 0;
	@%p16 bra 	BB4_34;
	bra.uni 	BB4_33;

BB4_34:
	shl.b64 	%rd929, %rd58, 32;
	or.b64  	%rd930, %rd929, %rd57;
	atom.shared.cas.b64 	%rd932, [%r42], %rd71, %rd930;
	setp.eq.s64	%p17, %rd932, 0;
	@%p17 bra 	BB4_36;

	shr.u32 	%r1046, %r39, 27;
	mul.lo.s32 	%r1047, %r1046, 140509184;
	mul.wide.s32 	%rd933, %r41, 4;
	add.s64 	%rd934, %rd2, %rd933;
	atom.global.add.u32 	%r1048, [%rd934], 2;
	mov.u32 	%r1049, 34300;
	min.s32 	%r1050, %r1049, %r1048;
	and.b32  	%r1051, %r41, 1023;
	mad.lo.s32 	%r1052, %r1051, 34304, %r1047;
	add.s32 	%r1053, %r1052, %r1050;
	shr.u32 	%r1054, %r1053, 31;
	add.s32 	%r1055, %r1053, %r1054;
	shr.s32 	%r1056, %r1055, 1;
	mul.wide.s32 	%rd935, %r1056, 16;
	add.s64 	%rd936, %rd1, %rd935;
	mov.u32 	%r1057, 0;
	st.global.v4.u32 	[%rd936], {%r39, %r40, %r1057, %r1057};
	bra.uni 	BB4_36;

BB4_33:
	shr.u32 	%r1033, %r39, 27;
	mul.lo.s32 	%r1034, %r1033, 140509184;
	mul.wide.s32 	%rd924, %r41, 4;
	add.s64 	%rd925, %rd2, %rd924;
	atom.global.add.u32 	%r1035, [%rd925], 2;
	mov.u32 	%r1036, 34300;
	min.s32 	%r1037, %r1036, %r1035;
	and.b32  	%r1038, %r41, 1023;
	mad.lo.s32 	%r1039, %r1038, 34304, %r1034;
	add.s32 	%r1040, %r1039, %r1037;
	shr.u32 	%r1041, %r1040, 31;
	add.s32 	%r1042, %r1040, %r1041;
	shr.s32 	%r1043, %r1042, 1;
	shr.u64 	%rd926, %rd59, 32;
	mul.wide.s32 	%rd927, %r1043, 16;
	add.s64 	%rd928, %rd1, %rd927;
	cvt.u32.u64	%r1044, %rd59;
	cvt.u32.u64	%r1045, %rd926;
	st.global.v4.u32 	[%rd928], {%r1044, %r1045, %r39, %r40};

BB4_36:
	add.s16 	%rs15, %rs15, -1;
	add.s64 	%rd967, %rd967, -32;
	setp.gt.s16	%p18, %rs15, 0;
	@%p18 bra 	BB4_20;

	cvt.u32.u16	%r1058, %rs13;
	add.s32 	%r1059, %r1058, 64;
	cvt.u16.u32	%rs13, %r1059;
	setp.lt.s16	%p19, %rs13, 256;
	@%p19 bra 	BB4_1;

	bar.sync 	0;
	ld.shared.u64 	%rd61, [%r2];
	setp.eq.s64	%p20, %rd61, 0;
	@%p20 bra 	BB4_40;

	mov.u32 	%r1132, %tid.x;
	shr.s32 	%r1060, %r1132, 31;
	shr.u32 	%r1061, %r1060, 22;
	add.s32 	%r1062, %r1132, %r1061;
	shr.s32 	%r1063, %r1062, 10;
	mul.wide.s32 	%rd937, %r1132, 4;
	add.s64 	%rd938, %rd2, %rd937;
	atom.global.add.u32 	%r1064, [%rd938], 2;
	mov.u32 	%r1065, 34300;
	min.s32 	%r1066, %r1065, %r1064;
	and.b32  	%r1067, %r1062, -1024;
	sub.s32 	%r1068, %r1132, %r1067;
	mad.lo.s32 	%r1069, %r1063, 140509184, %r1066;
	mad.lo.s32 	%r1070, %r1068, 34304, %r1069;
	shr.u32 	%r1071, %r1070, 31;
	add.s32 	%r1072, %r1070, %r1071;
	shr.s32 	%r1073, %r1072, 1;
	shr.u64 	%rd939, %rd61, 32;
	mul.wide.s32 	%rd940, %r1073, 16;
	add.s64 	%rd941, %rd1, %rd940;
	cvt.u32.u64	%r1074, %rd61;
	cvt.u32.u64	%r1075, %rd939;
	mov.u32 	%r1076, 0;
	st.global.v4.u32 	[%rd941], {%r1074, %r1075, %r1076, %r1076};

BB4_40:
	add.s32 	%r1128, %r2, 8192;
	ld.shared.u64 	%rd62, [%r1128];
	setp.eq.s64	%p21, %rd62, 0;
	@%p21 bra 	BB4_42;

	mov.u32 	%r1135, %tid.x;
	add.s32 	%r1134, %r1135, 1024;
	shr.s32 	%r1077, %r1134, 31;
	shr.u32 	%r1078, %r1077, 22;
	add.s32 	%r1079, %r1134, %r1078;
	shr.s32 	%r1080, %r1079, 10;
	mul.wide.s32 	%rd942, %r1134, 4;
	add.s64 	%rd943, %rd2, %rd942;
	atom.global.add.u32 	%r1081, [%rd943], 2;
	mov.u32 	%r1082, 34300;
	min.s32 	%r1083, %r1082, %r1081;
	and.b32  	%r1084, %r1079, -1024;
	sub.s32 	%r1085, %r1134, %r1084;
	mad.lo.s32 	%r1086, %r1080, 140509184, %r1083;
	mad.lo.s32 	%r1087, %r1085, 34304, %r1086;
	shr.u32 	%r1088, %r1087, 31;
	add.s32 	%r1089, %r1087, %r1088;
	shr.s32 	%r1090, %r1089, 1;
	shr.u64 	%rd944, %rd62, 32;
	mul.wide.s32 	%rd945, %r1090, 16;
	add.s64 	%rd946, %rd1, %rd945;
	cvt.u32.u64	%r1091, %rd62;
	cvt.u32.u64	%r1092, %rd944;
	mov.u32 	%r1093, 0;
	st.global.v4.u32 	[%rd946], {%r1091, %r1092, %r1093, %r1093};

BB4_42:
	add.s32 	%r1129, %r2, 16384;
	ld.shared.u64 	%rd63, [%r1129];
	setp.eq.s64	%p22, %rd63, 0;
	@%p22 bra 	BB4_44;

	mov.u32 	%r1137, %tid.x;
	add.s32 	%r1136, %r1137, 2048;
	shr.s32 	%r1094, %r1136, 31;
	shr.u32 	%r1095, %r1094, 22;
	add.s32 	%r1096, %r1136, %r1095;
	shr.s32 	%r1097, %r1096, 10;
	mul.wide.s32 	%rd947, %r1136, 4;
	add.s64 	%rd948, %rd2, %rd947;
	atom.global.add.u32 	%r1098, [%rd948], 2;
	mov.u32 	%r1099, 34300;
	min.s32 	%r1100, %r1099, %r1098;
	and.b32  	%r1101, %r1096, -1024;
	sub.s32 	%r1102, %r1136, %r1101;
	mad.lo.s32 	%r1103, %r1097, 140509184, %r1100;
	mad.lo.s32 	%r1104, %r1102, 34304, %r1103;
	shr.u32 	%r1105, %r1104, 31;
	add.s32 	%r1106, %r1104, %r1105;
	shr.s32 	%r1107, %r1106, 1;
	shr.u64 	%rd949, %rd63, 32;
	mul.wide.s32 	%rd950, %r1107, 16;
	add.s64 	%rd951, %rd1, %rd950;
	cvt.u32.u64	%r1108, %rd63;
	cvt.u32.u64	%r1109, %rd949;
	mov.u32 	%r1110, 0;
	st.global.v4.u32 	[%rd951], {%r1108, %r1109, %r1110, %r1110};

BB4_44:
	add.s32 	%r1130, %r2, 24576;
	ld.shared.u64 	%rd64, [%r1130];
	setp.eq.s64	%p23, %rd64, 0;
	@%p23 bra 	BB4_46;

	mov.u32 	%r1139, %tid.x;
	add.s32 	%r1138, %r1139, 3072;
	shr.s32 	%r1111, %r1138, 31;
	shr.u32 	%r1112, %r1111, 22;
	add.s32 	%r1113, %r1138, %r1112;
	shr.s32 	%r1114, %r1113, 10;
	mul.wide.s32 	%rd952, %r1138, 4;
	add.s64 	%rd953, %rd2, %rd952;
	atom.global.add.u32 	%r1115, [%rd953], 2;
	mov.u32 	%r1116, 34300;
	min.s32 	%r1117, %r1116, %r1115;
	and.b32  	%r1118, %r1113, -1024;
	sub.s32 	%r1119, %r1138, %r1118;
	mad.lo.s32 	%r1120, %r1114, 140509184, %r1117;
	mad.lo.s32 	%r1121, %r1119, 34304, %r1120;
	shr.u32 	%r1122, %r1121, 31;
	add.s32 	%r1123, %r1121, %r1122;
	shr.s32 	%r1124, %r1123, 1;
	shr.u64 	%rd954, %rd64, 32;
	mul.wide.s32 	%rd955, %r1124, 16;
	add.s64 	%rd956, %rd1, %rd955;
	cvt.u32.u64	%r1125, %rd64;
	cvt.u32.u64	%r1126, %rd954;
	mov.u32 	%r1127, 0;
	st.global.v4.u32 	[%rd956], {%r1125, %r1126, %r1127, %r1127};

BB4_46:
	ret;
}

	// .globl	FluffyRound_C1
.visible .entry FluffyRound_C1(
	.param .u64 FluffyRound_C1_param_0,
	.param .u64 FluffyRound_C1_param_1,
	.param .u64 FluffyRound_C1_param_2,
	.param .u64 FluffyRound_C1_param_3,
	.param .u32 FluffyRound_C1_param_4,
	.param .u32 FluffyRound_C1_param_5,
	.param .u32 FluffyRound_C1_param_6
)
{
	.reg .pred 	%p<249>;
	.reg .b32 	%r<1842>;
	.reg .b64 	%rd<574>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_C1$__cuda_local_var_207714_30_non_const_ecounters[49152];

	ld.param.u64 	%rd36, [FluffyRound_C1_param_0];
	ld.param.u64 	%rd37, [FluffyRound_C1_param_1];
	ld.param.u64 	%rd38, [FluffyRound_C1_param_2];
	ld.param.u64 	%rd39, [FluffyRound_C1_param_3];
	ld.param.u32 	%r299, [FluffyRound_C1_param_5];
	ld.param.u32 	%r300, [FluffyRound_C1_param_6];
	cvta.to.global.u64 	%rd1, %rd37;
	cvta.to.global.u64 	%rd2, %rd36;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r301, %r1, 2;
	mov.u32 	%r302, FluffyRound_C1$__cuda_local_var_207714_30_non_const_ecounters;
	add.s32 	%r303, %r302, %r301;
	mov.u32 	%r304, 0;
	st.shared.u32 	[%r303], %r304;
	add.s32 	%r2, %r1, 1024;
	st.shared.u32 	[%r303+4096], %r304;
	add.s32 	%r3, %r1, 2048;
	st.shared.u32 	[%r303+8192], %r304;
	add.s32 	%r4, %r1, 3072;
	st.shared.u32 	[%r303+12288], %r304;
	st.shared.u32 	[%r303+16384], %r304;
	st.shared.u32 	[%r303+20480], %r304;
	st.shared.u32 	[%r303+24576], %r304;
	st.shared.u32 	[%r303+28672], %r304;
	cvta.to.global.u64 	%rd3, %rd39;
	mov.u32 	%r305, %ctaid.x;
	add.s32 	%r306, %r305, %r300;
	cvta.to.global.u64 	%rd40, %rd38;
	mul.wide.s32 	%rd41, %r306, 4;
	add.s64 	%rd42, %rd40, %rd41;
	ld.global.u32 	%r307, [%rd42];
	mov.u32 	%r308, 34300;
	min.s32 	%r5, %r307, %r308;
	add.s32 	%r6, %r5, 1024;
	shr.s32 	%r309, %r6, 31;
	shr.u32 	%r310, %r309, 22;
	add.s32 	%r311, %r6, %r310;
	shr.s32 	%r7, %r311, 10;
	add.s32 	%r312, %r306, 4096;
	mul.wide.s32 	%rd43, %r312, 4;
	add.s64 	%rd44, %rd40, %rd43;
	ld.global.u32 	%r313, [%rd44];
	min.s32 	%r8, %r313, %r308;
	add.s32 	%r9, %r8, 1024;
	shr.s32 	%r314, %r9, 31;
	shr.u32 	%r315, %r314, 22;
	add.s32 	%r316, %r9, %r315;
	shr.s32 	%r10, %r316, 10;
	add.s32 	%r317, %r306, 8192;
	mul.wide.s32 	%rd45, %r317, 4;
	add.s64 	%rd46, %rd40, %rd45;
	ld.global.u32 	%r318, [%rd46];
	min.s32 	%r11, %r318, %r308;
	add.s32 	%r12, %r11, 1024;
	shr.s32 	%r319, %r12, 31;
	shr.u32 	%r320, %r319, 22;
	add.s32 	%r321, %r12, %r320;
	shr.s32 	%r13, %r321, 10;
	add.s32 	%r322, %r306, 12288;
	mul.wide.s32 	%rd47, %r322, 4;
	add.s64 	%rd48, %rd40, %rd47;
	ld.global.u32 	%r323, [%rd48];
	min.s32 	%r14, %r323, %r308;
	add.s32 	%r15, %r14, 1024;
	shr.s32 	%r324, %r15, 31;
	shr.u32 	%r325, %r324, 22;
	add.s32 	%r326, %r15, %r325;
	shr.s32 	%r16, %r326, 10;
	shl.b32 	%r17, %r300, 2;
	add.s32 	%r327, %r305, %r17;
	mul.lo.s32 	%r18, %r327, 34304;
	add.s32 	%r19, %r18, 35127296;
	add.s32 	%r20, %r18, 70254592;
	add.s32 	%r21, %r18, 105381888;
	bar.sync 	0;
	setp.lt.s32	%p2, %r6, 1024;
	@%p2 bra 	BB5_31;

	mov.u32 	%r332, 1;
	max.s32 	%r22, %r7, %r332;
	and.b32  	%r331, %r22, 3;
	mov.u32 	%r1802, 0;
	setp.eq.s32	%p3, %r331, 0;
	@%p3 bra 	BB5_16;

	setp.eq.s32	%p4, %r331, 1;
	@%p4 bra 	BB5_12;

	setp.eq.s32	%p5, %r331, 2;
	@%p5 bra 	BB5_8;

	setp.ge.s32	%p6, %r1, %r5;
	@%p6 bra 	BB5_5;

	add.s32 	%r335, %r1, %r18;
	mul.wide.s32 	%rd49, %r335, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.v2.u32 	{%r336, %r337}, [%rd50];
	or.b32  	%r339, %r336, %r337;
	setp.eq.s32	%p7, %r339, 0;
	mov.u32 	%r1802, %r332;
	@%p7 bra 	BB5_8;

	and.b32  	%r341, %r336, 131040;
	and.b32  	%r342, %r336, 31;
	mov.u32 	%r1802, 1;
	shl.b32 	%r343, %r1802, %r342;
	shr.u32 	%r344, %r341, 3;
	add.s32 	%r346, %r302, %r344;
	atom.shared.or.b32 	%r347, [%r346], %r343;
	bra.uni 	BB5_8;

BB5_5:
	mov.u32 	%r1802, %r332;

BB5_8:
	shl.b32 	%r348, %r1802, 10;
	add.s32 	%r25, %r348, %r1;
	setp.ge.s32	%p8, %r25, %r5;
	@%p8 bra 	BB5_11;

	add.s32 	%r349, %r25, %r18;
	mul.wide.s32 	%rd51, %r349, 8;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.v2.u32 	{%r350, %r351}, [%rd52];
	or.b32  	%r353, %r350, %r351;
	setp.eq.s32	%p9, %r353, 0;
	@%p9 bra 	BB5_11;

	and.b32  	%r354, %r350, 131040;
	and.b32  	%r355, %r350, 31;
	mov.u32 	%r356, 1;
	shl.b32 	%r357, %r356, %r355;
	shr.u32 	%r358, %r354, 3;
	add.s32 	%r360, %r302, %r358;
	atom.shared.or.b32 	%r361, [%r360], %r357;

BB5_11:
	add.s32 	%r1802, %r1802, 1;

BB5_12:
	shl.b32 	%r362, %r1802, 10;
	add.s32 	%r29, %r362, %r1;
	setp.ge.s32	%p10, %r29, %r5;
	@%p10 bra 	BB5_15;

	add.s32 	%r363, %r29, %r18;
	mul.wide.s32 	%rd53, %r363, 8;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.v2.u32 	{%r364, %r365}, [%rd54];
	or.b32  	%r367, %r364, %r365;
	setp.eq.s32	%p11, %r367, 0;
	@%p11 bra 	BB5_15;

	and.b32  	%r368, %r364, 131040;
	and.b32  	%r369, %r364, 31;
	mov.u32 	%r370, 1;
	shl.b32 	%r371, %r370, %r369;
	shr.u32 	%r372, %r368, 3;
	add.s32 	%r374, %r302, %r372;
	atom.shared.or.b32 	%r375, [%r374], %r371;

BB5_15:
	add.s32 	%r1802, %r1802, 1;

BB5_16:
	setp.lt.u32	%p12, %r22, 4;
	@%p12 bra 	BB5_31;

	mad.lo.s32 	%r1805, %r1802, 1024, %r1;

BB5_18:
	setp.ge.s32	%p13, %r1805, %r5;
	@%p13 bra 	BB5_21;

	add.s32 	%r376, %r1805, %r18;
	mul.wide.s32 	%rd55, %r376, 8;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.v2.u32 	{%r377, %r378}, [%rd56];
	or.b32  	%r380, %r377, %r378;
	setp.eq.s32	%p14, %r380, 0;
	@%p14 bra 	BB5_21;

	and.b32  	%r381, %r377, 131040;
	and.b32  	%r382, %r377, 31;
	mov.u32 	%r383, 1;
	shl.b32 	%r384, %r383, %r382;
	shr.u32 	%r385, %r381, 3;
	add.s32 	%r387, %r302, %r385;
	atom.shared.or.b32 	%r388, [%r387], %r384;

BB5_21:
	add.s32 	%r37, %r1805, 1024;
	setp.ge.s32	%p15, %r37, %r5;
	@%p15 bra 	BB5_24;

	add.s32 	%r389, %r37, %r18;
	mul.wide.s32 	%rd57, %r389, 8;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.v2.u32 	{%r390, %r391}, [%rd58];
	or.b32  	%r393, %r390, %r391;
	setp.eq.s32	%p16, %r393, 0;
	@%p16 bra 	BB5_24;

	and.b32  	%r394, %r390, 131040;
	and.b32  	%r395, %r390, 31;
	mov.u32 	%r396, 1;
	shl.b32 	%r397, %r396, %r395;
	shr.u32 	%r398, %r394, 3;
	add.s32 	%r400, %r302, %r398;
	atom.shared.or.b32 	%r401, [%r400], %r397;

BB5_24:
	add.s32 	%r39, %r1805, 2048;
	setp.ge.s32	%p17, %r39, %r5;
	@%p17 bra 	BB5_27;

	add.s32 	%r402, %r39, %r18;
	mul.wide.s32 	%rd59, %r402, 8;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.v2.u32 	{%r403, %r404}, [%rd60];
	or.b32  	%r406, %r403, %r404;
	setp.eq.s32	%p18, %r406, 0;
	@%p18 bra 	BB5_27;

	and.b32  	%r407, %r403, 131040;
	and.b32  	%r408, %r403, 31;
	mov.u32 	%r409, 1;
	shl.b32 	%r410, %r409, %r408;
	shr.u32 	%r411, %r407, 3;
	add.s32 	%r413, %r302, %r411;
	atom.shared.or.b32 	%r414, [%r413], %r410;

BB5_27:
	add.s32 	%r41, %r1805, 3072;
	setp.ge.s32	%p19, %r41, %r5;
	@%p19 bra 	BB5_30;

	add.s32 	%r415, %r41, %r18;
	mul.wide.s32 	%rd61, %r415, 8;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.v2.u32 	{%r416, %r417}, [%rd62];
	or.b32  	%r419, %r416, %r417;
	setp.eq.s32	%p20, %r419, 0;
	@%p20 bra 	BB5_30;

	and.b32  	%r420, %r416, 131040;
	and.b32  	%r421, %r416, 31;
	mov.u32 	%r422, 1;
	shl.b32 	%r423, %r422, %r421;
	shr.u32 	%r424, %r420, 3;
	add.s32 	%r426, %r302, %r424;
	atom.shared.or.b32 	%r427, [%r426], %r423;

BB5_30:
	add.s32 	%r1802, %r1802, 4;
	add.s32 	%r1805, %r1805, 4096;
	setp.lt.s32	%p21, %r1802, %r7;
	@%p21 bra 	BB5_18;

BB5_31:
	setp.lt.s32	%p22, %r9, 1024;
	@%p22 bra 	BB5_62;

	mov.u32 	%r432, 1;
	max.s32 	%r45, %r10, %r432;
	and.b32  	%r431, %r45, 3;
	mov.u32 	%r1807, 0;
	setp.eq.s32	%p23, %r431, 0;
	@%p23 bra 	BB5_47;

	setp.eq.s32	%p24, %r431, 1;
	@%p24 bra 	BB5_43;

	setp.eq.s32	%p25, %r431, 2;
	@%p25 bra 	BB5_39;

	setp.ge.s32	%p26, %r1, %r8;
	@%p26 bra 	BB5_36;

	add.s32 	%r435, %r1, %r19;
	mul.wide.s32 	%rd63, %r435, 8;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.v2.u32 	{%r436, %r437}, [%rd64];
	or.b32  	%r439, %r436, %r437;
	setp.eq.s32	%p27, %r439, 0;
	mov.u32 	%r1807, %r432;
	@%p27 bra 	BB5_39;

	and.b32  	%r441, %r436, 131040;
	and.b32  	%r442, %r436, 31;
	mov.u32 	%r1807, 1;
	shl.b32 	%r443, %r1807, %r442;
	shr.u32 	%r444, %r441, 3;
	add.s32 	%r446, %r302, %r444;
	atom.shared.or.b32 	%r447, [%r446], %r443;
	bra.uni 	BB5_39;

BB5_36:
	mov.u32 	%r1807, %r432;

BB5_39:
	shl.b32 	%r448, %r1807, 10;
	add.s32 	%r48, %r448, %r1;
	setp.ge.s32	%p28, %r48, %r8;
	@%p28 bra 	BB5_42;

	add.s32 	%r449, %r48, %r19;
	mul.wide.s32 	%rd65, %r449, 8;
	add.s64 	%rd66, %rd2, %rd65;
	ld.global.v2.u32 	{%r450, %r451}, [%rd66];
	or.b32  	%r453, %r450, %r451;
	setp.eq.s32	%p29, %r453, 0;
	@%p29 bra 	BB5_42;

	and.b32  	%r454, %r450, 131040;
	and.b32  	%r455, %r450, 31;
	mov.u32 	%r456, 1;
	shl.b32 	%r457, %r456, %r455;
	shr.u32 	%r458, %r454, 3;
	add.s32 	%r460, %r302, %r458;
	atom.shared.or.b32 	%r461, [%r460], %r457;

BB5_42:
	add.s32 	%r1807, %r1807, 1;

BB5_43:
	shl.b32 	%r462, %r1807, 10;
	add.s32 	%r52, %r462, %r1;
	setp.ge.s32	%p30, %r52, %r8;
	@%p30 bra 	BB5_46;

	add.s32 	%r463, %r52, %r19;
	mul.wide.s32 	%rd67, %r463, 8;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.v2.u32 	{%r464, %r465}, [%rd68];
	or.b32  	%r467, %r464, %r465;
	setp.eq.s32	%p31, %r467, 0;
	@%p31 bra 	BB5_46;

	and.b32  	%r468, %r464, 131040;
	and.b32  	%r469, %r464, 31;
	mov.u32 	%r470, 1;
	shl.b32 	%r471, %r470, %r469;
	shr.u32 	%r472, %r468, 3;
	add.s32 	%r474, %r302, %r472;
	atom.shared.or.b32 	%r475, [%r474], %r471;

BB5_46:
	add.s32 	%r1807, %r1807, 1;

BB5_47:
	setp.lt.u32	%p32, %r45, 4;
	@%p32 bra 	BB5_62;

	mad.lo.s32 	%r1810, %r1807, 1024, %r1;

BB5_49:
	setp.ge.s32	%p33, %r1810, %r8;
	@%p33 bra 	BB5_52;

	add.s32 	%r476, %r1810, %r19;
	mul.wide.s32 	%rd69, %r476, 8;
	add.s64 	%rd70, %rd2, %rd69;
	ld.global.v2.u32 	{%r477, %r478}, [%rd70];
	or.b32  	%r480, %r477, %r478;
	setp.eq.s32	%p34, %r480, 0;
	@%p34 bra 	BB5_52;

	and.b32  	%r481, %r477, 131040;
	and.b32  	%r482, %r477, 31;
	mov.u32 	%r483, 1;
	shl.b32 	%r484, %r483, %r482;
	shr.u32 	%r485, %r481, 3;
	add.s32 	%r487, %r302, %r485;
	atom.shared.or.b32 	%r488, [%r487], %r484;

BB5_52:
	add.s32 	%r60, %r1810, 1024;
	setp.ge.s32	%p35, %r60, %r8;
	@%p35 bra 	BB5_55;

	add.s32 	%r489, %r60, %r19;
	mul.wide.s32 	%rd71, %r489, 8;
	add.s64 	%rd72, %rd2, %rd71;
	ld.global.v2.u32 	{%r490, %r491}, [%rd72];
	or.b32  	%r493, %r490, %r491;
	setp.eq.s32	%p36, %r493, 0;
	@%p36 bra 	BB5_55;

	and.b32  	%r494, %r490, 131040;
	and.b32  	%r495, %r490, 31;
	mov.u32 	%r496, 1;
	shl.b32 	%r497, %r496, %r495;
	shr.u32 	%r498, %r494, 3;
	add.s32 	%r500, %r302, %r498;
	atom.shared.or.b32 	%r501, [%r500], %r497;

BB5_55:
	add.s32 	%r62, %r1810, 2048;
	setp.ge.s32	%p37, %r62, %r8;
	@%p37 bra 	BB5_58;

	add.s32 	%r502, %r62, %r19;
	mul.wide.s32 	%rd73, %r502, 8;
	add.s64 	%rd74, %rd2, %rd73;
	ld.global.v2.u32 	{%r503, %r504}, [%rd74];
	or.b32  	%r506, %r503, %r504;
	setp.eq.s32	%p38, %r506, 0;
	@%p38 bra 	BB5_58;

	and.b32  	%r507, %r503, 131040;
	and.b32  	%r508, %r503, 31;
	mov.u32 	%r509, 1;
	shl.b32 	%r510, %r509, %r508;
	shr.u32 	%r511, %r507, 3;
	add.s32 	%r513, %r302, %r511;
	atom.shared.or.b32 	%r514, [%r513], %r510;

BB5_58:
	add.s32 	%r64, %r1810, 3072;
	setp.ge.s32	%p39, %r64, %r8;
	@%p39 bra 	BB5_61;

	add.s32 	%r515, %r64, %r19;
	mul.wide.s32 	%rd75, %r515, 8;
	add.s64 	%rd76, %rd2, %rd75;
	ld.global.v2.u32 	{%r516, %r517}, [%rd76];
	or.b32  	%r519, %r516, %r517;
	setp.eq.s32	%p40, %r519, 0;
	@%p40 bra 	BB5_61;

	and.b32  	%r520, %r516, 131040;
	and.b32  	%r521, %r516, 31;
	mov.u32 	%r522, 1;
	shl.b32 	%r523, %r522, %r521;
	shr.u32 	%r524, %r520, 3;
	add.s32 	%r526, %r302, %r524;
	atom.shared.or.b32 	%r527, [%r526], %r523;

BB5_61:
	add.s32 	%r1807, %r1807, 4;
	add.s32 	%r1810, %r1810, 4096;
	setp.lt.s32	%p41, %r1807, %r10;
	@%p41 bra 	BB5_49;

BB5_62:
	setp.lt.s32	%p42, %r12, 1024;
	@%p42 bra 	BB5_93;

	mov.u32 	%r532, 1;
	max.s32 	%r68, %r13, %r532;
	and.b32  	%r531, %r68, 3;
	mov.u32 	%r1812, 0;
	setp.eq.s32	%p43, %r531, 0;
	@%p43 bra 	BB5_78;

	setp.eq.s32	%p44, %r531, 1;
	@%p44 bra 	BB5_74;

	setp.eq.s32	%p45, %r531, 2;
	@%p45 bra 	BB5_70;

	setp.ge.s32	%p46, %r1, %r11;
	@%p46 bra 	BB5_67;

	add.s32 	%r535, %r1, %r20;
	mul.wide.s32 	%rd77, %r535, 8;
	add.s64 	%rd78, %rd2, %rd77;
	ld.global.v2.u32 	{%r536, %r537}, [%rd78];
	or.b32  	%r539, %r536, %r537;
	setp.eq.s32	%p47, %r539, 0;
	mov.u32 	%r1812, %r532;
	@%p47 bra 	BB5_70;

	and.b32  	%r541, %r536, 131040;
	and.b32  	%r542, %r536, 31;
	mov.u32 	%r1812, 1;
	shl.b32 	%r543, %r1812, %r542;
	shr.u32 	%r544, %r541, 3;
	add.s32 	%r546, %r302, %r544;
	atom.shared.or.b32 	%r547, [%r546], %r543;
	bra.uni 	BB5_70;

BB5_67:
	mov.u32 	%r1812, %r532;

BB5_70:
	shl.b32 	%r548, %r1812, 10;
	add.s32 	%r71, %r548, %r1;
	setp.ge.s32	%p48, %r71, %r11;
	@%p48 bra 	BB5_73;

	add.s32 	%r549, %r71, %r20;
	mul.wide.s32 	%rd79, %r549, 8;
	add.s64 	%rd80, %rd2, %rd79;
	ld.global.v2.u32 	{%r550, %r551}, [%rd80];
	or.b32  	%r553, %r550, %r551;
	setp.eq.s32	%p49, %r553, 0;
	@%p49 bra 	BB5_73;

	and.b32  	%r554, %r550, 131040;
	and.b32  	%r555, %r550, 31;
	mov.u32 	%r556, 1;
	shl.b32 	%r557, %r556, %r555;
	shr.u32 	%r558, %r554, 3;
	add.s32 	%r560, %r302, %r558;
	atom.shared.or.b32 	%r561, [%r560], %r557;

BB5_73:
	add.s32 	%r1812, %r1812, 1;

BB5_74:
	shl.b32 	%r562, %r1812, 10;
	add.s32 	%r75, %r562, %r1;
	setp.ge.s32	%p50, %r75, %r11;
	@%p50 bra 	BB5_77;

	add.s32 	%r563, %r75, %r20;
	mul.wide.s32 	%rd81, %r563, 8;
	add.s64 	%rd82, %rd2, %rd81;
	ld.global.v2.u32 	{%r564, %r565}, [%rd82];
	or.b32  	%r567, %r564, %r565;
	setp.eq.s32	%p51, %r567, 0;
	@%p51 bra 	BB5_77;

	and.b32  	%r568, %r564, 131040;
	and.b32  	%r569, %r564, 31;
	mov.u32 	%r570, 1;
	shl.b32 	%r571, %r570, %r569;
	shr.u32 	%r572, %r568, 3;
	add.s32 	%r574, %r302, %r572;
	atom.shared.or.b32 	%r575, [%r574], %r571;

BB5_77:
	add.s32 	%r1812, %r1812, 1;

BB5_78:
	setp.lt.u32	%p52, %r68, 4;
	@%p52 bra 	BB5_93;

	mad.lo.s32 	%r1815, %r1812, 1024, %r1;

BB5_80:
	setp.ge.s32	%p53, %r1815, %r11;
	@%p53 bra 	BB5_83;

	add.s32 	%r576, %r1815, %r20;
	mul.wide.s32 	%rd83, %r576, 8;
	add.s64 	%rd84, %rd2, %rd83;
	ld.global.v2.u32 	{%r577, %r578}, [%rd84];
	or.b32  	%r580, %r577, %r578;
	setp.eq.s32	%p54, %r580, 0;
	@%p54 bra 	BB5_83;

	and.b32  	%r581, %r577, 131040;
	and.b32  	%r582, %r577, 31;
	mov.u32 	%r583, 1;
	shl.b32 	%r584, %r583, %r582;
	shr.u32 	%r585, %r581, 3;
	add.s32 	%r587, %r302, %r585;
	atom.shared.or.b32 	%r588, [%r587], %r584;

BB5_83:
	add.s32 	%r83, %r1815, 1024;
	setp.ge.s32	%p55, %r83, %r11;
	@%p55 bra 	BB5_86;

	add.s32 	%r589, %r83, %r20;
	mul.wide.s32 	%rd85, %r589, 8;
	add.s64 	%rd86, %rd2, %rd85;
	ld.global.v2.u32 	{%r590, %r591}, [%rd86];
	or.b32  	%r593, %r590, %r591;
	setp.eq.s32	%p56, %r593, 0;
	@%p56 bra 	BB5_86;

	and.b32  	%r594, %r590, 131040;
	and.b32  	%r595, %r590, 31;
	mov.u32 	%r596, 1;
	shl.b32 	%r597, %r596, %r595;
	shr.u32 	%r598, %r594, 3;
	add.s32 	%r600, %r302, %r598;
	atom.shared.or.b32 	%r601, [%r600], %r597;

BB5_86:
	add.s32 	%r85, %r1815, 2048;
	setp.ge.s32	%p57, %r85, %r11;
	@%p57 bra 	BB5_89;

	add.s32 	%r602, %r85, %r20;
	mul.wide.s32 	%rd87, %r602, 8;
	add.s64 	%rd88, %rd2, %rd87;
	ld.global.v2.u32 	{%r603, %r604}, [%rd88];
	or.b32  	%r606, %r603, %r604;
	setp.eq.s32	%p58, %r606, 0;
	@%p58 bra 	BB5_89;

	and.b32  	%r607, %r603, 131040;
	and.b32  	%r608, %r603, 31;
	mov.u32 	%r609, 1;
	shl.b32 	%r610, %r609, %r608;
	shr.u32 	%r611, %r607, 3;
	add.s32 	%r613, %r302, %r611;
	atom.shared.or.b32 	%r614, [%r613], %r610;

BB5_89:
	add.s32 	%r87, %r1815, 3072;
	setp.ge.s32	%p59, %r87, %r11;
	@%p59 bra 	BB5_92;

	add.s32 	%r615, %r87, %r20;
	mul.wide.s32 	%rd89, %r615, 8;
	add.s64 	%rd90, %rd2, %rd89;
	ld.global.v2.u32 	{%r616, %r617}, [%rd90];
	or.b32  	%r619, %r616, %r617;
	setp.eq.s32	%p60, %r619, 0;
	@%p60 bra 	BB5_92;

	and.b32  	%r620, %r616, 131040;
	and.b32  	%r621, %r616, 31;
	mov.u32 	%r622, 1;
	shl.b32 	%r623, %r622, %r621;
	shr.u32 	%r624, %r620, 3;
	add.s32 	%r626, %r302, %r624;
	atom.shared.or.b32 	%r627, [%r626], %r623;

BB5_92:
	add.s32 	%r1812, %r1812, 4;
	add.s32 	%r1815, %r1815, 4096;
	setp.lt.s32	%p61, %r1812, %r13;
	@%p61 bra 	BB5_80;

BB5_93:
	setp.lt.s32	%p62, %r15, 1024;
	@%p62 bra 	BB5_124;

	mov.u32 	%r632, 1;
	max.s32 	%r91, %r16, %r632;
	and.b32  	%r631, %r91, 3;
	mov.u32 	%r1817, 0;
	setp.eq.s32	%p63, %r631, 0;
	@%p63 bra 	BB5_109;

	setp.eq.s32	%p64, %r631, 1;
	@%p64 bra 	BB5_105;

	setp.eq.s32	%p65, %r631, 2;
	@%p65 bra 	BB5_101;

	setp.ge.s32	%p66, %r1, %r14;
	@%p66 bra 	BB5_98;

	add.s32 	%r635, %r1, %r21;
	mul.wide.s32 	%rd91, %r635, 8;
	add.s64 	%rd92, %rd2, %rd91;
	ld.global.v2.u32 	{%r636, %r637}, [%rd92];
	or.b32  	%r639, %r636, %r637;
	setp.eq.s32	%p67, %r639, 0;
	mov.u32 	%r1817, %r632;
	@%p67 bra 	BB5_101;

	and.b32  	%r641, %r636, 131040;
	and.b32  	%r642, %r636, 31;
	mov.u32 	%r1817, 1;
	shl.b32 	%r643, %r1817, %r642;
	shr.u32 	%r644, %r641, 3;
	add.s32 	%r646, %r302, %r644;
	atom.shared.or.b32 	%r647, [%r646], %r643;
	bra.uni 	BB5_101;

BB5_98:
	mov.u32 	%r1817, %r632;

BB5_101:
	shl.b32 	%r648, %r1817, 10;
	add.s32 	%r94, %r648, %r1;
	setp.ge.s32	%p68, %r94, %r14;
	@%p68 bra 	BB5_104;

	add.s32 	%r649, %r94, %r21;
	mul.wide.s32 	%rd93, %r649, 8;
	add.s64 	%rd94, %rd2, %rd93;
	ld.global.v2.u32 	{%r650, %r651}, [%rd94];
	or.b32  	%r653, %r650, %r651;
	setp.eq.s32	%p69, %r653, 0;
	@%p69 bra 	BB5_104;

	and.b32  	%r654, %r650, 131040;
	and.b32  	%r655, %r650, 31;
	mov.u32 	%r656, 1;
	shl.b32 	%r657, %r656, %r655;
	shr.u32 	%r658, %r654, 3;
	add.s32 	%r660, %r302, %r658;
	atom.shared.or.b32 	%r661, [%r660], %r657;

BB5_104:
	add.s32 	%r1817, %r1817, 1;

BB5_105:
	shl.b32 	%r662, %r1817, 10;
	add.s32 	%r98, %r662, %r1;
	setp.ge.s32	%p70, %r98, %r14;
	@%p70 bra 	BB5_108;

	add.s32 	%r663, %r98, %r21;
	mul.wide.s32 	%rd95, %r663, 8;
	add.s64 	%rd96, %rd2, %rd95;
	ld.global.v2.u32 	{%r664, %r665}, [%rd96];
	or.b32  	%r667, %r664, %r665;
	setp.eq.s32	%p71, %r667, 0;
	@%p71 bra 	BB5_108;

	and.b32  	%r668, %r664, 131040;
	and.b32  	%r669, %r664, 31;
	mov.u32 	%r670, 1;
	shl.b32 	%r671, %r670, %r669;
	shr.u32 	%r672, %r668, 3;
	add.s32 	%r674, %r302, %r672;
	atom.shared.or.b32 	%r675, [%r674], %r671;

BB5_108:
	add.s32 	%r1817, %r1817, 1;

BB5_109:
	setp.lt.u32	%p72, %r91, 4;
	@%p72 bra 	BB5_124;

	mad.lo.s32 	%r1820, %r1817, 1024, %r1;

BB5_111:
	setp.ge.s32	%p73, %r1820, %r14;
	@%p73 bra 	BB5_114;

	add.s32 	%r676, %r1820, %r21;
	mul.wide.s32 	%rd97, %r676, 8;
	add.s64 	%rd98, %rd2, %rd97;
	ld.global.v2.u32 	{%r677, %r678}, [%rd98];
	or.b32  	%r680, %r677, %r678;
	setp.eq.s32	%p74, %r680, 0;
	@%p74 bra 	BB5_114;

	and.b32  	%r681, %r677, 131040;
	and.b32  	%r682, %r677, 31;
	mov.u32 	%r683, 1;
	shl.b32 	%r684, %r683, %r682;
	shr.u32 	%r685, %r681, 3;
	add.s32 	%r687, %r302, %r685;
	atom.shared.or.b32 	%r688, [%r687], %r684;

BB5_114:
	add.s32 	%r106, %r1820, 1024;
	setp.ge.s32	%p75, %r106, %r14;
	@%p75 bra 	BB5_117;

	add.s32 	%r689, %r106, %r21;
	mul.wide.s32 	%rd99, %r689, 8;
	add.s64 	%rd100, %rd2, %rd99;
	ld.global.v2.u32 	{%r690, %r691}, [%rd100];
	or.b32  	%r693, %r690, %r691;
	setp.eq.s32	%p76, %r693, 0;
	@%p76 bra 	BB5_117;

	and.b32  	%r694, %r690, 131040;
	and.b32  	%r695, %r690, 31;
	mov.u32 	%r696, 1;
	shl.b32 	%r697, %r696, %r695;
	shr.u32 	%r698, %r694, 3;
	add.s32 	%r700, %r302, %r698;
	atom.shared.or.b32 	%r701, [%r700], %r697;

BB5_117:
	add.s32 	%r108, %r1820, 2048;
	setp.ge.s32	%p77, %r108, %r14;
	@%p77 bra 	BB5_120;

	add.s32 	%r702, %r108, %r21;
	mul.wide.s32 	%rd101, %r702, 8;
	add.s64 	%rd102, %rd2, %rd101;
	ld.global.v2.u32 	{%r703, %r704}, [%rd102];
	or.b32  	%r706, %r703, %r704;
	setp.eq.s32	%p78, %r706, 0;
	@%p78 bra 	BB5_120;

	and.b32  	%r707, %r703, 131040;
	and.b32  	%r708, %r703, 31;
	mov.u32 	%r709, 1;
	shl.b32 	%r710, %r709, %r708;
	shr.u32 	%r711, %r707, 3;
	add.s32 	%r713, %r302, %r711;
	atom.shared.or.b32 	%r714, [%r713], %r710;

BB5_120:
	add.s32 	%r110, %r1820, 3072;
	setp.ge.s32	%p79, %r110, %r14;
	@%p79 bra 	BB5_123;

	add.s32 	%r715, %r110, %r21;
	mul.wide.s32 	%rd103, %r715, 8;
	add.s64 	%rd104, %rd2, %rd103;
	ld.global.v2.u32 	{%r716, %r717}, [%rd104];
	or.b32  	%r719, %r716, %r717;
	setp.eq.s32	%p80, %r719, 0;
	@%p80 bra 	BB5_123;

	and.b32  	%r720, %r716, 131040;
	and.b32  	%r721, %r716, 31;
	mov.u32 	%r722, 1;
	shl.b32 	%r723, %r722, %r721;
	shr.u32 	%r724, %r720, 3;
	add.s32 	%r726, %r302, %r724;
	atom.shared.or.b32 	%r727, [%r726], %r723;

BB5_123:
	add.s32 	%r1817, %r1817, 4;
	add.s32 	%r1820, %r1820, 4096;
	setp.lt.s32	%p81, %r1817, %r16;
	@%p81 bra 	BB5_111;

BB5_124:
	setp.gt.s32	%p1, %r6, 1023;
	bar.sync 	0;
	shl.b32 	%r728, %r1, 3;
	add.s32 	%r730, %r302, 16384;
	add.s32 	%r114, %r730, %r728;
	mov.u64 	%rd105, 0;
	st.shared.u64 	[%r114], %rd105;
	shl.b32 	%r731, %r2, 3;
	add.s32 	%r115, %r730, %r731;
	st.shared.u64 	[%r115], %rd105;
	shl.b32 	%r732, %r3, 3;
	add.s32 	%r116, %r730, %r732;
	st.shared.u64 	[%r116], %rd105;
	shl.b32 	%r733, %r4, 3;
	add.s32 	%r117, %r730, %r733;
	st.shared.u64 	[%r117], %rd105;
	bar.sync 	0;
	@!%p1 bra 	BB5_183;
	bra.uni 	BB5_125;

BB5_125:
	add.s32 	%r118, %r299, -4;
	mov.u32 	%r738, 1;
	max.s32 	%r119, %r7, %r738;
	and.b32  	%r737, %r119, 3;
	mov.u32 	%r1822, 0;
	setp.eq.s32	%p82, %r737, 0;
	@%p82 bra 	BB5_152;

	setp.eq.s32	%p83, %r737, 1;
	@%p83 bra 	BB5_144;

	setp.eq.s32	%p84, %r737, 2;
	@%p84 bra 	BB5_136;

	setp.ge.s32	%p85, %r1, %r5;
	@%p85 bra 	BB5_129;

	add.s32 	%r741, %r1, %r18;
	mul.wide.s32 	%rd106, %r741, 8;
	add.s64 	%rd107, %rd2, %rd106;
	ld.global.v2.u32 	{%r742, %r743}, [%rd107];
	or.b32  	%r744, %r742, %r743;
	setp.eq.s32	%p86, %r744, 0;
	mov.u32 	%r1822, %r738;
	@%p86 bra 	BB5_136;

	and.b32  	%r746, %r742, 131040;
	and.b32  	%r747, %r742, 31;
	xor.b32  	%r748, %r747, 1;
	shr.u32 	%r749, %r746, 3;
	add.s32 	%r751, %r302, %r749;
	mov.u32 	%r1822, 1;
	shl.b32 	%r752, %r1822, %r748;
	ld.shared.u32 	%r753, [%r751];
	and.b32  	%r754, %r753, %r752;
	setp.eq.s32	%p87, %r754, 0;
	@%p87 bra 	BB5_136;

	bfe.u32 	%r122, %r743, 17, 12;
	shl.b32 	%r755, %r122, 3;
	add.s32 	%r757, %r302, %r755;
	add.s32 	%r123, %r757, 16384;
	atom.shared.exch.b64 	%rd4, [%r123], 0;
	setp.eq.s64	%p88, %rd4, 0;
	@%p88 bra 	BB5_134;

	add.s32 	%r759, %r122, %r17;
	mul.wide.s32 	%rd108, %r759, 4;
	add.s64 	%rd109, %rd3, %rd108;
	atom.global.add.u32 	%r760, [%rd109], 2;
	min.s32 	%r761, %r760, %r118;
	mad.lo.s32 	%r762, %r122, %r299, %r761;
	shr.u32 	%r763, %r762, 31;
	add.s32 	%r764, %r762, %r763;
	shr.s32 	%r765, %r764, 1;
	shr.u64 	%rd110, %rd4, 32;
	mul.wide.s32 	%rd111, %r765, 16;
	add.s64 	%rd112, %rd1, %rd111;
	cvt.u32.u64	%r766, %rd4;
	cvt.u32.u64	%r767, %rd110;
	st.global.v4.u32 	[%rd112], {%r743, %r742, %r767, %r766};
	bra.uni 	BB5_136;

BB5_129:
	mov.u32 	%r1822, %r738;

BB5_136:
	shl.b32 	%r778, %r1822, 10;
	add.s32 	%r125, %r778, %r1;
	setp.ge.s32	%p90, %r125, %r5;
	@%p90 bra 	BB5_143;

	add.s32 	%r779, %r125, %r18;
	mul.wide.s32 	%rd122, %r779, 8;
	add.s64 	%rd123, %rd2, %rd122;
	ld.global.v2.u32 	{%r780, %r781}, [%rd123];
	or.b32  	%r782, %r780, %r781;
	setp.eq.s32	%p91, %r782, 0;
	@%p91 bra 	BB5_143;

	and.b32  	%r783, %r780, 131040;
	and.b32  	%r784, %r780, 31;
	xor.b32  	%r785, %r784, 1;
	shr.u32 	%r786, %r783, 3;
	add.s32 	%r788, %r302, %r786;
	mov.u32 	%r789, 1;
	shl.b32 	%r790, %r789, %r785;
	ld.shared.u32 	%r791, [%r788];
	and.b32  	%r792, %r791, %r790;
	setp.eq.s32	%p92, %r792, 0;
	@%p92 bra 	BB5_143;

	bfe.u32 	%r128, %r781, 17, 12;
	shl.b32 	%r793, %r128, 3;
	add.s32 	%r795, %r302, %r793;
	add.s32 	%r129, %r795, 16384;
	atom.shared.exch.b64 	%rd5, [%r129], 0;
	setp.eq.s64	%p93, %rd5, 0;
	@%p93 bra 	BB5_141;

	add.s32 	%r796, %r128, %r17;
	mul.wide.s32 	%rd124, %r796, 4;
	add.s64 	%rd125, %rd3, %rd124;
	atom.global.add.u32 	%r797, [%rd125], 2;
	min.s32 	%r798, %r797, %r118;
	mad.lo.s32 	%r799, %r128, %r299, %r798;
	shr.u32 	%r800, %r799, 31;
	add.s32 	%r801, %r799, %r800;
	shr.s32 	%r802, %r801, 1;
	shr.u64 	%rd126, %rd5, 32;
	mul.wide.s32 	%rd127, %r802, 16;
	add.s64 	%rd128, %rd1, %rd127;
	cvt.u32.u64	%r803, %rd5;
	cvt.u32.u64	%r804, %rd126;
	st.global.v4.u32 	[%rd128], {%r781, %r780, %r804, %r803};
	bra.uni 	BB5_143;

BB5_141:
	cvt.u64.u32	%rd129, %r781;
	cvt.u64.u32	%rd130, %r780;
	bfi.b64 	%rd131, %rd129, %rd130, 32, 32;
	add.s32 	%r1775, %r795, 16384;
	atom.shared.cas.b64 	%rd133, [%r1775], %rd105, %rd131;
	setp.eq.s64	%p94, %rd133, 0;
	@%p94 bra 	BB5_143;

	add.s32 	%r805, %r128, %r17;
	mul.wide.s32 	%rd134, %r805, 4;
	add.s64 	%rd135, %rd3, %rd134;
	atom.global.add.u32 	%r806, [%rd135], 2;
	min.s32 	%r807, %r806, %r118;
	mad.lo.s32 	%r808, %r128, %r299, %r807;
	shr.u32 	%r809, %r808, 31;
	add.s32 	%r810, %r808, %r809;
	shr.s32 	%r811, %r810, 1;
	mul.wide.s32 	%rd136, %r811, 16;
	add.s64 	%rd137, %rd1, %rd136;
	mov.u32 	%r812, 0;
	st.global.v4.u32 	[%rd137], {%r781, %r780, %r812, %r812};

BB5_143:
	add.s32 	%r1822, %r1822, 1;

BB5_144:
	shl.b32 	%r813, %r1822, 10;
	add.s32 	%r132, %r813, %r1;
	setp.ge.s32	%p95, %r132, %r5;
	@%p95 bra 	BB5_151;

	add.s32 	%r814, %r132, %r18;
	mul.wide.s32 	%rd138, %r814, 8;
	add.s64 	%rd139, %rd2, %rd138;
	ld.global.v2.u32 	{%r815, %r816}, [%rd139];
	or.b32  	%r817, %r815, %r816;
	setp.eq.s32	%p96, %r817, 0;
	@%p96 bra 	BB5_151;

	and.b32  	%r818, %r815, 131040;
	and.b32  	%r819, %r815, 31;
	xor.b32  	%r820, %r819, 1;
	shr.u32 	%r821, %r818, 3;
	add.s32 	%r823, %r302, %r821;
	mov.u32 	%r824, 1;
	shl.b32 	%r825, %r824, %r820;
	ld.shared.u32 	%r826, [%r823];
	and.b32  	%r827, %r826, %r825;
	setp.eq.s32	%p97, %r827, 0;
	@%p97 bra 	BB5_151;

	bfe.u32 	%r135, %r816, 17, 12;
	shl.b32 	%r828, %r135, 3;
	add.s32 	%r830, %r302, %r828;
	add.s32 	%r136, %r830, 16384;
	atom.shared.exch.b64 	%rd6, [%r136], 0;
	setp.eq.s64	%p98, %rd6, 0;
	@%p98 bra 	BB5_149;

	add.s32 	%r831, %r135, %r17;
	mul.wide.s32 	%rd140, %r831, 4;
	add.s64 	%rd141, %rd3, %rd140;
	atom.global.add.u32 	%r832, [%rd141], 2;
	min.s32 	%r833, %r832, %r118;
	mad.lo.s32 	%r834, %r135, %r299, %r833;
	shr.u32 	%r835, %r834, 31;
	add.s32 	%r836, %r834, %r835;
	shr.s32 	%r837, %r836, 1;
	shr.u64 	%rd142, %rd6, 32;
	mul.wide.s32 	%rd143, %r837, 16;
	add.s64 	%rd144, %rd1, %rd143;
	cvt.u32.u64	%r838, %rd6;
	cvt.u32.u64	%r839, %rd142;
	st.global.v4.u32 	[%rd144], {%r816, %r815, %r839, %r838};
	bra.uni 	BB5_151;

BB5_149:
	cvt.u64.u32	%rd145, %r816;
	cvt.u64.u32	%rd146, %r815;
	bfi.b64 	%rd147, %rd145, %rd146, 32, 32;
	add.s32 	%r1776, %r830, 16384;
	atom.shared.cas.b64 	%rd149, [%r1776], %rd105, %rd147;
	setp.eq.s64	%p99, %rd149, 0;
	@%p99 bra 	BB5_151;

	add.s32 	%r840, %r135, %r17;
	mul.wide.s32 	%rd150, %r840, 4;
	add.s64 	%rd151, %rd3, %rd150;
	atom.global.add.u32 	%r841, [%rd151], 2;
	min.s32 	%r842, %r841, %r118;
	mad.lo.s32 	%r843, %r135, %r299, %r842;
	shr.u32 	%r844, %r843, 31;
	add.s32 	%r845, %r843, %r844;
	shr.s32 	%r846, %r845, 1;
	mul.wide.s32 	%rd152, %r846, 16;
	add.s64 	%rd153, %rd1, %rd152;
	mov.u32 	%r847, 0;
	st.global.v4.u32 	[%rd153], {%r816, %r815, %r847, %r847};

BB5_151:
	add.s32 	%r1822, %r1822, 1;

BB5_152:
	setp.lt.u32	%p100, %r119, 4;
	@%p100 bra 	BB5_183;

	mad.lo.s32 	%r1825, %r1822, 1024, %r1;

BB5_154:
	setp.ge.s32	%p101, %r1825, %r5;
	@%p101 bra 	BB5_161;

	add.s32 	%r848, %r1825, %r18;
	mul.wide.s32 	%rd154, %r848, 8;
	add.s64 	%rd155, %rd2, %rd154;
	ld.global.v2.u32 	{%r849, %r850}, [%rd155];
	or.b32  	%r851, %r849, %r850;
	setp.eq.s32	%p102, %r851, 0;
	@%p102 bra 	BB5_161;

	and.b32  	%r852, %r849, 131040;
	and.b32  	%r853, %r849, 31;
	xor.b32  	%r854, %r853, 1;
	shr.u32 	%r855, %r852, 3;
	add.s32 	%r857, %r302, %r855;
	mov.u32 	%r858, 1;
	shl.b32 	%r859, %r858, %r854;
	ld.shared.u32 	%r860, [%r857];
	and.b32  	%r861, %r860, %r859;
	setp.eq.s32	%p103, %r861, 0;
	@%p103 bra 	BB5_161;

	bfe.u32 	%r144, %r850, 17, 12;
	shl.b32 	%r862, %r144, 3;
	add.s32 	%r864, %r302, %r862;
	add.s32 	%r145, %r864, 16384;
	atom.shared.exch.b64 	%rd7, [%r145], 0;
	setp.eq.s64	%p104, %rd7, 0;
	@%p104 bra 	BB5_159;
	bra.uni 	BB5_158;

BB5_159:
	cvt.u64.u32	%rd161, %r850;
	cvt.u64.u32	%rd162, %r849;
	bfi.b64 	%rd163, %rd161, %rd162, 32, 32;
	add.s32 	%r1777, %r864, 16384;
	atom.shared.cas.b64 	%rd165, [%r1777], %rd105, %rd163;
	setp.eq.s64	%p105, %rd165, 0;
	@%p105 bra 	BB5_161;

	add.s32 	%r874, %r144, %r17;
	mul.wide.s32 	%rd166, %r874, 4;
	add.s64 	%rd167, %rd3, %rd166;
	atom.global.add.u32 	%r875, [%rd167], 2;
	min.s32 	%r876, %r875, %r118;
	mad.lo.s32 	%r877, %r144, %r299, %r876;
	shr.u32 	%r878, %r877, 31;
	add.s32 	%r879, %r877, %r878;
	shr.s32 	%r880, %r879, 1;
	mul.wide.s32 	%rd168, %r880, 16;
	add.s64 	%rd169, %rd1, %rd168;
	mov.u32 	%r881, 0;
	st.global.v4.u32 	[%rd169], {%r850, %r849, %r881, %r881};
	bra.uni 	BB5_161;

BB5_158:
	add.s32 	%r865, %r144, %r17;
	mul.wide.s32 	%rd156, %r865, 4;
	add.s64 	%rd157, %rd3, %rd156;
	atom.global.add.u32 	%r866, [%rd157], 2;
	min.s32 	%r867, %r866, %r118;
	mad.lo.s32 	%r868, %r144, %r299, %r867;
	shr.u32 	%r869, %r868, 31;
	add.s32 	%r870, %r868, %r869;
	shr.s32 	%r871, %r870, 1;
	shr.u64 	%rd158, %rd7, 32;
	mul.wide.s32 	%rd159, %r871, 16;
	add.s64 	%rd160, %rd1, %rd159;
	cvt.u32.u64	%r872, %rd7;
	cvt.u32.u64	%r873, %rd158;
	st.global.v4.u32 	[%rd160], {%r850, %r849, %r873, %r872};

BB5_161:
	add.s32 	%r146, %r1825, 1024;
	setp.ge.s32	%p106, %r146, %r5;
	@%p106 bra 	BB5_168;

	add.s32 	%r882, %r146, %r18;
	mul.wide.s32 	%rd170, %r882, 8;
	add.s64 	%rd171, %rd2, %rd170;
	ld.global.v2.u32 	{%r883, %r884}, [%rd171];
	or.b32  	%r885, %r883, %r884;
	setp.eq.s32	%p107, %r885, 0;
	@%p107 bra 	BB5_168;

	and.b32  	%r886, %r883, 131040;
	and.b32  	%r887, %r883, 31;
	xor.b32  	%r888, %r887, 1;
	shr.u32 	%r889, %r886, 3;
	add.s32 	%r891, %r302, %r889;
	mov.u32 	%r892, 1;
	shl.b32 	%r893, %r892, %r888;
	ld.shared.u32 	%r894, [%r891];
	and.b32  	%r895, %r894, %r893;
	setp.eq.s32	%p108, %r895, 0;
	@%p108 bra 	BB5_168;

	bfe.u32 	%r149, %r884, 17, 12;
	shl.b32 	%r896, %r149, 3;
	add.s32 	%r898, %r302, %r896;
	add.s32 	%r150, %r898, 16384;
	atom.shared.exch.b64 	%rd8, [%r150], 0;
	setp.eq.s64	%p109, %rd8, 0;
	@%p109 bra 	BB5_166;
	bra.uni 	BB5_165;

BB5_166:
	cvt.u64.u32	%rd177, %r884;
	cvt.u64.u32	%rd178, %r883;
	bfi.b64 	%rd179, %rd177, %rd178, 32, 32;
	add.s32 	%r1778, %r898, 16384;
	atom.shared.cas.b64 	%rd181, [%r1778], %rd105, %rd179;
	setp.eq.s64	%p110, %rd181, 0;
	@%p110 bra 	BB5_168;

	add.s32 	%r908, %r149, %r17;
	mul.wide.s32 	%rd182, %r908, 4;
	add.s64 	%rd183, %rd3, %rd182;
	atom.global.add.u32 	%r909, [%rd183], 2;
	min.s32 	%r910, %r909, %r118;
	mad.lo.s32 	%r911, %r149, %r299, %r910;
	shr.u32 	%r912, %r911, 31;
	add.s32 	%r913, %r911, %r912;
	shr.s32 	%r914, %r913, 1;
	mul.wide.s32 	%rd184, %r914, 16;
	add.s64 	%rd185, %rd1, %rd184;
	mov.u32 	%r915, 0;
	st.global.v4.u32 	[%rd185], {%r884, %r883, %r915, %r915};
	bra.uni 	BB5_168;

BB5_165:
	add.s32 	%r899, %r149, %r17;
	mul.wide.s32 	%rd172, %r899, 4;
	add.s64 	%rd173, %rd3, %rd172;
	atom.global.add.u32 	%r900, [%rd173], 2;
	min.s32 	%r901, %r900, %r118;
	mad.lo.s32 	%r902, %r149, %r299, %r901;
	shr.u32 	%r903, %r902, 31;
	add.s32 	%r904, %r902, %r903;
	shr.s32 	%r905, %r904, 1;
	shr.u64 	%rd174, %rd8, 32;
	mul.wide.s32 	%rd175, %r905, 16;
	add.s64 	%rd176, %rd1, %rd175;
	cvt.u32.u64	%r906, %rd8;
	cvt.u32.u64	%r907, %rd174;
	st.global.v4.u32 	[%rd176], {%r884, %r883, %r907, %r906};

BB5_168:
	add.s32 	%r151, %r1825, 2048;
	setp.ge.s32	%p111, %r151, %r5;
	@%p111 bra 	BB5_175;

	add.s32 	%r916, %r151, %r18;
	mul.wide.s32 	%rd186, %r916, 8;
	add.s64 	%rd187, %rd2, %rd186;
	ld.global.v2.u32 	{%r917, %r918}, [%rd187];
	or.b32  	%r919, %r917, %r918;
	setp.eq.s32	%p112, %r919, 0;
	@%p112 bra 	BB5_175;

	and.b32  	%r920, %r917, 131040;
	and.b32  	%r921, %r917, 31;
	xor.b32  	%r922, %r921, 1;
	shr.u32 	%r923, %r920, 3;
	add.s32 	%r925, %r302, %r923;
	mov.u32 	%r926, 1;
	shl.b32 	%r927, %r926, %r922;
	ld.shared.u32 	%r928, [%r925];
	and.b32  	%r929, %r928, %r927;
	setp.eq.s32	%p113, %r929, 0;
	@%p113 bra 	BB5_175;

	bfe.u32 	%r154, %r918, 17, 12;
	shl.b32 	%r930, %r154, 3;
	add.s32 	%r932, %r302, %r930;
	add.s32 	%r155, %r932, 16384;
	atom.shared.exch.b64 	%rd9, [%r155], 0;
	setp.eq.s64	%p114, %rd9, 0;
	@%p114 bra 	BB5_173;
	bra.uni 	BB5_172;

BB5_173:
	cvt.u64.u32	%rd193, %r918;
	cvt.u64.u32	%rd194, %r917;
	bfi.b64 	%rd195, %rd193, %rd194, 32, 32;
	add.s32 	%r1779, %r932, 16384;
	atom.shared.cas.b64 	%rd197, [%r1779], %rd105, %rd195;
	setp.eq.s64	%p115, %rd197, 0;
	@%p115 bra 	BB5_175;

	add.s32 	%r942, %r154, %r17;
	mul.wide.s32 	%rd198, %r942, 4;
	add.s64 	%rd199, %rd3, %rd198;
	atom.global.add.u32 	%r943, [%rd199], 2;
	min.s32 	%r944, %r943, %r118;
	mad.lo.s32 	%r945, %r154, %r299, %r944;
	shr.u32 	%r946, %r945, 31;
	add.s32 	%r947, %r945, %r946;
	shr.s32 	%r948, %r947, 1;
	mul.wide.s32 	%rd200, %r948, 16;
	add.s64 	%rd201, %rd1, %rd200;
	mov.u32 	%r949, 0;
	st.global.v4.u32 	[%rd201], {%r918, %r917, %r949, %r949};
	bra.uni 	BB5_175;

BB5_172:
	add.s32 	%r933, %r154, %r17;
	mul.wide.s32 	%rd188, %r933, 4;
	add.s64 	%rd189, %rd3, %rd188;
	atom.global.add.u32 	%r934, [%rd189], 2;
	min.s32 	%r935, %r934, %r118;
	mad.lo.s32 	%r936, %r154, %r299, %r935;
	shr.u32 	%r937, %r936, 31;
	add.s32 	%r938, %r936, %r937;
	shr.s32 	%r939, %r938, 1;
	shr.u64 	%rd190, %rd9, 32;
	mul.wide.s32 	%rd191, %r939, 16;
	add.s64 	%rd192, %rd1, %rd191;
	cvt.u32.u64	%r940, %rd9;
	cvt.u32.u64	%r941, %rd190;
	st.global.v4.u32 	[%rd192], {%r918, %r917, %r941, %r940};

BB5_175:
	add.s32 	%r156, %r1825, 3072;
	setp.ge.s32	%p116, %r156, %r5;
	@%p116 bra 	BB5_182;

	add.s32 	%r950, %r156, %r18;
	mul.wide.s32 	%rd202, %r950, 8;
	add.s64 	%rd203, %rd2, %rd202;
	ld.global.v2.u32 	{%r951, %r952}, [%rd203];
	or.b32  	%r953, %r951, %r952;
	setp.eq.s32	%p117, %r953, 0;
	@%p117 bra 	BB5_182;

	and.b32  	%r954, %r951, 131040;
	and.b32  	%r955, %r951, 31;
	xor.b32  	%r956, %r955, 1;
	shr.u32 	%r957, %r954, 3;
	add.s32 	%r959, %r302, %r957;
	mov.u32 	%r960, 1;
	shl.b32 	%r961, %r960, %r956;
	ld.shared.u32 	%r962, [%r959];
	and.b32  	%r963, %r962, %r961;
	setp.eq.s32	%p118, %r963, 0;
	@%p118 bra 	BB5_182;

	bfe.u32 	%r159, %r952, 17, 12;
	shl.b32 	%r964, %r159, 3;
	add.s32 	%r966, %r302, %r964;
	add.s32 	%r160, %r966, 16384;
	atom.shared.exch.b64 	%rd10, [%r160], 0;
	setp.eq.s64	%p119, %rd10, 0;
	@%p119 bra 	BB5_180;
	bra.uni 	BB5_179;

BB5_180:
	cvt.u64.u32	%rd209, %r952;
	cvt.u64.u32	%rd210, %r951;
	bfi.b64 	%rd211, %rd209, %rd210, 32, 32;
	add.s32 	%r1780, %r966, 16384;
	atom.shared.cas.b64 	%rd213, [%r1780], %rd105, %rd211;
	setp.eq.s64	%p120, %rd213, 0;
	@%p120 bra 	BB5_182;

	add.s32 	%r976, %r159, %r17;
	mul.wide.s32 	%rd214, %r976, 4;
	add.s64 	%rd215, %rd3, %rd214;
	atom.global.add.u32 	%r977, [%rd215], 2;
	min.s32 	%r978, %r977, %r118;
	mad.lo.s32 	%r979, %r159, %r299, %r978;
	shr.u32 	%r980, %r979, 31;
	add.s32 	%r981, %r979, %r980;
	shr.s32 	%r982, %r981, 1;
	mul.wide.s32 	%rd216, %r982, 16;
	add.s64 	%rd217, %rd1, %rd216;
	mov.u32 	%r983, 0;
	st.global.v4.u32 	[%rd217], {%r952, %r951, %r983, %r983};
	bra.uni 	BB5_182;

BB5_179:
	add.s32 	%r967, %r159, %r17;
	mul.wide.s32 	%rd204, %r967, 4;
	add.s64 	%rd205, %rd3, %rd204;
	atom.global.add.u32 	%r968, [%rd205], 2;
	min.s32 	%r969, %r968, %r118;
	mad.lo.s32 	%r970, %r159, %r299, %r969;
	shr.u32 	%r971, %r970, 31;
	add.s32 	%r972, %r970, %r971;
	shr.s32 	%r973, %r972, 1;
	shr.u64 	%rd206, %rd10, 32;
	mul.wide.s32 	%rd207, %r973, 16;
	add.s64 	%rd208, %rd1, %rd207;
	cvt.u32.u64	%r974, %rd10;
	cvt.u32.u64	%r975, %rd206;
	st.global.v4.u32 	[%rd208], {%r952, %r951, %r975, %r974};

BB5_182:
	add.s32 	%r1822, %r1822, 4;
	add.s32 	%r1825, %r1825, 4096;
	setp.lt.s32	%p121, %r1822, %r7;
	@%p121 bra 	BB5_154;

BB5_183:
	@%p22 bra 	BB5_242;

	add.s32 	%r163, %r299, -4;
	mov.u32 	%r988, 1;
	max.s32 	%r164, %r10, %r988;
	and.b32  	%r987, %r164, 3;
	mov.u32 	%r1827, 0;
	setp.eq.s32	%p123, %r987, 0;
	@%p123 bra 	BB5_211;

	setp.eq.s32	%p124, %r987, 1;
	@%p124 bra 	BB5_203;

	setp.eq.s32	%p125, %r987, 2;
	@%p125 bra 	BB5_195;

	setp.ge.s32	%p126, %r1, %r8;
	@%p126 bra 	BB5_188;

	add.s32 	%r991, %r1, %r19;
	mul.wide.s32 	%rd218, %r991, 8;
	add.s64 	%rd219, %rd2, %rd218;
	ld.global.v2.u32 	{%r992, %r993}, [%rd219];
	or.b32  	%r994, %r992, %r993;
	setp.eq.s32	%p127, %r994, 0;
	mov.u32 	%r1827, %r988;
	@%p127 bra 	BB5_195;

	and.b32  	%r996, %r992, 131040;
	and.b32  	%r997, %r992, 31;
	xor.b32  	%r998, %r997, 1;
	shr.u32 	%r999, %r996, 3;
	add.s32 	%r1001, %r302, %r999;
	mov.u32 	%r1827, 1;
	shl.b32 	%r1002, %r1827, %r998;
	ld.shared.u32 	%r1003, [%r1001];
	and.b32  	%r1004, %r1003, %r1002;
	setp.eq.s32	%p128, %r1004, 0;
	@%p128 bra 	BB5_195;

	bfe.u32 	%r167, %r993, 17, 12;
	shl.b32 	%r1005, %r167, 3;
	add.s32 	%r1007, %r302, %r1005;
	add.s32 	%r168, %r1007, 16384;
	atom.shared.exch.b64 	%rd11, [%r168], 0;
	setp.eq.s64	%p129, %rd11, 0;
	@%p129 bra 	BB5_193;

	add.s32 	%r1009, %r167, %r17;
	mul.wide.s32 	%rd220, %r1009, 4;
	add.s64 	%rd221, %rd3, %rd220;
	atom.global.add.u32 	%r1010, [%rd221], 2;
	min.s32 	%r1011, %r1010, %r163;
	mad.lo.s32 	%r1012, %r167, %r299, %r1011;
	shr.u32 	%r1013, %r1012, 31;
	add.s32 	%r1014, %r1012, %r1013;
	shr.s32 	%r1015, %r1014, 1;
	shr.u64 	%rd222, %rd11, 32;
	mul.wide.s32 	%rd223, %r1015, 16;
	add.s64 	%rd224, %rd1, %rd223;
	cvt.u32.u64	%r1016, %rd11;
	cvt.u32.u64	%r1017, %rd222;
	st.global.v4.u32 	[%rd224], {%r993, %r992, %r1017, %r1016};
	bra.uni 	BB5_195;

BB5_188:
	mov.u32 	%r1827, %r988;

BB5_195:
	shl.b32 	%r1028, %r1827, 10;
	add.s32 	%r170, %r1028, %r1;
	setp.ge.s32	%p131, %r170, %r8;
	@%p131 bra 	BB5_202;

	add.s32 	%r1029, %r170, %r19;
	mul.wide.s32 	%rd234, %r1029, 8;
	add.s64 	%rd235, %rd2, %rd234;
	ld.global.v2.u32 	{%r1030, %r1031}, [%rd235];
	or.b32  	%r1032, %r1030, %r1031;
	setp.eq.s32	%p132, %r1032, 0;
	@%p132 bra 	BB5_202;

	and.b32  	%r1033, %r1030, 131040;
	and.b32  	%r1034, %r1030, 31;
	xor.b32  	%r1035, %r1034, 1;
	shr.u32 	%r1036, %r1033, 3;
	add.s32 	%r1038, %r302, %r1036;
	mov.u32 	%r1039, 1;
	shl.b32 	%r1040, %r1039, %r1035;
	ld.shared.u32 	%r1041, [%r1038];
	and.b32  	%r1042, %r1041, %r1040;
	setp.eq.s32	%p133, %r1042, 0;
	@%p133 bra 	BB5_202;

	bfe.u32 	%r173, %r1031, 17, 12;
	shl.b32 	%r1043, %r173, 3;
	add.s32 	%r1045, %r302, %r1043;
	add.s32 	%r174, %r1045, 16384;
	atom.shared.exch.b64 	%rd12, [%r174], 0;
	setp.eq.s64	%p134, %rd12, 0;
	@%p134 bra 	BB5_200;

	add.s32 	%r1046, %r173, %r17;
	mul.wide.s32 	%rd236, %r1046, 4;
	add.s64 	%rd237, %rd3, %rd236;
	atom.global.add.u32 	%r1047, [%rd237], 2;
	min.s32 	%r1048, %r1047, %r163;
	mad.lo.s32 	%r1049, %r173, %r299, %r1048;
	shr.u32 	%r1050, %r1049, 31;
	add.s32 	%r1051, %r1049, %r1050;
	shr.s32 	%r1052, %r1051, 1;
	shr.u64 	%rd238, %rd12, 32;
	mul.wide.s32 	%rd239, %r1052, 16;
	add.s64 	%rd240, %rd1, %rd239;
	cvt.u32.u64	%r1053, %rd12;
	cvt.u32.u64	%r1054, %rd238;
	st.global.v4.u32 	[%rd240], {%r1031, %r1030, %r1054, %r1053};
	bra.uni 	BB5_202;

BB5_200:
	cvt.u64.u32	%rd241, %r1031;
	cvt.u64.u32	%rd242, %r1030;
	bfi.b64 	%rd243, %rd241, %rd242, 32, 32;
	add.s32 	%r1782, %r1045, 16384;
	atom.shared.cas.b64 	%rd245, [%r1782], %rd105, %rd243;
	setp.eq.s64	%p135, %rd245, 0;
	@%p135 bra 	BB5_202;

	add.s32 	%r1055, %r173, %r17;
	mul.wide.s32 	%rd246, %r1055, 4;
	add.s64 	%rd247, %rd3, %rd246;
	atom.global.add.u32 	%r1056, [%rd247], 2;
	min.s32 	%r1057, %r1056, %r163;
	mad.lo.s32 	%r1058, %r173, %r299, %r1057;
	shr.u32 	%r1059, %r1058, 31;
	add.s32 	%r1060, %r1058, %r1059;
	shr.s32 	%r1061, %r1060, 1;
	mul.wide.s32 	%rd248, %r1061, 16;
	add.s64 	%rd249, %rd1, %rd248;
	mov.u32 	%r1062, 0;
	st.global.v4.u32 	[%rd249], {%r1031, %r1030, %r1062, %r1062};

BB5_202:
	add.s32 	%r1827, %r1827, 1;

BB5_203:
	shl.b32 	%r1063, %r1827, 10;
	add.s32 	%r177, %r1063, %r1;
	setp.ge.s32	%p136, %r177, %r8;
	@%p136 bra 	BB5_210;

	add.s32 	%r1064, %r177, %r19;
	mul.wide.s32 	%rd250, %r1064, 8;
	add.s64 	%rd251, %rd2, %rd250;
	ld.global.v2.u32 	{%r1065, %r1066}, [%rd251];
	or.b32  	%r1067, %r1065, %r1066;
	setp.eq.s32	%p137, %r1067, 0;
	@%p137 bra 	BB5_210;

	and.b32  	%r1068, %r1065, 131040;
	and.b32  	%r1069, %r1065, 31;
	xor.b32  	%r1070, %r1069, 1;
	shr.u32 	%r1071, %r1068, 3;
	add.s32 	%r1073, %r302, %r1071;
	mov.u32 	%r1074, 1;
	shl.b32 	%r1075, %r1074, %r1070;
	ld.shared.u32 	%r1076, [%r1073];
	and.b32  	%r1077, %r1076, %r1075;
	setp.eq.s32	%p138, %r1077, 0;
	@%p138 bra 	BB5_210;

	bfe.u32 	%r180, %r1066, 17, 12;
	shl.b32 	%r1078, %r180, 3;
	add.s32 	%r1080, %r302, %r1078;
	add.s32 	%r181, %r1080, 16384;
	atom.shared.exch.b64 	%rd13, [%r181], 0;
	setp.eq.s64	%p139, %rd13, 0;
	@%p139 bra 	BB5_208;

	add.s32 	%r1081, %r180, %r17;
	mul.wide.s32 	%rd252, %r1081, 4;
	add.s64 	%rd253, %rd3, %rd252;
	atom.global.add.u32 	%r1082, [%rd253], 2;
	min.s32 	%r1083, %r1082, %r163;
	mad.lo.s32 	%r1084, %r180, %r299, %r1083;
	shr.u32 	%r1085, %r1084, 31;
	add.s32 	%r1086, %r1084, %r1085;
	shr.s32 	%r1087, %r1086, 1;
	shr.u64 	%rd254, %rd13, 32;
	mul.wide.s32 	%rd255, %r1087, 16;
	add.s64 	%rd256, %rd1, %rd255;
	cvt.u32.u64	%r1088, %rd13;
	cvt.u32.u64	%r1089, %rd254;
	st.global.v4.u32 	[%rd256], {%r1066, %r1065, %r1089, %r1088};
	bra.uni 	BB5_210;

BB5_208:
	cvt.u64.u32	%rd257, %r1066;
	cvt.u64.u32	%rd258, %r1065;
	bfi.b64 	%rd259, %rd257, %rd258, 32, 32;
	add.s32 	%r1783, %r1080, 16384;
	atom.shared.cas.b64 	%rd261, [%r1783], %rd105, %rd259;
	setp.eq.s64	%p140, %rd261, 0;
	@%p140 bra 	BB5_210;

	add.s32 	%r1090, %r180, %r17;
	mul.wide.s32 	%rd262, %r1090, 4;
	add.s64 	%rd263, %rd3, %rd262;
	atom.global.add.u32 	%r1091, [%rd263], 2;
	min.s32 	%r1092, %r1091, %r163;
	mad.lo.s32 	%r1093, %r180, %r299, %r1092;
	shr.u32 	%r1094, %r1093, 31;
	add.s32 	%r1095, %r1093, %r1094;
	shr.s32 	%r1096, %r1095, 1;
	mul.wide.s32 	%rd264, %r1096, 16;
	add.s64 	%rd265, %rd1, %rd264;
	mov.u32 	%r1097, 0;
	st.global.v4.u32 	[%rd265], {%r1066, %r1065, %r1097, %r1097};

BB5_210:
	add.s32 	%r1827, %r1827, 1;

BB5_211:
	setp.lt.u32	%p141, %r164, 4;
	@%p141 bra 	BB5_242;

	mad.lo.s32 	%r1830, %r1827, 1024, %r1;

BB5_213:
	setp.ge.s32	%p142, %r1830, %r8;
	@%p142 bra 	BB5_220;

	add.s32 	%r1098, %r1830, %r19;
	mul.wide.s32 	%rd266, %r1098, 8;
	add.s64 	%rd267, %rd2, %rd266;
	ld.global.v2.u32 	{%r1099, %r1100}, [%rd267];
	or.b32  	%r1101, %r1099, %r1100;
	setp.eq.s32	%p143, %r1101, 0;
	@%p143 bra 	BB5_220;

	and.b32  	%r1102, %r1099, 131040;
	and.b32  	%r1103, %r1099, 31;
	xor.b32  	%r1104, %r1103, 1;
	shr.u32 	%r1105, %r1102, 3;
	add.s32 	%r1107, %r302, %r1105;
	mov.u32 	%r1108, 1;
	shl.b32 	%r1109, %r1108, %r1104;
	ld.shared.u32 	%r1110, [%r1107];
	and.b32  	%r1111, %r1110, %r1109;
	setp.eq.s32	%p144, %r1111, 0;
	@%p144 bra 	BB5_220;

	bfe.u32 	%r189, %r1100, 17, 12;
	shl.b32 	%r1112, %r189, 3;
	add.s32 	%r1114, %r302, %r1112;
	add.s32 	%r190, %r1114, 16384;
	atom.shared.exch.b64 	%rd14, [%r190], 0;
	setp.eq.s64	%p145, %rd14, 0;
	@%p145 bra 	BB5_218;
	bra.uni 	BB5_217;

BB5_218:
	cvt.u64.u32	%rd273, %r1100;
	cvt.u64.u32	%rd274, %r1099;
	bfi.b64 	%rd275, %rd273, %rd274, 32, 32;
	add.s32 	%r1784, %r1114, 16384;
	atom.shared.cas.b64 	%rd277, [%r1784], %rd105, %rd275;
	setp.eq.s64	%p146, %rd277, 0;
	@%p146 bra 	BB5_220;

	add.s32 	%r1124, %r189, %r17;
	mul.wide.s32 	%rd278, %r1124, 4;
	add.s64 	%rd279, %rd3, %rd278;
	atom.global.add.u32 	%r1125, [%rd279], 2;
	min.s32 	%r1126, %r1125, %r163;
	mad.lo.s32 	%r1127, %r189, %r299, %r1126;
	shr.u32 	%r1128, %r1127, 31;
	add.s32 	%r1129, %r1127, %r1128;
	shr.s32 	%r1130, %r1129, 1;
	mul.wide.s32 	%rd280, %r1130, 16;
	add.s64 	%rd281, %rd1, %rd280;
	mov.u32 	%r1131, 0;
	st.global.v4.u32 	[%rd281], {%r1100, %r1099, %r1131, %r1131};
	bra.uni 	BB5_220;

BB5_217:
	add.s32 	%r1115, %r189, %r17;
	mul.wide.s32 	%rd268, %r1115, 4;
	add.s64 	%rd269, %rd3, %rd268;
	atom.global.add.u32 	%r1116, [%rd269], 2;
	min.s32 	%r1117, %r1116, %r163;
	mad.lo.s32 	%r1118, %r189, %r299, %r1117;
	shr.u32 	%r1119, %r1118, 31;
	add.s32 	%r1120, %r1118, %r1119;
	shr.s32 	%r1121, %r1120, 1;
	shr.u64 	%rd270, %rd14, 32;
	mul.wide.s32 	%rd271, %r1121, 16;
	add.s64 	%rd272, %rd1, %rd271;
	cvt.u32.u64	%r1122, %rd14;
	cvt.u32.u64	%r1123, %rd270;
	st.global.v4.u32 	[%rd272], {%r1100, %r1099, %r1123, %r1122};

BB5_220:
	add.s32 	%r191, %r1830, 1024;
	setp.ge.s32	%p147, %r191, %r8;
	@%p147 bra 	BB5_227;

	add.s32 	%r1132, %r191, %r19;
	mul.wide.s32 	%rd282, %r1132, 8;
	add.s64 	%rd283, %rd2, %rd282;
	ld.global.v2.u32 	{%r1133, %r1134}, [%rd283];
	or.b32  	%r1135, %r1133, %r1134;
	setp.eq.s32	%p148, %r1135, 0;
	@%p148 bra 	BB5_227;

	and.b32  	%r1136, %r1133, 131040;
	and.b32  	%r1137, %r1133, 31;
	xor.b32  	%r1138, %r1137, 1;
	shr.u32 	%r1139, %r1136, 3;
	add.s32 	%r1141, %r302, %r1139;
	mov.u32 	%r1142, 1;
	shl.b32 	%r1143, %r1142, %r1138;
	ld.shared.u32 	%r1144, [%r1141];
	and.b32  	%r1145, %r1144, %r1143;
	setp.eq.s32	%p149, %r1145, 0;
	@%p149 bra 	BB5_227;

	bfe.u32 	%r194, %r1134, 17, 12;
	shl.b32 	%r1146, %r194, 3;
	add.s32 	%r1148, %r302, %r1146;
	add.s32 	%r195, %r1148, 16384;
	atom.shared.exch.b64 	%rd15, [%r195], 0;
	setp.eq.s64	%p150, %rd15, 0;
	@%p150 bra 	BB5_225;
	bra.uni 	BB5_224;

BB5_225:
	cvt.u64.u32	%rd289, %r1134;
	cvt.u64.u32	%rd290, %r1133;
	bfi.b64 	%rd291, %rd289, %rd290, 32, 32;
	add.s32 	%r1785, %r1148, 16384;
	atom.shared.cas.b64 	%rd293, [%r1785], %rd105, %rd291;
	setp.eq.s64	%p151, %rd293, 0;
	@%p151 bra 	BB5_227;

	add.s32 	%r1158, %r194, %r17;
	mul.wide.s32 	%rd294, %r1158, 4;
	add.s64 	%rd295, %rd3, %rd294;
	atom.global.add.u32 	%r1159, [%rd295], 2;
	min.s32 	%r1160, %r1159, %r163;
	mad.lo.s32 	%r1161, %r194, %r299, %r1160;
	shr.u32 	%r1162, %r1161, 31;
	add.s32 	%r1163, %r1161, %r1162;
	shr.s32 	%r1164, %r1163, 1;
	mul.wide.s32 	%rd296, %r1164, 16;
	add.s64 	%rd297, %rd1, %rd296;
	mov.u32 	%r1165, 0;
	st.global.v4.u32 	[%rd297], {%r1134, %r1133, %r1165, %r1165};
	bra.uni 	BB5_227;

BB5_224:
	add.s32 	%r1149, %r194, %r17;
	mul.wide.s32 	%rd284, %r1149, 4;
	add.s64 	%rd285, %rd3, %rd284;
	atom.global.add.u32 	%r1150, [%rd285], 2;
	min.s32 	%r1151, %r1150, %r163;
	mad.lo.s32 	%r1152, %r194, %r299, %r1151;
	shr.u32 	%r1153, %r1152, 31;
	add.s32 	%r1154, %r1152, %r1153;
	shr.s32 	%r1155, %r1154, 1;
	shr.u64 	%rd286, %rd15, 32;
	mul.wide.s32 	%rd287, %r1155, 16;
	add.s64 	%rd288, %rd1, %rd287;
	cvt.u32.u64	%r1156, %rd15;
	cvt.u32.u64	%r1157, %rd286;
	st.global.v4.u32 	[%rd288], {%r1134, %r1133, %r1157, %r1156};

BB5_227:
	add.s32 	%r196, %r1830, 2048;
	setp.ge.s32	%p152, %r196, %r8;
	@%p152 bra 	BB5_234;

	add.s32 	%r1166, %r196, %r19;
	mul.wide.s32 	%rd298, %r1166, 8;
	add.s64 	%rd299, %rd2, %rd298;
	ld.global.v2.u32 	{%r1167, %r1168}, [%rd299];
	or.b32  	%r1169, %r1167, %r1168;
	setp.eq.s32	%p153, %r1169, 0;
	@%p153 bra 	BB5_234;

	and.b32  	%r1170, %r1167, 131040;
	and.b32  	%r1171, %r1167, 31;
	xor.b32  	%r1172, %r1171, 1;
	shr.u32 	%r1173, %r1170, 3;
	add.s32 	%r1175, %r302, %r1173;
	mov.u32 	%r1176, 1;
	shl.b32 	%r1177, %r1176, %r1172;
	ld.shared.u32 	%r1178, [%r1175];
	and.b32  	%r1179, %r1178, %r1177;
	setp.eq.s32	%p154, %r1179, 0;
	@%p154 bra 	BB5_234;

	bfe.u32 	%r199, %r1168, 17, 12;
	shl.b32 	%r1180, %r199, 3;
	add.s32 	%r1182, %r302, %r1180;
	add.s32 	%r200, %r1182, 16384;
	atom.shared.exch.b64 	%rd16, [%r200], 0;
	setp.eq.s64	%p155, %rd16, 0;
	@%p155 bra 	BB5_232;
	bra.uni 	BB5_231;

BB5_232:
	cvt.u64.u32	%rd305, %r1168;
	cvt.u64.u32	%rd306, %r1167;
	bfi.b64 	%rd307, %rd305, %rd306, 32, 32;
	add.s32 	%r1786, %r1182, 16384;
	atom.shared.cas.b64 	%rd309, [%r1786], %rd105, %rd307;
	setp.eq.s64	%p156, %rd309, 0;
	@%p156 bra 	BB5_234;

	add.s32 	%r1192, %r199, %r17;
	mul.wide.s32 	%rd310, %r1192, 4;
	add.s64 	%rd311, %rd3, %rd310;
	atom.global.add.u32 	%r1193, [%rd311], 2;
	min.s32 	%r1194, %r1193, %r163;
	mad.lo.s32 	%r1195, %r199, %r299, %r1194;
	shr.u32 	%r1196, %r1195, 31;
	add.s32 	%r1197, %r1195, %r1196;
	shr.s32 	%r1198, %r1197, 1;
	mul.wide.s32 	%rd312, %r1198, 16;
	add.s64 	%rd313, %rd1, %rd312;
	mov.u32 	%r1199, 0;
	st.global.v4.u32 	[%rd313], {%r1168, %r1167, %r1199, %r1199};
	bra.uni 	BB5_234;

BB5_231:
	add.s32 	%r1183, %r199, %r17;
	mul.wide.s32 	%rd300, %r1183, 4;
	add.s64 	%rd301, %rd3, %rd300;
	atom.global.add.u32 	%r1184, [%rd301], 2;
	min.s32 	%r1185, %r1184, %r163;
	mad.lo.s32 	%r1186, %r199, %r299, %r1185;
	shr.u32 	%r1187, %r1186, 31;
	add.s32 	%r1188, %r1186, %r1187;
	shr.s32 	%r1189, %r1188, 1;
	shr.u64 	%rd302, %rd16, 32;
	mul.wide.s32 	%rd303, %r1189, 16;
	add.s64 	%rd304, %rd1, %rd303;
	cvt.u32.u64	%r1190, %rd16;
	cvt.u32.u64	%r1191, %rd302;
	st.global.v4.u32 	[%rd304], {%r1168, %r1167, %r1191, %r1190};

BB5_234:
	add.s32 	%r201, %r1830, 3072;
	setp.ge.s32	%p157, %r201, %r8;
	@%p157 bra 	BB5_241;

	add.s32 	%r1200, %r201, %r19;
	mul.wide.s32 	%rd314, %r1200, 8;
	add.s64 	%rd315, %rd2, %rd314;
	ld.global.v2.u32 	{%r1201, %r1202}, [%rd315];
	or.b32  	%r1203, %r1201, %r1202;
	setp.eq.s32	%p158, %r1203, 0;
	@%p158 bra 	BB5_241;

	and.b32  	%r1204, %r1201, 131040;
	and.b32  	%r1205, %r1201, 31;
	xor.b32  	%r1206, %r1205, 1;
	shr.u32 	%r1207, %r1204, 3;
	add.s32 	%r1209, %r302, %r1207;
	mov.u32 	%r1210, 1;
	shl.b32 	%r1211, %r1210, %r1206;
	ld.shared.u32 	%r1212, [%r1209];
	and.b32  	%r1213, %r1212, %r1211;
	setp.eq.s32	%p159, %r1213, 0;
	@%p159 bra 	BB5_241;

	bfe.u32 	%r204, %r1202, 17, 12;
	shl.b32 	%r1214, %r204, 3;
	add.s32 	%r1216, %r302, %r1214;
	add.s32 	%r205, %r1216, 16384;
	atom.shared.exch.b64 	%rd17, [%r205], 0;
	setp.eq.s64	%p160, %rd17, 0;
	@%p160 bra 	BB5_239;
	bra.uni 	BB5_238;

BB5_239:
	cvt.u64.u32	%rd321, %r1202;
	cvt.u64.u32	%rd322, %r1201;
	bfi.b64 	%rd323, %rd321, %rd322, 32, 32;
	add.s32 	%r1787, %r1216, 16384;
	atom.shared.cas.b64 	%rd325, [%r1787], %rd105, %rd323;
	setp.eq.s64	%p161, %rd325, 0;
	@%p161 bra 	BB5_241;

	add.s32 	%r1226, %r204, %r17;
	mul.wide.s32 	%rd326, %r1226, 4;
	add.s64 	%rd327, %rd3, %rd326;
	atom.global.add.u32 	%r1227, [%rd327], 2;
	min.s32 	%r1228, %r1227, %r163;
	mad.lo.s32 	%r1229, %r204, %r299, %r1228;
	shr.u32 	%r1230, %r1229, 31;
	add.s32 	%r1231, %r1229, %r1230;
	shr.s32 	%r1232, %r1231, 1;
	mul.wide.s32 	%rd328, %r1232, 16;
	add.s64 	%rd329, %rd1, %rd328;
	mov.u32 	%r1233, 0;
	st.global.v4.u32 	[%rd329], {%r1202, %r1201, %r1233, %r1233};
	bra.uni 	BB5_241;

BB5_238:
	add.s32 	%r1217, %r204, %r17;
	mul.wide.s32 	%rd316, %r1217, 4;
	add.s64 	%rd317, %rd3, %rd316;
	atom.global.add.u32 	%r1218, [%rd317], 2;
	min.s32 	%r1219, %r1218, %r163;
	mad.lo.s32 	%r1220, %r204, %r299, %r1219;
	shr.u32 	%r1221, %r1220, 31;
	add.s32 	%r1222, %r1220, %r1221;
	shr.s32 	%r1223, %r1222, 1;
	shr.u64 	%rd318, %rd17, 32;
	mul.wide.s32 	%rd319, %r1223, 16;
	add.s64 	%rd320, %rd1, %rd319;
	cvt.u32.u64	%r1224, %rd17;
	cvt.u32.u64	%r1225, %rd318;
	st.global.v4.u32 	[%rd320], {%r1202, %r1201, %r1225, %r1224};

BB5_241:
	add.s32 	%r1827, %r1827, 4;
	add.s32 	%r1830, %r1830, 4096;
	setp.lt.s32	%p162, %r1827, %r10;
	@%p162 bra 	BB5_213;

BB5_242:
	@%p42 bra 	BB5_301;

	add.s32 	%r208, %r299, -4;
	mov.u32 	%r1238, 1;
	max.s32 	%r209, %r13, %r1238;
	and.b32  	%r1237, %r209, 3;
	mov.u32 	%r1832, 0;
	setp.eq.s32	%p164, %r1237, 0;
	@%p164 bra 	BB5_270;

	setp.eq.s32	%p165, %r1237, 1;
	@%p165 bra 	BB5_262;

	setp.eq.s32	%p166, %r1237, 2;
	@%p166 bra 	BB5_254;

	setp.ge.s32	%p167, %r1, %r11;
	@%p167 bra 	BB5_247;

	add.s32 	%r1241, %r1, %r20;
	mul.wide.s32 	%rd330, %r1241, 8;
	add.s64 	%rd331, %rd2, %rd330;
	ld.global.v2.u32 	{%r1242, %r1243}, [%rd331];
	or.b32  	%r1244, %r1242, %r1243;
	setp.eq.s32	%p168, %r1244, 0;
	mov.u32 	%r1832, %r1238;
	@%p168 bra 	BB5_254;

	and.b32  	%r1246, %r1242, 131040;
	and.b32  	%r1247, %r1242, 31;
	xor.b32  	%r1248, %r1247, 1;
	shr.u32 	%r1249, %r1246, 3;
	add.s32 	%r1251, %r302, %r1249;
	mov.u32 	%r1832, 1;
	shl.b32 	%r1252, %r1832, %r1248;
	ld.shared.u32 	%r1253, [%r1251];
	and.b32  	%r1254, %r1253, %r1252;
	setp.eq.s32	%p169, %r1254, 0;
	@%p169 bra 	BB5_254;

	bfe.u32 	%r212, %r1243, 17, 12;
	shl.b32 	%r1255, %r212, 3;
	add.s32 	%r1257, %r302, %r1255;
	add.s32 	%r213, %r1257, 16384;
	atom.shared.exch.b64 	%rd18, [%r213], 0;
	setp.eq.s64	%p170, %rd18, 0;
	@%p170 bra 	BB5_252;

	add.s32 	%r1259, %r212, %r17;
	mul.wide.s32 	%rd332, %r1259, 4;
	add.s64 	%rd333, %rd3, %rd332;
	atom.global.add.u32 	%r1260, [%rd333], 2;
	min.s32 	%r1261, %r1260, %r208;
	mad.lo.s32 	%r1262, %r212, %r299, %r1261;
	shr.u32 	%r1263, %r1262, 31;
	add.s32 	%r1264, %r1262, %r1263;
	shr.s32 	%r1265, %r1264, 1;
	shr.u64 	%rd334, %rd18, 32;
	mul.wide.s32 	%rd335, %r1265, 16;
	add.s64 	%rd336, %rd1, %rd335;
	cvt.u32.u64	%r1266, %rd18;
	cvt.u32.u64	%r1267, %rd334;
	st.global.v4.u32 	[%rd336], {%r1243, %r1242, %r1267, %r1266};
	bra.uni 	BB5_254;

BB5_247:
	mov.u32 	%r1832, %r1238;

BB5_254:
	shl.b32 	%r1278, %r1832, 10;
	add.s32 	%r215, %r1278, %r1;
	setp.ge.s32	%p172, %r215, %r11;
	@%p172 bra 	BB5_261;

	add.s32 	%r1279, %r215, %r20;
	mul.wide.s32 	%rd346, %r1279, 8;
	add.s64 	%rd347, %rd2, %rd346;
	ld.global.v2.u32 	{%r1280, %r1281}, [%rd347];
	or.b32  	%r1282, %r1280, %r1281;
	setp.eq.s32	%p173, %r1282, 0;
	@%p173 bra 	BB5_261;

	and.b32  	%r1283, %r1280, 131040;
	and.b32  	%r1284, %r1280, 31;
	xor.b32  	%r1285, %r1284, 1;
	shr.u32 	%r1286, %r1283, 3;
	add.s32 	%r1288, %r302, %r1286;
	mov.u32 	%r1289, 1;
	shl.b32 	%r1290, %r1289, %r1285;
	ld.shared.u32 	%r1291, [%r1288];
	and.b32  	%r1292, %r1291, %r1290;
	setp.eq.s32	%p174, %r1292, 0;
	@%p174 bra 	BB5_261;

	bfe.u32 	%r218, %r1281, 17, 12;
	shl.b32 	%r1293, %r218, 3;
	add.s32 	%r1295, %r302, %r1293;
	add.s32 	%r219, %r1295, 16384;
	atom.shared.exch.b64 	%rd19, [%r219], 0;
	setp.eq.s64	%p175, %rd19, 0;
	@%p175 bra 	BB5_259;

	add.s32 	%r1296, %r218, %r17;
	mul.wide.s32 	%rd348, %r1296, 4;
	add.s64 	%rd349, %rd3, %rd348;
	atom.global.add.u32 	%r1297, [%rd349], 2;
	min.s32 	%r1298, %r1297, %r208;
	mad.lo.s32 	%r1299, %r218, %r299, %r1298;
	shr.u32 	%r1300, %r1299, 31;
	add.s32 	%r1301, %r1299, %r1300;
	shr.s32 	%r1302, %r1301, 1;
	shr.u64 	%rd350, %rd19, 32;
	mul.wide.s32 	%rd351, %r1302, 16;
	add.s64 	%rd352, %rd1, %rd351;
	cvt.u32.u64	%r1303, %rd19;
	cvt.u32.u64	%r1304, %rd350;
	st.global.v4.u32 	[%rd352], {%r1281, %r1280, %r1304, %r1303};
	bra.uni 	BB5_261;

BB5_259:
	cvt.u64.u32	%rd353, %r1281;
	cvt.u64.u32	%rd354, %r1280;
	bfi.b64 	%rd355, %rd353, %rd354, 32, 32;
	add.s32 	%r1789, %r1295, 16384;
	atom.shared.cas.b64 	%rd357, [%r1789], %rd105, %rd355;
	setp.eq.s64	%p176, %rd357, 0;
	@%p176 bra 	BB5_261;

	add.s32 	%r1305, %r218, %r17;
	mul.wide.s32 	%rd358, %r1305, 4;
	add.s64 	%rd359, %rd3, %rd358;
	atom.global.add.u32 	%r1306, [%rd359], 2;
	min.s32 	%r1307, %r1306, %r208;
	mad.lo.s32 	%r1308, %r218, %r299, %r1307;
	shr.u32 	%r1309, %r1308, 31;
	add.s32 	%r1310, %r1308, %r1309;
	shr.s32 	%r1311, %r1310, 1;
	mul.wide.s32 	%rd360, %r1311, 16;
	add.s64 	%rd361, %rd1, %rd360;
	mov.u32 	%r1312, 0;
	st.global.v4.u32 	[%rd361], {%r1281, %r1280, %r1312, %r1312};

BB5_261:
	add.s32 	%r1832, %r1832, 1;

BB5_262:
	shl.b32 	%r1313, %r1832, 10;
	add.s32 	%r222, %r1313, %r1;
	setp.ge.s32	%p177, %r222, %r11;
	@%p177 bra 	BB5_269;

	add.s32 	%r1314, %r222, %r20;
	mul.wide.s32 	%rd362, %r1314, 8;
	add.s64 	%rd363, %rd2, %rd362;
	ld.global.v2.u32 	{%r1315, %r1316}, [%rd363];
	or.b32  	%r1317, %r1315, %r1316;
	setp.eq.s32	%p178, %r1317, 0;
	@%p178 bra 	BB5_269;

	and.b32  	%r1318, %r1315, 131040;
	and.b32  	%r1319, %r1315, 31;
	xor.b32  	%r1320, %r1319, 1;
	shr.u32 	%r1321, %r1318, 3;
	add.s32 	%r1323, %r302, %r1321;
	mov.u32 	%r1324, 1;
	shl.b32 	%r1325, %r1324, %r1320;
	ld.shared.u32 	%r1326, [%r1323];
	and.b32  	%r1327, %r1326, %r1325;
	setp.eq.s32	%p179, %r1327, 0;
	@%p179 bra 	BB5_269;

	bfe.u32 	%r225, %r1316, 17, 12;
	shl.b32 	%r1328, %r225, 3;
	add.s32 	%r1330, %r302, %r1328;
	add.s32 	%r226, %r1330, 16384;
	atom.shared.exch.b64 	%rd20, [%r226], 0;
	setp.eq.s64	%p180, %rd20, 0;
	@%p180 bra 	BB5_267;

	add.s32 	%r1331, %r225, %r17;
	mul.wide.s32 	%rd364, %r1331, 4;
	add.s64 	%rd365, %rd3, %rd364;
	atom.global.add.u32 	%r1332, [%rd365], 2;
	min.s32 	%r1333, %r1332, %r208;
	mad.lo.s32 	%r1334, %r225, %r299, %r1333;
	shr.u32 	%r1335, %r1334, 31;
	add.s32 	%r1336, %r1334, %r1335;
	shr.s32 	%r1337, %r1336, 1;
	shr.u64 	%rd366, %rd20, 32;
	mul.wide.s32 	%rd367, %r1337, 16;
	add.s64 	%rd368, %rd1, %rd367;
	cvt.u32.u64	%r1338, %rd20;
	cvt.u32.u64	%r1339, %rd366;
	st.global.v4.u32 	[%rd368], {%r1316, %r1315, %r1339, %r1338};
	bra.uni 	BB5_269;

BB5_267:
	cvt.u64.u32	%rd369, %r1316;
	cvt.u64.u32	%rd370, %r1315;
	bfi.b64 	%rd371, %rd369, %rd370, 32, 32;
	add.s32 	%r1790, %r1330, 16384;
	atom.shared.cas.b64 	%rd373, [%r1790], %rd105, %rd371;
	setp.eq.s64	%p181, %rd373, 0;
	@%p181 bra 	BB5_269;

	add.s32 	%r1340, %r225, %r17;
	mul.wide.s32 	%rd374, %r1340, 4;
	add.s64 	%rd375, %rd3, %rd374;
	atom.global.add.u32 	%r1341, [%rd375], 2;
	min.s32 	%r1342, %r1341, %r208;
	mad.lo.s32 	%r1343, %r225, %r299, %r1342;
	shr.u32 	%r1344, %r1343, 31;
	add.s32 	%r1345, %r1343, %r1344;
	shr.s32 	%r1346, %r1345, 1;
	mul.wide.s32 	%rd376, %r1346, 16;
	add.s64 	%rd377, %rd1, %rd376;
	mov.u32 	%r1347, 0;
	st.global.v4.u32 	[%rd377], {%r1316, %r1315, %r1347, %r1347};

BB5_269:
	add.s32 	%r1832, %r1832, 1;

BB5_270:
	setp.lt.u32	%p182, %r209, 4;
	@%p182 bra 	BB5_301;

	mad.lo.s32 	%r1835, %r1832, 1024, %r1;

BB5_272:
	setp.ge.s32	%p183, %r1835, %r11;
	@%p183 bra 	BB5_279;

	add.s32 	%r1348, %r1835, %r20;
	mul.wide.s32 	%rd378, %r1348, 8;
	add.s64 	%rd379, %rd2, %rd378;
	ld.global.v2.u32 	{%r1349, %r1350}, [%rd379];
	or.b32  	%r1351, %r1349, %r1350;
	setp.eq.s32	%p184, %r1351, 0;
	@%p184 bra 	BB5_279;

	and.b32  	%r1352, %r1349, 131040;
	and.b32  	%r1353, %r1349, 31;
	xor.b32  	%r1354, %r1353, 1;
	shr.u32 	%r1355, %r1352, 3;
	add.s32 	%r1357, %r302, %r1355;
	mov.u32 	%r1358, 1;
	shl.b32 	%r1359, %r1358, %r1354;
	ld.shared.u32 	%r1360, [%r1357];
	and.b32  	%r1361, %r1360, %r1359;
	setp.eq.s32	%p185, %r1361, 0;
	@%p185 bra 	BB5_279;

	bfe.u32 	%r234, %r1350, 17, 12;
	shl.b32 	%r1362, %r234, 3;
	add.s32 	%r1364, %r302, %r1362;
	add.s32 	%r235, %r1364, 16384;
	atom.shared.exch.b64 	%rd21, [%r235], 0;
	setp.eq.s64	%p186, %rd21, 0;
	@%p186 bra 	BB5_277;
	bra.uni 	BB5_276;

BB5_277:
	cvt.u64.u32	%rd385, %r1350;
	cvt.u64.u32	%rd386, %r1349;
	bfi.b64 	%rd387, %rd385, %rd386, 32, 32;
	add.s32 	%r1791, %r1364, 16384;
	atom.shared.cas.b64 	%rd389, [%r1791], %rd105, %rd387;
	setp.eq.s64	%p187, %rd389, 0;
	@%p187 bra 	BB5_279;

	add.s32 	%r1374, %r234, %r17;
	mul.wide.s32 	%rd390, %r1374, 4;
	add.s64 	%rd391, %rd3, %rd390;
	atom.global.add.u32 	%r1375, [%rd391], 2;
	min.s32 	%r1376, %r1375, %r208;
	mad.lo.s32 	%r1377, %r234, %r299, %r1376;
	shr.u32 	%r1378, %r1377, 31;
	add.s32 	%r1379, %r1377, %r1378;
	shr.s32 	%r1380, %r1379, 1;
	mul.wide.s32 	%rd392, %r1380, 16;
	add.s64 	%rd393, %rd1, %rd392;
	mov.u32 	%r1381, 0;
	st.global.v4.u32 	[%rd393], {%r1350, %r1349, %r1381, %r1381};
	bra.uni 	BB5_279;

BB5_276:
	add.s32 	%r1365, %r234, %r17;
	mul.wide.s32 	%rd380, %r1365, 4;
	add.s64 	%rd381, %rd3, %rd380;
	atom.global.add.u32 	%r1366, [%rd381], 2;
	min.s32 	%r1367, %r1366, %r208;
	mad.lo.s32 	%r1368, %r234, %r299, %r1367;
	shr.u32 	%r1369, %r1368, 31;
	add.s32 	%r1370, %r1368, %r1369;
	shr.s32 	%r1371, %r1370, 1;
	shr.u64 	%rd382, %rd21, 32;
	mul.wide.s32 	%rd383, %r1371, 16;
	add.s64 	%rd384, %rd1, %rd383;
	cvt.u32.u64	%r1372, %rd21;
	cvt.u32.u64	%r1373, %rd382;
	st.global.v4.u32 	[%rd384], {%r1350, %r1349, %r1373, %r1372};

BB5_279:
	add.s32 	%r236, %r1835, 1024;
	setp.ge.s32	%p188, %r236, %r11;
	@%p188 bra 	BB5_286;

	add.s32 	%r1382, %r236, %r20;
	mul.wide.s32 	%rd394, %r1382, 8;
	add.s64 	%rd395, %rd2, %rd394;
	ld.global.v2.u32 	{%r1383, %r1384}, [%rd395];
	or.b32  	%r1385, %r1383, %r1384;
	setp.eq.s32	%p189, %r1385, 0;
	@%p189 bra 	BB5_286;

	and.b32  	%r1386, %r1383, 131040;
	and.b32  	%r1387, %r1383, 31;
	xor.b32  	%r1388, %r1387, 1;
	shr.u32 	%r1389, %r1386, 3;
	add.s32 	%r1391, %r302, %r1389;
	mov.u32 	%r1392, 1;
	shl.b32 	%r1393, %r1392, %r1388;
	ld.shared.u32 	%r1394, [%r1391];
	and.b32  	%r1395, %r1394, %r1393;
	setp.eq.s32	%p190, %r1395, 0;
	@%p190 bra 	BB5_286;

	bfe.u32 	%r239, %r1384, 17, 12;
	shl.b32 	%r1396, %r239, 3;
	add.s32 	%r1398, %r302, %r1396;
	add.s32 	%r240, %r1398, 16384;
	atom.shared.exch.b64 	%rd22, [%r240], 0;
	setp.eq.s64	%p191, %rd22, 0;
	@%p191 bra 	BB5_284;
	bra.uni 	BB5_283;

BB5_284:
	cvt.u64.u32	%rd401, %r1384;
	cvt.u64.u32	%rd402, %r1383;
	bfi.b64 	%rd403, %rd401, %rd402, 32, 32;
	add.s32 	%r1792, %r1398, 16384;
	atom.shared.cas.b64 	%rd405, [%r1792], %rd105, %rd403;
	setp.eq.s64	%p192, %rd405, 0;
	@%p192 bra 	BB5_286;

	add.s32 	%r1408, %r239, %r17;
	mul.wide.s32 	%rd406, %r1408, 4;
	add.s64 	%rd407, %rd3, %rd406;
	atom.global.add.u32 	%r1409, [%rd407], 2;
	min.s32 	%r1410, %r1409, %r208;
	mad.lo.s32 	%r1411, %r239, %r299, %r1410;
	shr.u32 	%r1412, %r1411, 31;
	add.s32 	%r1413, %r1411, %r1412;
	shr.s32 	%r1414, %r1413, 1;
	mul.wide.s32 	%rd408, %r1414, 16;
	add.s64 	%rd409, %rd1, %rd408;
	mov.u32 	%r1415, 0;
	st.global.v4.u32 	[%rd409], {%r1384, %r1383, %r1415, %r1415};
	bra.uni 	BB5_286;

BB5_283:
	add.s32 	%r1399, %r239, %r17;
	mul.wide.s32 	%rd396, %r1399, 4;
	add.s64 	%rd397, %rd3, %rd396;
	atom.global.add.u32 	%r1400, [%rd397], 2;
	min.s32 	%r1401, %r1400, %r208;
	mad.lo.s32 	%r1402, %r239, %r299, %r1401;
	shr.u32 	%r1403, %r1402, 31;
	add.s32 	%r1404, %r1402, %r1403;
	shr.s32 	%r1405, %r1404, 1;
	shr.u64 	%rd398, %rd22, 32;
	mul.wide.s32 	%rd399, %r1405, 16;
	add.s64 	%rd400, %rd1, %rd399;
	cvt.u32.u64	%r1406, %rd22;
	cvt.u32.u64	%r1407, %rd398;
	st.global.v4.u32 	[%rd400], {%r1384, %r1383, %r1407, %r1406};

BB5_286:
	add.s32 	%r241, %r1835, 2048;
	setp.ge.s32	%p193, %r241, %r11;
	@%p193 bra 	BB5_293;

	add.s32 	%r1416, %r241, %r20;
	mul.wide.s32 	%rd410, %r1416, 8;
	add.s64 	%rd411, %rd2, %rd410;
	ld.global.v2.u32 	{%r1417, %r1418}, [%rd411];
	or.b32  	%r1419, %r1417, %r1418;
	setp.eq.s32	%p194, %r1419, 0;
	@%p194 bra 	BB5_293;

	and.b32  	%r1420, %r1417, 131040;
	and.b32  	%r1421, %r1417, 31;
	xor.b32  	%r1422, %r1421, 1;
	shr.u32 	%r1423, %r1420, 3;
	add.s32 	%r1425, %r302, %r1423;
	mov.u32 	%r1426, 1;
	shl.b32 	%r1427, %r1426, %r1422;
	ld.shared.u32 	%r1428, [%r1425];
	and.b32  	%r1429, %r1428, %r1427;
	setp.eq.s32	%p195, %r1429, 0;
	@%p195 bra 	BB5_293;

	bfe.u32 	%r244, %r1418, 17, 12;
	shl.b32 	%r1430, %r244, 3;
	add.s32 	%r1432, %r302, %r1430;
	add.s32 	%r245, %r1432, 16384;
	atom.shared.exch.b64 	%rd23, [%r245], 0;
	setp.eq.s64	%p196, %rd23, 0;
	@%p196 bra 	BB5_291;
	bra.uni 	BB5_290;

BB5_291:
	cvt.u64.u32	%rd417, %r1418;
	cvt.u64.u32	%rd418, %r1417;
	bfi.b64 	%rd419, %rd417, %rd418, 32, 32;
	add.s32 	%r1793, %r1432, 16384;
	atom.shared.cas.b64 	%rd421, [%r1793], %rd105, %rd419;
	setp.eq.s64	%p197, %rd421, 0;
	@%p197 bra 	BB5_293;

	add.s32 	%r1442, %r244, %r17;
	mul.wide.s32 	%rd422, %r1442, 4;
	add.s64 	%rd423, %rd3, %rd422;
	atom.global.add.u32 	%r1443, [%rd423], 2;
	min.s32 	%r1444, %r1443, %r208;
	mad.lo.s32 	%r1445, %r244, %r299, %r1444;
	shr.u32 	%r1446, %r1445, 31;
	add.s32 	%r1447, %r1445, %r1446;
	shr.s32 	%r1448, %r1447, 1;
	mul.wide.s32 	%rd424, %r1448, 16;
	add.s64 	%rd425, %rd1, %rd424;
	mov.u32 	%r1449, 0;
	st.global.v4.u32 	[%rd425], {%r1418, %r1417, %r1449, %r1449};
	bra.uni 	BB5_293;

BB5_290:
	add.s32 	%r1433, %r244, %r17;
	mul.wide.s32 	%rd412, %r1433, 4;
	add.s64 	%rd413, %rd3, %rd412;
	atom.global.add.u32 	%r1434, [%rd413], 2;
	min.s32 	%r1435, %r1434, %r208;
	mad.lo.s32 	%r1436, %r244, %r299, %r1435;
	shr.u32 	%r1437, %r1436, 31;
	add.s32 	%r1438, %r1436, %r1437;
	shr.s32 	%r1439, %r1438, 1;
	shr.u64 	%rd414, %rd23, 32;
	mul.wide.s32 	%rd415, %r1439, 16;
	add.s64 	%rd416, %rd1, %rd415;
	cvt.u32.u64	%r1440, %rd23;
	cvt.u32.u64	%r1441, %rd414;
	st.global.v4.u32 	[%rd416], {%r1418, %r1417, %r1441, %r1440};

BB5_293:
	add.s32 	%r246, %r1835, 3072;
	setp.ge.s32	%p198, %r246, %r11;
	@%p198 bra 	BB5_300;

	add.s32 	%r1450, %r246, %r20;
	mul.wide.s32 	%rd426, %r1450, 8;
	add.s64 	%rd427, %rd2, %rd426;
	ld.global.v2.u32 	{%r1451, %r1452}, [%rd427];
	or.b32  	%r1453, %r1451, %r1452;
	setp.eq.s32	%p199, %r1453, 0;
	@%p199 bra 	BB5_300;

	and.b32  	%r1454, %r1451, 131040;
	and.b32  	%r1455, %r1451, 31;
	xor.b32  	%r1456, %r1455, 1;
	shr.u32 	%r1457, %r1454, 3;
	add.s32 	%r1459, %r302, %r1457;
	mov.u32 	%r1460, 1;
	shl.b32 	%r1461, %r1460, %r1456;
	ld.shared.u32 	%r1462, [%r1459];
	and.b32  	%r1463, %r1462, %r1461;
	setp.eq.s32	%p200, %r1463, 0;
	@%p200 bra 	BB5_300;

	bfe.u32 	%r249, %r1452, 17, 12;
	shl.b32 	%r1464, %r249, 3;
	add.s32 	%r1466, %r302, %r1464;
	add.s32 	%r250, %r1466, 16384;
	atom.shared.exch.b64 	%rd24, [%r250], 0;
	setp.eq.s64	%p201, %rd24, 0;
	@%p201 bra 	BB5_298;
	bra.uni 	BB5_297;

BB5_298:
	cvt.u64.u32	%rd433, %r1452;
	cvt.u64.u32	%rd434, %r1451;
	bfi.b64 	%rd435, %rd433, %rd434, 32, 32;
	add.s32 	%r1794, %r1466, 16384;
	atom.shared.cas.b64 	%rd437, [%r1794], %rd105, %rd435;
	setp.eq.s64	%p202, %rd437, 0;
	@%p202 bra 	BB5_300;

	add.s32 	%r1476, %r249, %r17;
	mul.wide.s32 	%rd438, %r1476, 4;
	add.s64 	%rd439, %rd3, %rd438;
	atom.global.add.u32 	%r1477, [%rd439], 2;
	min.s32 	%r1478, %r1477, %r208;
	mad.lo.s32 	%r1479, %r249, %r299, %r1478;
	shr.u32 	%r1480, %r1479, 31;
	add.s32 	%r1481, %r1479, %r1480;
	shr.s32 	%r1482, %r1481, 1;
	mul.wide.s32 	%rd440, %r1482, 16;
	add.s64 	%rd441, %rd1, %rd440;
	mov.u32 	%r1483, 0;
	st.global.v4.u32 	[%rd441], {%r1452, %r1451, %r1483, %r1483};
	bra.uni 	BB5_300;

BB5_297:
	add.s32 	%r1467, %r249, %r17;
	mul.wide.s32 	%rd428, %r1467, 4;
	add.s64 	%rd429, %rd3, %rd428;
	atom.global.add.u32 	%r1468, [%rd429], 2;
	min.s32 	%r1469, %r1468, %r208;
	mad.lo.s32 	%r1470, %r249, %r299, %r1469;
	shr.u32 	%r1471, %r1470, 31;
	add.s32 	%r1472, %r1470, %r1471;
	shr.s32 	%r1473, %r1472, 1;
	shr.u64 	%rd430, %rd24, 32;
	mul.wide.s32 	%rd431, %r1473, 16;
	add.s64 	%rd432, %rd1, %rd431;
	cvt.u32.u64	%r1474, %rd24;
	cvt.u32.u64	%r1475, %rd430;
	st.global.v4.u32 	[%rd432], {%r1452, %r1451, %r1475, %r1474};

BB5_300:
	add.s32 	%r1832, %r1832, 4;
	add.s32 	%r1835, %r1835, 4096;
	setp.lt.s32	%p203, %r1832, %r13;
	@%p203 bra 	BB5_272;

BB5_301:
	add.s32 	%r253, %r299, -4;
	@%p62 bra 	BB5_360;

	mov.u32 	%r1488, 1;
	max.s32 	%r254, %r16, %r1488;
	and.b32  	%r1487, %r254, 3;
	setp.eq.s32	%p205, %r1487, 0;
	mov.u32 	%r1841, %r304;
	@%p205 bra 	BB5_329;

	setp.eq.s32	%p206, %r1487, 1;
	mov.u32 	%r1838, %r304;
	@%p206 bra 	BB5_321;

	setp.eq.s32	%p207, %r1487, 2;
	mov.u32 	%r1837, %r304;
	@%p207 bra 	BB5_313;

	setp.ge.s32	%p208, %r1, %r14;
	@%p208 bra 	BB5_306;

	add.s32 	%r1491, %r1, %r21;
	mul.wide.s32 	%rd442, %r1491, 8;
	add.s64 	%rd443, %rd2, %rd442;
	ld.global.v2.u32 	{%r1492, %r1493}, [%rd443];
	or.b32  	%r1494, %r1492, %r1493;
	setp.eq.s32	%p209, %r1494, 0;
	mov.u32 	%r1837, %r1488;
	@%p209 bra 	BB5_313;

	and.b32  	%r1496, %r1492, 131040;
	and.b32  	%r1497, %r1492, 31;
	xor.b32  	%r1498, %r1497, 1;
	shr.u32 	%r1499, %r1496, 3;
	add.s32 	%r1501, %r302, %r1499;
	mov.u32 	%r1837, 1;
	shl.b32 	%r1502, %r1837, %r1498;
	ld.shared.u32 	%r1503, [%r1501];
	and.b32  	%r1504, %r1503, %r1502;
	setp.eq.s32	%p210, %r1504, 0;
	@%p210 bra 	BB5_313;

	bfe.u32 	%r257, %r1493, 17, 12;
	shl.b32 	%r1505, %r257, 3;
	add.s32 	%r1507, %r302, %r1505;
	add.s32 	%r258, %r1507, 16384;
	atom.shared.exch.b64 	%rd25, [%r258], 0;
	setp.eq.s64	%p211, %rd25, 0;
	@%p211 bra 	BB5_311;

	add.s32 	%r1509, %r257, %r17;
	mul.wide.s32 	%rd444, %r1509, 4;
	add.s64 	%rd445, %rd3, %rd444;
	atom.global.add.u32 	%r1510, [%rd445], 2;
	min.s32 	%r1511, %r1510, %r253;
	mad.lo.s32 	%r1512, %r257, %r299, %r1511;
	shr.u32 	%r1513, %r1512, 31;
	add.s32 	%r1514, %r1512, %r1513;
	shr.s32 	%r1515, %r1514, 1;
	shr.u64 	%rd446, %rd25, 32;
	mul.wide.s32 	%rd447, %r1515, 16;
	add.s64 	%rd448, %rd1, %rd447;
	cvt.u32.u64	%r1516, %rd25;
	cvt.u32.u64	%r1517, %rd446;
	st.global.v4.u32 	[%rd448], {%r1493, %r1492, %r1517, %r1516};
	bra.uni 	BB5_313;

BB5_306:
	mov.u32 	%r1837, %r1488;

BB5_313:
	shl.b32 	%r1528, %r1837, 10;
	add.s32 	%r260, %r1528, %r1;
	setp.ge.s32	%p213, %r260, %r14;
	@%p213 bra 	BB5_320;

	add.s32 	%r1529, %r260, %r21;
	mul.wide.s32 	%rd458, %r1529, 8;
	add.s64 	%rd459, %rd2, %rd458;
	ld.global.v2.u32 	{%r1530, %r1531}, [%rd459];
	or.b32  	%r1532, %r1530, %r1531;
	setp.eq.s32	%p214, %r1532, 0;
	@%p214 bra 	BB5_320;

	and.b32  	%r1533, %r1530, 131040;
	and.b32  	%r1534, %r1530, 31;
	xor.b32  	%r1535, %r1534, 1;
	shr.u32 	%r1536, %r1533, 3;
	add.s32 	%r1538, %r302, %r1536;
	mov.u32 	%r1539, 1;
	shl.b32 	%r1540, %r1539, %r1535;
	ld.shared.u32 	%r1541, [%r1538];
	and.b32  	%r1542, %r1541, %r1540;
	setp.eq.s32	%p215, %r1542, 0;
	@%p215 bra 	BB5_320;

	bfe.u32 	%r263, %r1531, 17, 12;
	shl.b32 	%r1543, %r263, 3;
	add.s32 	%r1545, %r302, %r1543;
	add.s32 	%r264, %r1545, 16384;
	atom.shared.exch.b64 	%rd26, [%r264], 0;
	setp.eq.s64	%p216, %rd26, 0;
	@%p216 bra 	BB5_318;

	add.s32 	%r1546, %r263, %r17;
	mul.wide.s32 	%rd460, %r1546, 4;
	add.s64 	%rd461, %rd3, %rd460;
	atom.global.add.u32 	%r1547, [%rd461], 2;
	min.s32 	%r1548, %r1547, %r253;
	mad.lo.s32 	%r1549, %r263, %r299, %r1548;
	shr.u32 	%r1550, %r1549, 31;
	add.s32 	%r1551, %r1549, %r1550;
	shr.s32 	%r1552, %r1551, 1;
	shr.u64 	%rd462, %rd26, 32;
	mul.wide.s32 	%rd463, %r1552, 16;
	add.s64 	%rd464, %rd1, %rd463;
	cvt.u32.u64	%r1553, %rd26;
	cvt.u32.u64	%r1554, %rd462;
	st.global.v4.u32 	[%rd464], {%r1531, %r1530, %r1554, %r1553};
	bra.uni 	BB5_320;

BB5_318:
	cvt.u64.u32	%rd465, %r1531;
	cvt.u64.u32	%rd466, %r1530;
	bfi.b64 	%rd467, %rd465, %rd466, 32, 32;
	add.s32 	%r1796, %r1545, 16384;
	atom.shared.cas.b64 	%rd469, [%r1796], %rd105, %rd467;
	setp.eq.s64	%p217, %rd469, 0;
	@%p217 bra 	BB5_320;

	add.s32 	%r1555, %r263, %r17;
	mul.wide.s32 	%rd470, %r1555, 4;
	add.s64 	%rd471, %rd3, %rd470;
	atom.global.add.u32 	%r1556, [%rd471], 2;
	min.s32 	%r1557, %r1556, %r253;
	mad.lo.s32 	%r1558, %r263, %r299, %r1557;
	shr.u32 	%r1559, %r1558, 31;
	add.s32 	%r1560, %r1558, %r1559;
	shr.s32 	%r1561, %r1560, 1;
	mul.wide.s32 	%rd472, %r1561, 16;
	add.s64 	%rd473, %rd1, %rd472;
	mov.u32 	%r1562, 0;
	st.global.v4.u32 	[%rd473], {%r1531, %r1530, %r1562, %r1562};

BB5_320:
	add.s32 	%r1838, %r1837, 1;

BB5_321:
	shl.b32 	%r1563, %r1838, 10;
	add.s32 	%r267, %r1563, %r1;
	setp.ge.s32	%p218, %r267, %r14;
	@%p218 bra 	BB5_328;

	add.s32 	%r1564, %r267, %r21;
	mul.wide.s32 	%rd474, %r1564, 8;
	add.s64 	%rd475, %rd2, %rd474;
	ld.global.v2.u32 	{%r1565, %r1566}, [%rd475];
	or.b32  	%r1567, %r1565, %r1566;
	setp.eq.s32	%p219, %r1567, 0;
	@%p219 bra 	BB5_328;

	and.b32  	%r1568, %r1565, 131040;
	and.b32  	%r1569, %r1565, 31;
	xor.b32  	%r1570, %r1569, 1;
	shr.u32 	%r1571, %r1568, 3;
	add.s32 	%r1573, %r302, %r1571;
	mov.u32 	%r1574, 1;
	shl.b32 	%r1575, %r1574, %r1570;
	ld.shared.u32 	%r1576, [%r1573];
	and.b32  	%r1577, %r1576, %r1575;
	setp.eq.s32	%p220, %r1577, 0;
	@%p220 bra 	BB5_328;

	bfe.u32 	%r270, %r1566, 17, 12;
	shl.b32 	%r1578, %r270, 3;
	add.s32 	%r1580, %r302, %r1578;
	add.s32 	%r271, %r1580, 16384;
	atom.shared.exch.b64 	%rd27, [%r271], 0;
	setp.eq.s64	%p221, %rd27, 0;
	@%p221 bra 	BB5_326;

	add.s32 	%r1581, %r270, %r17;
	mul.wide.s32 	%rd476, %r1581, 4;
	add.s64 	%rd477, %rd3, %rd476;
	atom.global.add.u32 	%r1582, [%rd477], 2;
	min.s32 	%r1583, %r1582, %r253;
	mad.lo.s32 	%r1584, %r270, %r299, %r1583;
	shr.u32 	%r1585, %r1584, 31;
	add.s32 	%r1586, %r1584, %r1585;
	shr.s32 	%r1587, %r1586, 1;
	shr.u64 	%rd478, %rd27, 32;
	mul.wide.s32 	%rd479, %r1587, 16;
	add.s64 	%rd480, %rd1, %rd479;
	cvt.u32.u64	%r1588, %rd27;
	cvt.u32.u64	%r1589, %rd478;
	st.global.v4.u32 	[%rd480], {%r1566, %r1565, %r1589, %r1588};
	bra.uni 	BB5_328;

BB5_326:
	cvt.u64.u32	%rd481, %r1566;
	cvt.u64.u32	%rd482, %r1565;
	bfi.b64 	%rd483, %rd481, %rd482, 32, 32;
	add.s32 	%r1797, %r1580, 16384;
	atom.shared.cas.b64 	%rd485, [%r1797], %rd105, %rd483;
	setp.eq.s64	%p222, %rd485, 0;
	@%p222 bra 	BB5_328;

	add.s32 	%r1590, %r270, %r17;
	mul.wide.s32 	%rd486, %r1590, 4;
	add.s64 	%rd487, %rd3, %rd486;
	atom.global.add.u32 	%r1591, [%rd487], 2;
	min.s32 	%r1592, %r1591, %r253;
	mad.lo.s32 	%r1593, %r270, %r299, %r1592;
	shr.u32 	%r1594, %r1593, 31;
	add.s32 	%r1595, %r1593, %r1594;
	shr.s32 	%r1596, %r1595, 1;
	mul.wide.s32 	%rd488, %r1596, 16;
	add.s64 	%rd489, %rd1, %rd488;
	mov.u32 	%r1597, 0;
	st.global.v4.u32 	[%rd489], {%r1566, %r1565, %r1597, %r1597};

BB5_328:
	add.s32 	%r1841, %r1838, 1;

BB5_329:
	setp.lt.u32	%p223, %r254, 4;
	@%p223 bra 	BB5_360;

	mad.lo.s32 	%r1840, %r1841, 1024, %r1;

BB5_331:
	setp.ge.s32	%p224, %r1840, %r14;
	@%p224 bra 	BB5_338;

	add.s32 	%r1598, %r1840, %r21;
	mul.wide.s32 	%rd490, %r1598, 8;
	add.s64 	%rd491, %rd2, %rd490;
	ld.global.v2.u32 	{%r1599, %r1600}, [%rd491];
	or.b32  	%r1601, %r1599, %r1600;
	setp.eq.s32	%p225, %r1601, 0;
	@%p225 bra 	BB5_338;

	and.b32  	%r1602, %r1599, 131040;
	and.b32  	%r1603, %r1599, 31;
	xor.b32  	%r1604, %r1603, 1;
	shr.u32 	%r1605, %r1602, 3;
	add.s32 	%r1607, %r302, %r1605;
	mov.u32 	%r1608, 1;
	shl.b32 	%r1609, %r1608, %r1604;
	ld.shared.u32 	%r1610, [%r1607];
	and.b32  	%r1611, %r1610, %r1609;
	setp.eq.s32	%p226, %r1611, 0;
	@%p226 bra 	BB5_338;

	bfe.u32 	%r279, %r1600, 17, 12;
	shl.b32 	%r1612, %r279, 3;
	add.s32 	%r1614, %r302, %r1612;
	add.s32 	%r280, %r1614, 16384;
	atom.shared.exch.b64 	%rd28, [%r280], 0;
	setp.eq.s64	%p227, %rd28, 0;
	@%p227 bra 	BB5_336;
	bra.uni 	BB5_335;

BB5_336:
	cvt.u64.u32	%rd497, %r1600;
	cvt.u64.u32	%rd498, %r1599;
	bfi.b64 	%rd499, %rd497, %rd498, 32, 32;
	add.s32 	%r1798, %r1614, 16384;
	atom.shared.cas.b64 	%rd501, [%r1798], %rd105, %rd499;
	setp.eq.s64	%p228, %rd501, 0;
	@%p228 bra 	BB5_338;

	add.s32 	%r1624, %r279, %r17;
	mul.wide.s32 	%rd502, %r1624, 4;
	add.s64 	%rd503, %rd3, %rd502;
	atom.global.add.u32 	%r1625, [%rd503], 2;
	min.s32 	%r1626, %r1625, %r253;
	mad.lo.s32 	%r1627, %r279, %r299, %r1626;
	shr.u32 	%r1628, %r1627, 31;
	add.s32 	%r1629, %r1627, %r1628;
	shr.s32 	%r1630, %r1629, 1;
	mul.wide.s32 	%rd504, %r1630, 16;
	add.s64 	%rd505, %rd1, %rd504;
	mov.u32 	%r1631, 0;
	st.global.v4.u32 	[%rd505], {%r1600, %r1599, %r1631, %r1631};
	bra.uni 	BB5_338;

BB5_335:
	add.s32 	%r1615, %r279, %r17;
	mul.wide.s32 	%rd492, %r1615, 4;
	add.s64 	%rd493, %rd3, %rd492;
	atom.global.add.u32 	%r1616, [%rd493], 2;
	min.s32 	%r1617, %r1616, %r253;
	mad.lo.s32 	%r1618, %r279, %r299, %r1617;
	shr.u32 	%r1619, %r1618, 31;
	add.s32 	%r1620, %r1618, %r1619;
	shr.s32 	%r1621, %r1620, 1;
	shr.u64 	%rd494, %rd28, 32;
	mul.wide.s32 	%rd495, %r1621, 16;
	add.s64 	%rd496, %rd1, %rd495;
	cvt.u32.u64	%r1622, %rd28;
	cvt.u32.u64	%r1623, %rd494;
	st.global.v4.u32 	[%rd496], {%r1600, %r1599, %r1623, %r1622};

BB5_338:
	add.s32 	%r281, %r1840, 1024;
	setp.ge.s32	%p229, %r281, %r14;
	@%p229 bra 	BB5_345;

	add.s32 	%r1632, %r281, %r21;
	mul.wide.s32 	%rd506, %r1632, 8;
	add.s64 	%rd507, %rd2, %rd506;
	ld.global.v2.u32 	{%r1633, %r1634}, [%rd507];
	or.b32  	%r1635, %r1633, %r1634;
	setp.eq.s32	%p230, %r1635, 0;
	@%p230 bra 	BB5_345;

	and.b32  	%r1636, %r1633, 131040;
	and.b32  	%r1637, %r1633, 31;
	xor.b32  	%r1638, %r1637, 1;
	shr.u32 	%r1639, %r1636, 3;
	add.s32 	%r1641, %r302, %r1639;
	mov.u32 	%r1642, 1;
	shl.b32 	%r1643, %r1642, %r1638;
	ld.shared.u32 	%r1644, [%r1641];
	and.b32  	%r1645, %r1644, %r1643;
	setp.eq.s32	%p231, %r1645, 0;
	@%p231 bra 	BB5_345;

	bfe.u32 	%r284, %r1634, 17, 12;
	shl.b32 	%r1646, %r284, 3;
	add.s32 	%r1648, %r302, %r1646;
	add.s32 	%r285, %r1648, 16384;
	atom.shared.exch.b64 	%rd29, [%r285], 0;
	setp.eq.s64	%p232, %rd29, 0;
	@%p232 bra 	BB5_343;
	bra.uni 	BB5_342;

BB5_343:
	cvt.u64.u32	%rd513, %r1634;
	cvt.u64.u32	%rd514, %r1633;
	bfi.b64 	%rd515, %rd513, %rd514, 32, 32;
	add.s32 	%r1799, %r1648, 16384;
	atom.shared.cas.b64 	%rd517, [%r1799], %rd105, %rd515;
	setp.eq.s64	%p233, %rd517, 0;
	@%p233 bra 	BB5_345;

	add.s32 	%r1658, %r284, %r17;
	mul.wide.s32 	%rd518, %r1658, 4;
	add.s64 	%rd519, %rd3, %rd518;
	atom.global.add.u32 	%r1659, [%rd519], 2;
	min.s32 	%r1660, %r1659, %r253;
	mad.lo.s32 	%r1661, %r284, %r299, %r1660;
	shr.u32 	%r1662, %r1661, 31;
	add.s32 	%r1663, %r1661, %r1662;
	shr.s32 	%r1664, %r1663, 1;
	mul.wide.s32 	%rd520, %r1664, 16;
	add.s64 	%rd521, %rd1, %rd520;
	mov.u32 	%r1665, 0;
	st.global.v4.u32 	[%rd521], {%r1634, %r1633, %r1665, %r1665};
	bra.uni 	BB5_345;

BB5_342:
	add.s32 	%r1649, %r284, %r17;
	mul.wide.s32 	%rd508, %r1649, 4;
	add.s64 	%rd509, %rd3, %rd508;
	atom.global.add.u32 	%r1650, [%rd509], 2;
	min.s32 	%r1651, %r1650, %r253;
	mad.lo.s32 	%r1652, %r284, %r299, %r1651;
	shr.u32 	%r1653, %r1652, 31;
	add.s32 	%r1654, %r1652, %r1653;
	shr.s32 	%r1655, %r1654, 1;
	shr.u64 	%rd510, %rd29, 32;
	mul.wide.s32 	%rd511, %r1655, 16;
	add.s64 	%rd512, %rd1, %rd511;
	cvt.u32.u64	%r1656, %rd29;
	cvt.u32.u64	%r1657, %rd510;
	st.global.v4.u32 	[%rd512], {%r1634, %r1633, %r1657, %r1656};

BB5_345:
	add.s32 	%r286, %r1840, 2048;
	setp.ge.s32	%p234, %r286, %r14;
	@%p234 bra 	BB5_352;

	add.s32 	%r1666, %r286, %r21;
	mul.wide.s32 	%rd522, %r1666, 8;
	add.s64 	%rd523, %rd2, %rd522;
	ld.global.v2.u32 	{%r1667, %r1668}, [%rd523];
	or.b32  	%r1669, %r1667, %r1668;
	setp.eq.s32	%p235, %r1669, 0;
	@%p235 bra 	BB5_352;

	and.b32  	%r1670, %r1667, 131040;
	and.b32  	%r1671, %r1667, 31;
	xor.b32  	%r1672, %r1671, 1;
	shr.u32 	%r1673, %r1670, 3;
	add.s32 	%r1675, %r302, %r1673;
	mov.u32 	%r1676, 1;
	shl.b32 	%r1677, %r1676, %r1672;
	ld.shared.u32 	%r1678, [%r1675];
	and.b32  	%r1679, %r1678, %r1677;
	setp.eq.s32	%p236, %r1679, 0;
	@%p236 bra 	BB5_352;

	bfe.u32 	%r289, %r1668, 17, 12;
	shl.b32 	%r1680, %r289, 3;
	add.s32 	%r1682, %r302, %r1680;
	add.s32 	%r290, %r1682, 16384;
	atom.shared.exch.b64 	%rd30, [%r290], 0;
	setp.eq.s64	%p237, %rd30, 0;
	@%p237 bra 	BB5_350;
	bra.uni 	BB5_349;

BB5_350:
	cvt.u64.u32	%rd529, %r1668;
	cvt.u64.u32	%rd530, %r1667;
	bfi.b64 	%rd531, %rd529, %rd530, 32, 32;
	add.s32 	%r1800, %r1682, 16384;
	atom.shared.cas.b64 	%rd533, [%r1800], %rd105, %rd531;
	setp.eq.s64	%p238, %rd533, 0;
	@%p238 bra 	BB5_352;

	add.s32 	%r1692, %r289, %r17;
	mul.wide.s32 	%rd534, %r1692, 4;
	add.s64 	%rd535, %rd3, %rd534;
	atom.global.add.u32 	%r1693, [%rd535], 2;
	min.s32 	%r1694, %r1693, %r253;
	mad.lo.s32 	%r1695, %r289, %r299, %r1694;
	shr.u32 	%r1696, %r1695, 31;
	add.s32 	%r1697, %r1695, %r1696;
	shr.s32 	%r1698, %r1697, 1;
	mul.wide.s32 	%rd536, %r1698, 16;
	add.s64 	%rd537, %rd1, %rd536;
	mov.u32 	%r1699, 0;
	st.global.v4.u32 	[%rd537], {%r1668, %r1667, %r1699, %r1699};
	bra.uni 	BB5_352;

BB5_349:
	add.s32 	%r1683, %r289, %r17;
	mul.wide.s32 	%rd524, %r1683, 4;
	add.s64 	%rd525, %rd3, %rd524;
	atom.global.add.u32 	%r1684, [%rd525], 2;
	min.s32 	%r1685, %r1684, %r253;
	mad.lo.s32 	%r1686, %r289, %r299, %r1685;
	shr.u32 	%r1687, %r1686, 31;
	add.s32 	%r1688, %r1686, %r1687;
	shr.s32 	%r1689, %r1688, 1;
	shr.u64 	%rd526, %rd30, 32;
	mul.wide.s32 	%rd527, %r1689, 16;
	add.s64 	%rd528, %rd1, %rd527;
	cvt.u32.u64	%r1690, %rd30;
	cvt.u32.u64	%r1691, %rd526;
	st.global.v4.u32 	[%rd528], {%r1668, %r1667, %r1691, %r1690};

BB5_352:
	add.s32 	%r291, %r1840, 3072;
	setp.ge.s32	%p239, %r291, %r14;
	@%p239 bra 	BB5_359;

	add.s32 	%r1700, %r291, %r21;
	mul.wide.s32 	%rd538, %r1700, 8;
	add.s64 	%rd539, %rd2, %rd538;
	ld.global.v2.u32 	{%r1701, %r1702}, [%rd539];
	or.b32  	%r1703, %r1701, %r1702;
	setp.eq.s32	%p240, %r1703, 0;
	@%p240 bra 	BB5_359;

	and.b32  	%r1704, %r1701, 131040;
	and.b32  	%r1705, %r1701, 31;
	xor.b32  	%r1706, %r1705, 1;
	shr.u32 	%r1707, %r1704, 3;
	add.s32 	%r1709, %r302, %r1707;
	mov.u32 	%r1710, 1;
	shl.b32 	%r1711, %r1710, %r1706;
	ld.shared.u32 	%r1712, [%r1709];
	and.b32  	%r1713, %r1712, %r1711;
	setp.eq.s32	%p241, %r1713, 0;
	@%p241 bra 	BB5_359;

	bfe.u32 	%r294, %r1702, 17, 12;
	shl.b32 	%r1714, %r294, 3;
	add.s32 	%r1716, %r302, %r1714;
	add.s32 	%r295, %r1716, 16384;
	atom.shared.exch.b64 	%rd31, [%r295], 0;
	setp.eq.s64	%p242, %rd31, 0;
	@%p242 bra 	BB5_357;
	bra.uni 	BB5_356;

BB5_357:
	cvt.u64.u32	%rd545, %r1702;
	cvt.u64.u32	%rd546, %r1701;
	bfi.b64 	%rd547, %rd545, %rd546, 32, 32;
	add.s32 	%r1801, %r1716, 16384;
	atom.shared.cas.b64 	%rd549, [%r1801], %rd105, %rd547;
	setp.eq.s64	%p243, %rd549, 0;
	@%p243 bra 	BB5_359;

	add.s32 	%r1726, %r294, %r17;
	mul.wide.s32 	%rd550, %r1726, 4;
	add.s64 	%rd551, %rd3, %rd550;
	atom.global.add.u32 	%r1727, [%rd551], 2;
	min.s32 	%r1728, %r1727, %r253;
	mad.lo.s32 	%r1729, %r294, %r299, %r1728;
	shr.u32 	%r1730, %r1729, 31;
	add.s32 	%r1731, %r1729, %r1730;
	shr.s32 	%r1732, %r1731, 1;
	mul.wide.s32 	%rd552, %r1732, 16;
	add.s64 	%rd553, %rd1, %rd552;
	mov.u32 	%r1733, 0;
	st.global.v4.u32 	[%rd553], {%r1702, %r1701, %r1733, %r1733};
	bra.uni 	BB5_359;

BB5_356:
	add.s32 	%r1717, %r294, %r17;
	mul.wide.s32 	%rd540, %r1717, 4;
	add.s64 	%rd541, %rd3, %rd540;
	atom.global.add.u32 	%r1718, [%rd541], 2;
	min.s32 	%r1719, %r1718, %r253;
	mad.lo.s32 	%r1720, %r294, %r299, %r1719;
	shr.u32 	%r1721, %r1720, 31;
	add.s32 	%r1722, %r1720, %r1721;
	shr.s32 	%r1723, %r1722, 1;
	shr.u64 	%rd542, %rd31, 32;
	mul.wide.s32 	%rd543, %r1723, 16;
	add.s64 	%rd544, %rd1, %rd543;
	cvt.u32.u64	%r1724, %rd31;
	cvt.u32.u64	%r1725, %rd542;
	st.global.v4.u32 	[%rd544], {%r1702, %r1701, %r1725, %r1724};

BB5_359:
	add.s32 	%r1841, %r1841, 4;
	add.s32 	%r1840, %r1840, 4096;
	setp.lt.s32	%p244, %r1841, %r16;
	@%p244 bra 	BB5_331;

BB5_360:
	bar.sync 	0;
	ld.shared.u64 	%rd32, [%r114];
	setp.eq.s64	%p245, %rd32, 0;
	@%p245 bra 	BB5_362;

	add.s32 	%r1734, %r1, %r17;
	mul.wide.s32 	%rd554, %r1734, 4;
	add.s64 	%rd555, %rd3, %rd554;
	atom.global.add.u32 	%r1735, [%rd555], 2;
	min.s32 	%r1736, %r1735, %r253;
	mad.lo.s32 	%r1737, %r1, %r299, %r1736;
	shr.u32 	%r1738, %r1737, 31;
	add.s32 	%r1739, %r1737, %r1738;
	shr.s32 	%r1740, %r1739, 1;
	shr.u64 	%rd556, %rd32, 32;
	mul.wide.s32 	%rd557, %r1740, 16;
	add.s64 	%rd558, %rd1, %rd557;
	cvt.u32.u64	%r1741, %rd32;
	cvt.u32.u64	%r1742, %rd556;
	st.global.v4.u32 	[%rd558], {%r1742, %r1741, %r304, %r304};

BB5_362:
	ld.shared.u64 	%rd33, [%r115];
	setp.eq.s64	%p246, %rd33, 0;
	@%p246 bra 	BB5_364;

	add.s32 	%r1744, %r2, %r17;
	mul.wide.s32 	%rd559, %r1744, 4;
	add.s64 	%rd560, %rd3, %rd559;
	atom.global.add.u32 	%r1745, [%rd560], 2;
	min.s32 	%r1746, %r1745, %r253;
	mad.lo.s32 	%r1747, %r2, %r299, %r1746;
	shr.u32 	%r1748, %r1747, 31;
	add.s32 	%r1749, %r1747, %r1748;
	shr.s32 	%r1750, %r1749, 1;
	shr.u64 	%rd561, %rd33, 32;
	mul.wide.s32 	%rd562, %r1750, 16;
	add.s64 	%rd563, %rd1, %rd562;
	cvt.u32.u64	%r1751, %rd33;
	cvt.u32.u64	%r1752, %rd561;
	st.global.v4.u32 	[%rd563], {%r1752, %r1751, %r304, %r304};

BB5_364:
	ld.shared.u64 	%rd34, [%r116];
	setp.eq.s64	%p247, %rd34, 0;
	@%p247 bra 	BB5_366;

	add.s32 	%r1754, %r3, %r17;
	mul.wide.s32 	%rd564, %r1754, 4;
	add.s64 	%rd565, %rd3, %rd564;
	atom.global.add.u32 	%r1755, [%rd565], 2;
	min.s32 	%r1756, %r1755, %r253;
	mad.lo.s32 	%r1757, %r3, %r299, %r1756;
	shr.u32 	%r1758, %r1757, 31;
	add.s32 	%r1759, %r1757, %r1758;
	shr.s32 	%r1760, %r1759, 1;
	shr.u64 	%rd566, %rd34, 32;
	mul.wide.s32 	%rd567, %r1760, 16;
	add.s64 	%rd568, %rd1, %rd567;
	cvt.u32.u64	%r1761, %rd34;
	cvt.u32.u64	%r1762, %rd566;
	st.global.v4.u32 	[%rd568], {%r1762, %r1761, %r304, %r304};

BB5_366:
	ld.shared.u64 	%rd35, [%r117];
	setp.eq.s64	%p248, %rd35, 0;
	@%p248 bra 	BB5_368;

	add.s32 	%r1764, %r4, %r17;
	mul.wide.s32 	%rd569, %r1764, 4;
	add.s64 	%rd570, %rd3, %rd569;
	atom.global.add.u32 	%r1765, [%rd570], 2;
	min.s32 	%r1766, %r1765, %r253;
	mad.lo.s32 	%r1767, %r4, %r299, %r1766;
	shr.u32 	%r1768, %r1767, 31;
	add.s32 	%r1769, %r1767, %r1768;
	shr.s32 	%r1770, %r1769, 1;
	shr.u64 	%rd571, %rd35, 32;
	mul.wide.s32 	%rd572, %r1770, 16;
	add.s64 	%rd573, %rd1, %rd572;
	cvt.u32.u64	%r1771, %rd35;
	cvt.u32.u64	%r1772, %rd571;
	st.global.v4.u32 	[%rd573], {%r1772, %r1771, %r304, %r304};

BB5_368:
	ret;

BB5_134:
	cvt.u64.u32	%rd113, %r743;
	cvt.u64.u32	%rd114, %r742;
	bfi.b64 	%rd115, %rd113, %rd114, 32, 32;
	add.s32 	%r1774, %r757, 16384;
	atom.shared.cas.b64 	%rd117, [%r1774], %rd105, %rd115;
	setp.eq.s64	%p89, %rd117, 0;
	@%p89 bra 	BB5_136;

	add.s32 	%r770, %r122, %r17;
	mul.wide.s32 	%rd118, %r770, 4;
	add.s64 	%rd119, %rd3, %rd118;
	atom.global.add.u32 	%r771, [%rd119], 2;
	min.s32 	%r772, %r771, %r118;
	mad.lo.s32 	%r773, %r122, %r299, %r772;
	shr.u32 	%r774, %r773, 31;
	add.s32 	%r775, %r773, %r774;
	shr.s32 	%r776, %r775, 1;
	mul.wide.s32 	%rd120, %r776, 16;
	add.s64 	%rd121, %rd1, %rd120;
	mov.u32 	%r777, 0;
	st.global.v4.u32 	[%rd121], {%r743, %r742, %r777, %r777};
	bra.uni 	BB5_136;

BB5_193:
	cvt.u64.u32	%rd225, %r993;
	cvt.u64.u32	%rd226, %r992;
	bfi.b64 	%rd227, %rd225, %rd226, 32, 32;
	add.s32 	%r1781, %r1007, 16384;
	atom.shared.cas.b64 	%rd229, [%r1781], %rd105, %rd227;
	setp.eq.s64	%p130, %rd229, 0;
	@%p130 bra 	BB5_195;

	add.s32 	%r1020, %r167, %r17;
	mul.wide.s32 	%rd230, %r1020, 4;
	add.s64 	%rd231, %rd3, %rd230;
	atom.global.add.u32 	%r1021, [%rd231], 2;
	min.s32 	%r1022, %r1021, %r163;
	mad.lo.s32 	%r1023, %r167, %r299, %r1022;
	shr.u32 	%r1024, %r1023, 31;
	add.s32 	%r1025, %r1023, %r1024;
	shr.s32 	%r1026, %r1025, 1;
	mul.wide.s32 	%rd232, %r1026, 16;
	add.s64 	%rd233, %rd1, %rd232;
	mov.u32 	%r1027, 0;
	st.global.v4.u32 	[%rd233], {%r993, %r992, %r1027, %r1027};
	bra.uni 	BB5_195;

BB5_252:
	cvt.u64.u32	%rd337, %r1243;
	cvt.u64.u32	%rd338, %r1242;
	bfi.b64 	%rd339, %rd337, %rd338, 32, 32;
	add.s32 	%r1788, %r1257, 16384;
	atom.shared.cas.b64 	%rd341, [%r1788], %rd105, %rd339;
	setp.eq.s64	%p171, %rd341, 0;
	@%p171 bra 	BB5_254;

	add.s32 	%r1270, %r212, %r17;
	mul.wide.s32 	%rd342, %r1270, 4;
	add.s64 	%rd343, %rd3, %rd342;
	atom.global.add.u32 	%r1271, [%rd343], 2;
	min.s32 	%r1272, %r1271, %r208;
	mad.lo.s32 	%r1273, %r212, %r299, %r1272;
	shr.u32 	%r1274, %r1273, 31;
	add.s32 	%r1275, %r1273, %r1274;
	shr.s32 	%r1276, %r1275, 1;
	mul.wide.s32 	%rd344, %r1276, 16;
	add.s64 	%rd345, %rd1, %rd344;
	mov.u32 	%r1277, 0;
	st.global.v4.u32 	[%rd345], {%r1243, %r1242, %r1277, %r1277};
	bra.uni 	BB5_254;

BB5_311:
	cvt.u64.u32	%rd449, %r1493;
	cvt.u64.u32	%rd450, %r1492;
	bfi.b64 	%rd451, %rd449, %rd450, 32, 32;
	add.s32 	%r1795, %r1507, 16384;
	atom.shared.cas.b64 	%rd453, [%r1795], %rd105, %rd451;
	setp.eq.s64	%p212, %rd453, 0;
	@%p212 bra 	BB5_313;

	add.s32 	%r1520, %r257, %r17;
	mul.wide.s32 	%rd454, %r1520, 4;
	add.s64 	%rd455, %rd3, %rd454;
	atom.global.add.u32 	%r1521, [%rd455], 2;
	min.s32 	%r1522, %r1521, %r253;
	mad.lo.s32 	%r1523, %r257, %r299, %r1522;
	shr.u32 	%r1524, %r1523, 31;
	add.s32 	%r1525, %r1523, %r1524;
	shr.s32 	%r1526, %r1525, 1;
	mul.wide.s32 	%rd456, %r1526, 16;
	add.s64 	%rd457, %rd1, %rd456;
	mov.u32 	%r1527, 0;
	st.global.v4.u32 	[%rd457], {%r1493, %r1492, %r1527, %r1527};
	bra.uni 	BB5_313;
}

	// .globl	FluffyRound_C2
.visible .entry FluffyRound_C2(
	.param .u64 FluffyRound_C2_param_0,
	.param .u64 FluffyRound_C2_param_1,
	.param .u64 FluffyRound_C2_param_2,
	.param .u64 FluffyRound_C2_param_3,
	.param .u32 FluffyRound_C2_param_4,
	.param .u32 FluffyRound_C2_param_5,
	.param .u32 FluffyRound_C2_param_6,
	.param .u64 FluffyRound_C2_param_7
)
{
	.reg .pred 	%p<41>;
	.reg .b32 	%r<382>;
	.reg .b64 	%rd<86>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_C2$__cuda_local_var_207858_30_non_const_ecounters[32768];

	ld.param.u64 	%rd2, [FluffyRound_C2_param_0];
	ld.param.u64 	%rd3, [FluffyRound_C2_param_1];
	ld.param.u64 	%rd5, [FluffyRound_C2_param_2];
	ld.param.u64 	%rd4, [FluffyRound_C2_param_3];
	ld.param.u32 	%r70, [FluffyRound_C2_param_4];
	ld.param.u32 	%r69, [FluffyRound_C2_param_5];
	mov.u32 	%r71, %ctaid.x;
	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r71, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r72, [%rd8];
	min.s32 	%r1, %r72, %r70;
	add.s32 	%r2, %r1, 1024;
	shr.s32 	%r73, %r2, 31;
	shr.u32 	%r74, %r73, 22;
	add.s32 	%r75, %r2, %r74;
	shr.s32 	%r3, %r75, 10;
	mov.u32 	%r4, %tid.x;
	shl.b32 	%r76, %r4, 2;
	mov.u32 	%r77, FluffyRound_C2$__cuda_local_var_207858_30_non_const_ecounters;
	add.s32 	%r78, %r77, %r76;
	mov.u32 	%r372, 0;
	st.shared.u32 	[%r78], %r372;
	st.shared.u32 	[%r78+4096], %r372;
	st.shared.u32 	[%r78+8192], %r372;
	st.shared.u32 	[%r78+12288], %r372;
	st.shared.u32 	[%r78+16384], %r372;
	st.shared.u32 	[%r78+20480], %r372;
	st.shared.u32 	[%r78+24576], %r372;
	st.shared.u32 	[%r78+28672], %r372;
	cvta.to.global.u64 	%rd1, %rd2;
	mul.lo.s32 	%r5, %r71, %r70;
	bar.sync 	0;
	setp.gt.s32	%p1, %r2, 2047;
	@%p1 bra 	BB6_6;
	bra.uni 	BB6_1;

BB6_6:
	add.s32 	%r10, %r3, -1;
	setp.lt.s32	%p5, %r10, 1;
	@%p5 bra 	BB6_28;

	and.b32  	%r112, %r10, 3;
	setp.eq.s32	%p6, %r112, 0;
	@%p6 bra 	BB6_18;

	setp.eq.s32	%p7, %r112, 1;
	@%p7 bra 	BB6_15;

	setp.eq.s32	%p8, %r112, 2;
	@%p8 bra 	BB6_12;

	mov.u32 	%r372, 1;
	setp.ge.s32	%p9, %r4, %r1;
	@%p9 bra 	BB6_12;

	add.s32 	%r115, %r4, %r5;
	mul.wide.s32 	%rd16, %r115, 8;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.u32 	%r116, [%rd17];
	and.b32  	%r117, %r116, 131040;
	and.b32  	%r118, %r116, 31;
	mov.u32 	%r372, 1;
	shl.b32 	%r119, %r372, %r118;
	shr.u32 	%r120, %r117, 3;
	add.s32 	%r122, %r77, %r120;
	atom.shared.or.b32 	%r123, [%r122], %r119;

BB6_12:
	shl.b32 	%r124, %r372, 10;
	add.s32 	%r12, %r124, %r4;
	setp.ge.s32	%p10, %r12, %r1;
	@%p10 bra 	BB6_14;

	add.s32 	%r125, %r12, %r5;
	mul.wide.s32 	%rd18, %r125, 8;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.u32 	%r126, [%rd19];
	and.b32  	%r127, %r126, 131040;
	and.b32  	%r128, %r126, 31;
	mov.u32 	%r129, 1;
	shl.b32 	%r130, %r129, %r128;
	shr.u32 	%r131, %r127, 3;
	add.s32 	%r133, %r77, %r131;
	atom.shared.or.b32 	%r134, [%r133], %r130;

BB6_14:
	add.s32 	%r372, %r372, 1;

BB6_15:
	shl.b32 	%r135, %r372, 10;
	add.s32 	%r15, %r135, %r4;
	setp.ge.s32	%p11, %r15, %r1;
	@%p11 bra 	BB6_17;

	add.s32 	%r136, %r15, %r5;
	mul.wide.s32 	%rd20, %r136, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r137, [%rd21];
	and.b32  	%r138, %r137, 131040;
	and.b32  	%r139, %r137, 31;
	mov.u32 	%r140, 1;
	shl.b32 	%r141, %r140, %r139;
	shr.u32 	%r142, %r138, 3;
	add.s32 	%r144, %r77, %r142;
	atom.shared.or.b32 	%r145, [%r144], %r141;

BB6_17:
	add.s32 	%r372, %r372, 1;

BB6_18:
	setp.lt.u32	%p12, %r10, 4;
	@%p12 bra 	BB6_28;

BB6_19:
	shl.b32 	%r146, %r372, 10;
	add.s32 	%r19, %r146, %r4;
	setp.ge.s32	%p13, %r19, %r1;
	@%p13 bra 	BB6_21;

	add.s32 	%r147, %r19, %r5;
	mul.wide.s32 	%rd22, %r147, 8;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.u32 	%r148, [%rd23];
	and.b32  	%r149, %r148, 131040;
	and.b32  	%r150, %r148, 31;
	mov.u32 	%r151, 1;
	shl.b32 	%r152, %r151, %r150;
	shr.u32 	%r153, %r149, 3;
	add.s32 	%r155, %r77, %r153;
	atom.shared.or.b32 	%r156, [%r155], %r152;

BB6_21:
	add.s32 	%r20, %r19, 1024;
	setp.ge.s32	%p14, %r20, %r1;
	@%p14 bra 	BB6_23;

	add.s32 	%r159, %r20, %r5;
	mul.wide.s32 	%rd24, %r159, 8;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.u32 	%r160, [%rd25];
	and.b32  	%r161, %r160, 131040;
	and.b32  	%r162, %r160, 31;
	mov.u32 	%r163, 1;
	shl.b32 	%r164, %r163, %r162;
	shr.u32 	%r165, %r161, 3;
	add.s32 	%r167, %r77, %r165;
	atom.shared.or.b32 	%r168, [%r167], %r164;

BB6_23:
	add.s32 	%r21, %r19, 2048;
	setp.ge.s32	%p15, %r21, %r1;
	@%p15 bra 	BB6_25;

	add.s32 	%r171, %r21, %r5;
	mul.wide.s32 	%rd26, %r171, 8;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.u32 	%r172, [%rd27];
	and.b32  	%r173, %r172, 131040;
	and.b32  	%r174, %r172, 31;
	mov.u32 	%r175, 1;
	shl.b32 	%r176, %r175, %r174;
	shr.u32 	%r177, %r173, 3;
	add.s32 	%r179, %r77, %r177;
	atom.shared.or.b32 	%r180, [%r179], %r176;

BB6_25:
	add.s32 	%r22, %r19, 3072;
	setp.ge.s32	%p16, %r22, %r1;
	@%p16 bra 	BB6_27;

	add.s32 	%r183, %r22, %r5;
	mul.wide.s32 	%rd28, %r183, 8;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.u32 	%r184, [%rd29];
	and.b32  	%r185, %r184, 131040;
	and.b32  	%r186, %r184, 31;
	mov.u32 	%r187, 1;
	shl.b32 	%r188, %r187, %r186;
	shr.u32 	%r189, %r185, 3;
	add.s32 	%r191, %r77, %r189;
	atom.shared.or.b32 	%r192, [%r191], %r188;

BB6_27:
	add.s32 	%r372, %r372, 4;
	setp.lt.s32	%p17, %r372, %r10;
	@%p17 bra 	BB6_19;

BB6_28:
	shl.b32 	%r194, %r10, 10;
	add.s32 	%r24, %r194, %r4;
	setp.ge.s32	%p18, %r24, %r1;
	@%p18 bra 	BB6_30;

	add.s32 	%r197, %r24, %r5;
	mul.wide.s32 	%rd31, %r197, 8;
	add.s64 	%rd30, %rd2, %rd31;
	// inline asm
	ld.global.cs.v2.u32 {%r376,%r377}, [%rd30];
	// inline asm
	and.b32  	%r198, %r376, 131040;
	and.b32  	%r199, %r376, 31;
	mov.u32 	%r200, 1;
	shl.b32 	%r201, %r200, %r199;
	shr.u32 	%r202, %r198, 3;
	add.s32 	%r204, %r77, %r202;
	atom.shared.or.b32 	%r205, [%r204], %r201;

BB6_30:
	bar.sync 	0;
	@%p18 bra 	BB6_33;

	and.b32  	%r206, %r376, 131040;
	and.b32  	%r207, %r376, 31;
	xor.b32  	%r208, %r207, 1;
	shr.u32 	%r209, %r206, 3;
	add.s32 	%r211, %r77, %r209;
	mov.u32 	%r212, 1;
	shl.b32 	%r213, %r212, %r208;
	ld.shared.u32 	%r214, [%r211];
	and.b32  	%r215, %r214, %r213;
	setp.eq.s32	%p20, %r215, 0;
	@%p20 bra 	BB6_33;

	bfe.u32 	%r218, %r377, 17, 12;
	cvta.to.global.u64 	%rd33, %rd4;
	mul.wide.u32 	%rd34, %r218, 4;
	add.s64 	%rd35, %rd33, %rd34;
	atom.global.add.u32 	%r219, [%rd35], 1;
	add.s32 	%r220, %r69, -2;
	min.s32 	%r221, %r219, %r220;
	mad.lo.s32 	%r222, %r218, %r69, %r221;
	mul.wide.s32 	%rd36, %r222, 8;
	add.s64 	%rd32, %rd3, %rd36;
	// inline asm
	st.global.cg.v2.u32 [%rd32], {%r377, %r376};
	// inline asm

BB6_33:
	add.s32 	%r29, %r3, -2;
	setp.lt.s32	%p21, %r29, 0;
	@%p21 bra 	BB6_63;

	add.s32 	%r30, %r69, -2;
	mov.u32 	%r224, 1;
	sub.s32 	%r225, %r224, %r3;
	mov.u32 	%r226, -1;
	max.s32 	%r227, %r225, %r226;
	add.s32 	%r31, %r3, %r227;
	and.b32  	%r223, %r31, 3;
	setp.eq.s32	%p22, %r223, 0;
	@%p22 bra 	BB6_49;

	setp.eq.s32	%p23, %r223, 1;
	@%p23 bra 	BB6_45;

	setp.eq.s32	%p24, %r223, 2;
	@%p24 bra 	BB6_41;

	shl.b32 	%r228, %r29, 10;
	add.s32 	%r32, %r228, %r4;
	setp.ge.s32	%p25, %r32, %r1;
	@%p25 bra 	BB6_40;

	add.s32 	%r229, %r32, %r5;
	mul.wide.s32 	%rd37, %r229, 8;
	add.s64 	%rd38, %rd1, %rd37;
	ld.global.v2.u32 	{%r230, %r231}, [%rd38];
	and.b32  	%r232, %r230, 131040;
	and.b32  	%r233, %r230, 31;
	xor.b32  	%r234, %r233, 1;
	shr.u32 	%r235, %r232, 3;
	add.s32 	%r237, %r77, %r235;
	shl.b32 	%r239, %r224, %r234;
	ld.shared.u32 	%r240, [%r237];
	and.b32  	%r241, %r239, %r240;
	setp.eq.s32	%p26, %r241, 0;
	@%p26 bra 	BB6_40;

	bfe.u32 	%r244, %r231, 17, 12;
	cvta.to.global.u64 	%rd40, %rd4;
	mul.wide.u32 	%rd41, %r244, 4;
	add.s64 	%rd42, %rd40, %rd41;
	atom.global.add.u32 	%r245, [%rd42], 1;
	min.s32 	%r246, %r245, %r30;
	mad.lo.s32 	%r247, %r244, %r69, %r246;
	mul.wide.s32 	%rd43, %r247, 8;
	add.s64 	%rd39, %rd3, %rd43;
	// inline asm
	st.global.cg.v2.u32 [%rd39], {%r231, %r230};
	// inline asm

BB6_40:
	add.s32 	%r29, %r3, -3;

BB6_41:
	shl.b32 	%r248, %r29, 10;
	add.s32 	%r38, %r248, %r4;
	setp.ge.s32	%p27, %r38, %r1;
	@%p27 bra 	BB6_44;

	add.s32 	%r249, %r38, %r5;
	mul.wide.s32 	%rd44, %r249, 8;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.v2.u32 	{%r250, %r251}, [%rd45];
	and.b32  	%r252, %r250, 131040;
	and.b32  	%r253, %r250, 31;
	xor.b32  	%r254, %r253, 1;
	shr.u32 	%r255, %r252, 3;
	add.s32 	%r257, %r77, %r255;
	shl.b32 	%r259, %r224, %r254;
	ld.shared.u32 	%r260, [%r257];
	and.b32  	%r261, %r259, %r260;
	setp.eq.s32	%p28, %r261, 0;
	@%p28 bra 	BB6_44;

	bfe.u32 	%r264, %r251, 17, 12;
	cvta.to.global.u64 	%rd47, %rd4;
	mul.wide.u32 	%rd48, %r264, 4;
	add.s64 	%rd49, %rd47, %rd48;
	atom.global.add.u32 	%r265, [%rd49], 1;
	min.s32 	%r266, %r265, %r30;
	mad.lo.s32 	%r267, %r264, %r69, %r266;
	mul.wide.s32 	%rd50, %r267, 8;
	add.s64 	%rd46, %rd3, %rd50;
	// inline asm
	st.global.cg.v2.u32 [%rd46], {%r251, %r250};
	// inline asm

BB6_44:
	add.s32 	%r29, %r29, -1;

BB6_45:
	shl.b32 	%r268, %r29, 10;
	add.s32 	%r44, %r268, %r4;
	setp.ge.s32	%p29, %r44, %r1;
	@%p29 bra 	BB6_48;

	add.s32 	%r269, %r44, %r5;
	mul.wide.s32 	%rd51, %r269, 8;
	add.s64 	%rd52, %rd1, %rd51;
	ld.global.v2.u32 	{%r270, %r271}, [%rd52];
	and.b32  	%r272, %r270, 131040;
	and.b32  	%r273, %r270, 31;
	xor.b32  	%r274, %r273, 1;
	shr.u32 	%r275, %r272, 3;
	add.s32 	%r277, %r77, %r275;
	shl.b32 	%r279, %r224, %r274;
	ld.shared.u32 	%r280, [%r277];
	and.b32  	%r281, %r279, %r280;
	setp.eq.s32	%p30, %r281, 0;
	@%p30 bra 	BB6_48;

	bfe.u32 	%r284, %r271, 17, 12;
	cvta.to.global.u64 	%rd54, %rd4;
	mul.wide.u32 	%rd55, %r284, 4;
	add.s64 	%rd56, %rd54, %rd55;
	atom.global.add.u32 	%r285, [%rd56], 1;
	min.s32 	%r286, %r285, %r30;
	mad.lo.s32 	%r287, %r284, %r69, %r286;
	mul.wide.s32 	%rd57, %r287, 8;
	add.s64 	%rd53, %rd3, %rd57;
	// inline asm
	st.global.cg.v2.u32 [%rd53], {%r271, %r270};
	// inline asm

BB6_48:
	add.s32 	%r29, %r29, -1;

BB6_49:
	setp.lt.u32	%p31, %r31, 4;
	@%p31 bra 	BB6_63;

BB6_50:
	shl.b32 	%r288, %r29, 10;
	add.s32 	%r51, %r288, %r4;
	setp.ge.s32	%p32, %r51, %r1;
	@%p32 bra 	BB6_53;

	add.s32 	%r289, %r51, %r5;
	mul.wide.s32 	%rd58, %r289, 8;
	add.s64 	%rd59, %rd1, %rd58;
	ld.global.v2.u32 	{%r290, %r291}, [%rd59];
	and.b32  	%r292, %r290, 131040;
	and.b32  	%r293, %r290, 31;
	xor.b32  	%r294, %r293, 1;
	shr.u32 	%r295, %r292, 3;
	add.s32 	%r297, %r77, %r295;
	shl.b32 	%r299, %r224, %r294;
	ld.shared.u32 	%r300, [%r297];
	and.b32  	%r301, %r299, %r300;
	setp.eq.s32	%p33, %r301, 0;
	@%p33 bra 	BB6_53;

	bfe.u32 	%r304, %r291, 17, 12;
	cvta.to.global.u64 	%rd61, %rd4;
	mul.wide.u32 	%rd62, %r304, 4;
	add.s64 	%rd63, %rd61, %rd62;
	atom.global.add.u32 	%r305, [%rd63], 1;
	min.s32 	%r306, %r305, %r30;
	mad.lo.s32 	%r307, %r304, %r69, %r306;
	mul.wide.s32 	%rd64, %r307, 8;
	add.s64 	%rd60, %rd3, %rd64;
	// inline asm
	st.global.cg.v2.u32 [%rd60], {%r291, %r290};
	// inline asm

BB6_53:
	add.s32 	%r55, %r51, -1024;
	setp.ge.s32	%p34, %r55, %r1;
	@%p34 bra 	BB6_56;

	add.s32 	%r310, %r55, %r5;
	mul.wide.s32 	%rd65, %r310, 8;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.v2.u32 	{%r311, %r312}, [%rd66];
	and.b32  	%r313, %r311, 131040;
	and.b32  	%r314, %r311, 31;
	xor.b32  	%r315, %r314, 1;
	shr.u32 	%r316, %r313, 3;
	add.s32 	%r318, %r77, %r316;
	shl.b32 	%r320, %r224, %r315;
	ld.shared.u32 	%r321, [%r318];
	and.b32  	%r322, %r320, %r321;
	setp.eq.s32	%p35, %r322, 0;
	@%p35 bra 	BB6_56;

	bfe.u32 	%r325, %r312, 17, 12;
	cvta.to.global.u64 	%rd68, %rd4;
	mul.wide.u32 	%rd69, %r325, 4;
	add.s64 	%rd70, %rd68, %rd69;
	atom.global.add.u32 	%r326, [%rd70], 1;
	min.s32 	%r327, %r326, %r30;
	mad.lo.s32 	%r328, %r325, %r69, %r327;
	mul.wide.s32 	%rd71, %r328, 8;
	add.s64 	%rd67, %rd3, %rd71;
	// inline asm
	st.global.cg.v2.u32 [%rd67], {%r312, %r311};
	// inline asm

BB6_56:
	add.s32 	%r59, %r51, -2048;
	setp.ge.s32	%p36, %r59, %r1;
	@%p36 bra 	BB6_59;

	add.s32 	%r331, %r59, %r5;
	mul.wide.s32 	%rd72, %r331, 8;
	add.s64 	%rd73, %rd1, %rd72;
	ld.global.v2.u32 	{%r332, %r333}, [%rd73];
	and.b32  	%r334, %r332, 131040;
	and.b32  	%r335, %r332, 31;
	xor.b32  	%r336, %r335, 1;
	shr.u32 	%r337, %r334, 3;
	add.s32 	%r339, %r77, %r337;
	shl.b32 	%r341, %r224, %r336;
	ld.shared.u32 	%r342, [%r339];
	and.b32  	%r343, %r341, %r342;
	setp.eq.s32	%p37, %r343, 0;
	@%p37 bra 	BB6_59;

	bfe.u32 	%r346, %r333, 17, 12;
	cvta.to.global.u64 	%rd75, %rd4;
	mul.wide.u32 	%rd76, %r346, 4;
	add.s64 	%rd77, %rd75, %rd76;
	atom.global.add.u32 	%r347, [%rd77], 1;
	min.s32 	%r348, %r347, %r30;
	mad.lo.s32 	%r349, %r346, %r69, %r348;
	mul.wide.s32 	%rd78, %r349, 8;
	add.s64 	%rd74, %rd3, %rd78;
	// inline asm
	st.global.cg.v2.u32 [%rd74], {%r333, %r332};
	// inline asm

BB6_59:
	add.s32 	%r63, %r29, -3;
	shl.b32 	%r350, %r63, 10;
	add.s32 	%r64, %r350, %r4;
	setp.ge.s32	%p38, %r64, %r1;
	@%p38 bra 	BB6_62;

	add.s32 	%r351, %r64, %r5;
	mul.wide.s32 	%rd79, %r351, 8;
	add.s64 	%rd80, %rd1, %rd79;
	ld.global.v2.u32 	{%r352, %r353}, [%rd80];
	and.b32  	%r354, %r352, 131040;
	and.b32  	%r355, %r352, 31;
	xor.b32  	%r356, %r355, 1;
	shr.u32 	%r357, %r354, 3;
	add.s32 	%r359, %r77, %r357;
	shl.b32 	%r361, %r224, %r356;
	ld.shared.u32 	%r362, [%r359];
	and.b32  	%r363, %r361, %r362;
	setp.eq.s32	%p39, %r363, 0;
	@%p39 bra 	BB6_62;

	bfe.u32 	%r366, %r353, 17, 12;
	cvta.to.global.u64 	%rd82, %rd4;
	mul.wide.u32 	%rd83, %r366, 4;
	add.s64 	%rd84, %rd82, %rd83;
	atom.global.add.u32 	%r367, [%rd84], 1;
	min.s32 	%r368, %r367, %r30;
	mad.lo.s32 	%r369, %r366, %r69, %r368;
	mul.wide.s32 	%rd85, %r369, 8;
	add.s64 	%rd81, %rd3, %rd85;
	// inline asm
	st.global.cg.v2.u32 [%rd81], {%r353, %r352};
	// inline asm

BB6_62:
	add.s32 	%r29, %r29, -4;
	setp.gt.s32	%p40, %r63, 0;
	@%p40 bra 	BB6_50;
	bra.uni 	BB6_63;

BB6_1:
	setp.ge.s32	%p2, %r4, %r1;
	@%p2 bra 	BB6_3;

	add.s32 	%r83, %r5, %r4;
	mul.wide.s32 	%rd10, %r83, 8;
	add.s64 	%rd9, %rd2, %rd10;
	// inline asm
	ld.global.cs.v2.u32 {%r371,%r370}, [%rd9];
	// inline asm
	and.b32  	%r84, %r371, 131040;
	and.b32  	%r85, %r371, 31;
	mov.u32 	%r86, 1;
	shl.b32 	%r87, %r86, %r85;
	shr.u32 	%r88, %r84, 3;
	add.s32 	%r90, %r77, %r88;
	atom.shared.or.b32 	%r91, [%r90], %r87;

BB6_3:
	bar.sync 	0;
	@%p2 bra 	BB6_63;

	and.b32  	%r92, %r371, 131040;
	and.b32  	%r93, %r371, 31;
	xor.b32  	%r94, %r93, 1;
	shr.u32 	%r95, %r92, 3;
	add.s32 	%r97, %r77, %r95;
	mov.u32 	%r98, 1;
	shl.b32 	%r99, %r98, %r94;
	ld.shared.u32 	%r100, [%r97];
	and.b32  	%r101, %r100, %r99;
	setp.eq.s32	%p4, %r101, 0;
	@%p4 bra 	BB6_63;

	cvta.to.global.u64 	%rd12, %rd4;
	bfe.u32 	%r104, %r370, 17, 12;
	mul.wide.u32 	%rd13, %r104, 4;
	add.s64 	%rd14, %rd12, %rd13;
	atom.global.add.u32 	%r105, [%rd14], 1;
	add.s32 	%r106, %r69, -2;
	min.s32 	%r107, %r105, %r106;
	mad.lo.s32 	%r108, %r104, %r69, %r107;
	mul.wide.s32 	%rd15, %r108, 8;
	add.s64 	%rd11, %rd3, %rd15;
	// inline asm
	st.global.cg.v2.u32 [%rd11], {%r370, %r371};
	// inline asm

BB6_63:
	ret;
}

	// .globl	FluffyRound_C3
.visible .entry FluffyRound_C3(
	.param .u64 FluffyRound_C3_param_0,
	.param .u64 FluffyRound_C3_param_1,
	.param .u64 FluffyRound_C3_param_2,
	.param .u64 FluffyRound_C3_param_3,
	.param .u64 FluffyRound_C3_param_4,
	.param .u64 FluffyRound_C3_param_5,
	.param .u64 FluffyRound_C3_param_6,
	.param .u32 FluffyRound_C3_param_7,
	.param .u32 FluffyRound_C3_param_8
)
{
	.reg .pred 	%p<189>;
	.reg .b32 	%r<1301>;
	.reg .b64 	%rd<242>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_C3$__cuda_local_var_207964_30_non_const_ecounters[32768];

	ld.param.u64 	%rd2, [FluffyRound_C3_param_0];
	ld.param.u64 	%rd3, [FluffyRound_C3_param_1];
	ld.param.u64 	%rd4, [FluffyRound_C3_param_2];
	ld.param.u64 	%rd5, [FluffyRound_C3_param_3];
	ld.param.u64 	%rd6, [FluffyRound_C3_param_4];
	ld.param.u64 	%rd7, [FluffyRound_C3_param_5];
	ld.param.u64 	%rd8, [FluffyRound_C3_param_6];
	ld.param.u32 	%r216, [FluffyRound_C3_param_7];
	ld.param.u32 	%r215, [FluffyRound_C3_param_8];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r217, %r1, 2;
	mov.u32 	%r218, FluffyRound_C3$__cuda_local_var_207964_30_non_const_ecounters;
	add.s32 	%r219, %r218, %r217;
	mov.u32 	%r1297, 0;
	st.shared.u32 	[%r219], %r1297;
	st.shared.u32 	[%r219+4096], %r1297;
	st.shared.u32 	[%r219+8192], %r1297;
	st.shared.u32 	[%r219+12288], %r1297;
	st.shared.u32 	[%r219+16384], %r1297;
	st.shared.u32 	[%r219+20480], %r1297;
	st.shared.u32 	[%r219+24576], %r1297;
	st.shared.u32 	[%r219+28672], %r1297;
	mov.u32 	%r221, %ctaid.x;
	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.s32 	%rd10, %r221, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%r222, [%rd11];
	min.s32 	%r2, %r222, %r216;
	add.s32 	%r223, %r221, 4096;
	mul.wide.s32 	%rd12, %r223, 4;
	add.s64 	%rd13, %rd9, %rd12;
	ld.global.u32 	%r224, [%rd13];
	min.s32 	%r3, %r224, %r216;
	add.s32 	%r225, %r221, 8192;
	mul.wide.s32 	%rd14, %r225, 4;
	add.s64 	%rd15, %rd9, %rd14;
	ld.global.u32 	%r226, [%rd15];
	min.s32 	%r4, %r226, %r216;
	add.s32 	%r227, %r221, 12288;
	mul.wide.s32 	%rd16, %r227, 4;
	add.s64 	%rd17, %rd9, %rd16;
	ld.global.u32 	%r228, [%rd17];
	min.s32 	%r5, %r228, %r216;
	add.s32 	%r6, %r2, 1024;
	shr.s32 	%r229, %r6, 31;
	shr.u32 	%r230, %r229, 22;
	add.s32 	%r231, %r6, %r230;
	shr.s32 	%r7, %r231, 10;
	add.s32 	%r8, %r3, 1024;
	shr.s32 	%r232, %r8, 31;
	shr.u32 	%r233, %r232, 22;
	add.s32 	%r234, %r8, %r233;
	shr.s32 	%r9, %r234, 10;
	add.s32 	%r10, %r4, 1024;
	shr.s32 	%r235, %r10, 31;
	shr.u32 	%r236, %r235, 22;
	add.s32 	%r237, %r10, %r236;
	shr.s32 	%r11, %r237, 10;
	add.s32 	%r12, %r5, 1024;
	shr.s32 	%r238, %r12, 31;
	shr.u32 	%r239, %r238, 22;
	add.s32 	%r240, %r12, %r239;
	shr.s32 	%r13, %r240, 10;
	mul.lo.s32 	%r14, %r221, %r216;
	bar.sync 	0;
	setp.lt.s32	%p2, %r6, 1024;
	@%p2 bra 	BB7_30;

	mov.u32 	%r245, 1;
	max.s32 	%r15, %r7, %r245;
	and.b32  	%r244, %r15, 3;
	mov.u32 	%r1269, 0;
	setp.eq.s32	%p3, %r244, 0;
	@%p3 bra 	BB7_16;

	setp.eq.s32	%p4, %r244, 1;
	@%p4 bra 	BB7_12;

	setp.eq.s32	%p5, %r244, 2;
	@%p5 bra 	BB7_8;

	setp.ge.s32	%p6, %r1, %r2;
	@%p6 bra 	BB7_5;

	add.s32 	%r250, %r1, %r14;
	mul.wide.s32 	%rd19, %r250, 8;
	add.s64 	%rd18, %rd2, %rd19;
	// inline asm
	ld.global.cs.v2.u32 {%r247,%r248}, [%rd18];
	// inline asm
	or.b32  	%r251, %r247, %r248;
	setp.eq.s32	%p7, %r251, 0;
	mov.u32 	%r1269, %r245;
	@%p7 bra 	BB7_8;

	and.b32  	%r253, %r247, 131040;
	and.b32  	%r254, %r247, 31;
	mov.u32 	%r1269, 1;
	shl.b32 	%r255, %r1269, %r254;
	shr.u32 	%r256, %r253, 3;
	add.s32 	%r258, %r218, %r256;
	atom.shared.or.b32 	%r259, [%r258], %r255;
	bra.uni 	BB7_8;

BB7_5:
	mov.u32 	%r1269, %r245;

BB7_8:
	shl.b32 	%r260, %r1269, 10;
	add.s32 	%r18, %r260, %r1;
	setp.ge.s32	%p8, %r18, %r2;
	@%p8 bra 	BB7_11;

	add.s32 	%r263, %r18, %r14;
	mul.wide.s32 	%rd21, %r263, 8;
	add.s64 	%rd20, %rd2, %rd21;
	// inline asm
	ld.global.cs.v2.u32 {%r261,%r262}, [%rd20];
	// inline asm
	or.b32  	%r264, %r261, %r262;
	setp.eq.s32	%p9, %r264, 0;
	@%p9 bra 	BB7_11;

	and.b32  	%r265, %r261, 131040;
	and.b32  	%r266, %r261, 31;
	mov.u32 	%r267, 1;
	shl.b32 	%r268, %r267, %r266;
	shr.u32 	%r269, %r265, 3;
	add.s32 	%r271, %r218, %r269;
	atom.shared.or.b32 	%r272, [%r271], %r268;

BB7_11:
	add.s32 	%r1269, %r1269, 1;

BB7_12:
	shl.b32 	%r273, %r1269, 10;
	add.s32 	%r22, %r273, %r1;
	setp.ge.s32	%p10, %r22, %r2;
	@%p10 bra 	BB7_15;

	add.s32 	%r276, %r22, %r14;
	mul.wide.s32 	%rd23, %r276, 8;
	add.s64 	%rd22, %rd2, %rd23;
	// inline asm
	ld.global.cs.v2.u32 {%r274,%r275}, [%rd22];
	// inline asm
	or.b32  	%r277, %r274, %r275;
	setp.eq.s32	%p11, %r277, 0;
	@%p11 bra 	BB7_15;

	and.b32  	%r278, %r274, 131040;
	and.b32  	%r279, %r274, 31;
	mov.u32 	%r280, 1;
	shl.b32 	%r281, %r280, %r279;
	shr.u32 	%r282, %r278, 3;
	add.s32 	%r284, %r218, %r282;
	atom.shared.or.b32 	%r285, [%r284], %r281;

BB7_15:
	add.s32 	%r1269, %r1269, 1;

BB7_16:
	setp.lt.u32	%p12, %r15, 4;
	@%p12 bra 	BB7_30;

BB7_17:
	shl.b32 	%r286, %r1269, 10;
	add.s32 	%r27, %r286, %r1;
	setp.ge.s32	%p13, %r27, %r2;
	@%p13 bra 	BB7_20;

	add.s32 	%r289, %r27, %r14;
	mul.wide.s32 	%rd25, %r289, 8;
	add.s64 	%rd24, %rd2, %rd25;
	// inline asm
	ld.global.cs.v2.u32 {%r287,%r288}, [%rd24];
	// inline asm
	or.b32  	%r290, %r287, %r288;
	setp.eq.s32	%p14, %r290, 0;
	@%p14 bra 	BB7_20;

	and.b32  	%r291, %r287, 131040;
	and.b32  	%r292, %r287, 31;
	mov.u32 	%r293, 1;
	shl.b32 	%r294, %r293, %r292;
	shr.u32 	%r295, %r291, 3;
	add.s32 	%r297, %r218, %r295;
	atom.shared.or.b32 	%r298, [%r297], %r294;

BB7_20:
	add.s32 	%r29, %r27, 1024;
	setp.ge.s32	%p15, %r29, %r2;
	@%p15 bra 	BB7_23;

	add.s32 	%r303, %r29, %r14;
	mul.wide.s32 	%rd27, %r303, 8;
	add.s64 	%rd26, %rd2, %rd27;
	// inline asm
	ld.global.cs.v2.u32 {%r301,%r302}, [%rd26];
	// inline asm
	or.b32  	%r304, %r301, %r302;
	setp.eq.s32	%p16, %r304, 0;
	@%p16 bra 	BB7_23;

	and.b32  	%r305, %r301, 131040;
	and.b32  	%r306, %r301, 31;
	mov.u32 	%r307, 1;
	shl.b32 	%r308, %r307, %r306;
	shr.u32 	%r309, %r305, 3;
	add.s32 	%r311, %r218, %r309;
	atom.shared.or.b32 	%r312, [%r311], %r308;

BB7_23:
	add.s32 	%r31, %r27, 2048;
	setp.ge.s32	%p17, %r31, %r2;
	@%p17 bra 	BB7_26;

	add.s32 	%r317, %r31, %r14;
	mul.wide.s32 	%rd29, %r317, 8;
	add.s64 	%rd28, %rd2, %rd29;
	// inline asm
	ld.global.cs.v2.u32 {%r315,%r316}, [%rd28];
	// inline asm
	or.b32  	%r318, %r315, %r316;
	setp.eq.s32	%p18, %r318, 0;
	@%p18 bra 	BB7_26;

	and.b32  	%r319, %r315, 131040;
	and.b32  	%r320, %r315, 31;
	mov.u32 	%r321, 1;
	shl.b32 	%r322, %r321, %r320;
	shr.u32 	%r323, %r319, 3;
	add.s32 	%r325, %r218, %r323;
	atom.shared.or.b32 	%r326, [%r325], %r322;

BB7_26:
	add.s32 	%r33, %r27, 3072;
	setp.ge.s32	%p19, %r33, %r2;
	@%p19 bra 	BB7_29;

	add.s32 	%r331, %r33, %r14;
	mul.wide.s32 	%rd31, %r331, 8;
	add.s64 	%rd30, %rd2, %rd31;
	// inline asm
	ld.global.cs.v2.u32 {%r329,%r330}, [%rd30];
	// inline asm
	or.b32  	%r332, %r329, %r330;
	setp.eq.s32	%p20, %r332, 0;
	@%p20 bra 	BB7_29;

	and.b32  	%r333, %r329, 131040;
	and.b32  	%r334, %r329, 31;
	mov.u32 	%r335, 1;
	shl.b32 	%r336, %r335, %r334;
	shr.u32 	%r337, %r333, 3;
	add.s32 	%r339, %r218, %r337;
	atom.shared.or.b32 	%r340, [%r339], %r336;

BB7_29:
	add.s32 	%r1269, %r1269, 4;
	setp.lt.s32	%p21, %r1269, %r7;
	@%p21 bra 	BB7_17;

BB7_30:
	setp.lt.s32	%p22, %r8, 1024;
	@%p22 bra 	BB7_60;

	mov.u32 	%r345, 1;
	max.s32 	%r36, %r9, %r345;
	and.b32  	%r344, %r36, 3;
	mov.u32 	%r1273, 0;
	setp.eq.s32	%p23, %r344, 0;
	@%p23 bra 	BB7_46;

	setp.eq.s32	%p24, %r344, 1;
	@%p24 bra 	BB7_42;

	setp.eq.s32	%p25, %r344, 2;
	@%p25 bra 	BB7_38;

	setp.ge.s32	%p26, %r1, %r3;
	@%p26 bra 	BB7_35;

	add.s32 	%r350, %r1, %r14;
	mul.wide.s32 	%rd33, %r350, 8;
	add.s64 	%rd32, %rd3, %rd33;
	// inline asm
	ld.global.cs.v2.u32 {%r347,%r348}, [%rd32];
	// inline asm
	or.b32  	%r351, %r347, %r348;
	setp.eq.s32	%p27, %r351, 0;
	mov.u32 	%r1273, %r345;
	@%p27 bra 	BB7_38;

	and.b32  	%r353, %r347, 131040;
	and.b32  	%r354, %r347, 31;
	mov.u32 	%r1273, 1;
	shl.b32 	%r355, %r1273, %r354;
	shr.u32 	%r356, %r353, 3;
	add.s32 	%r358, %r218, %r356;
	atom.shared.or.b32 	%r359, [%r358], %r355;
	bra.uni 	BB7_38;

BB7_35:
	mov.u32 	%r1273, %r345;

BB7_38:
	shl.b32 	%r360, %r1273, 10;
	add.s32 	%r39, %r360, %r1;
	setp.ge.s32	%p28, %r39, %r3;
	@%p28 bra 	BB7_41;

	add.s32 	%r363, %r39, %r14;
	mul.wide.s32 	%rd35, %r363, 8;
	add.s64 	%rd34, %rd3, %rd35;
	// inline asm
	ld.global.cs.v2.u32 {%r361,%r362}, [%rd34];
	// inline asm
	or.b32  	%r364, %r361, %r362;
	setp.eq.s32	%p29, %r364, 0;
	@%p29 bra 	BB7_41;

	and.b32  	%r365, %r361, 131040;
	and.b32  	%r366, %r361, 31;
	mov.u32 	%r367, 1;
	shl.b32 	%r368, %r367, %r366;
	shr.u32 	%r369, %r365, 3;
	add.s32 	%r371, %r218, %r369;
	atom.shared.or.b32 	%r372, [%r371], %r368;

BB7_41:
	add.s32 	%r1273, %r1273, 1;

BB7_42:
	shl.b32 	%r373, %r1273, 10;
	add.s32 	%r43, %r373, %r1;
	setp.ge.s32	%p30, %r43, %r3;
	@%p30 bra 	BB7_45;

	add.s32 	%r376, %r43, %r14;
	mul.wide.s32 	%rd37, %r376, 8;
	add.s64 	%rd36, %rd3, %rd37;
	// inline asm
	ld.global.cs.v2.u32 {%r374,%r375}, [%rd36];
	// inline asm
	or.b32  	%r377, %r374, %r375;
	setp.eq.s32	%p31, %r377, 0;
	@%p31 bra 	BB7_45;

	and.b32  	%r378, %r374, 131040;
	and.b32  	%r379, %r374, 31;
	mov.u32 	%r380, 1;
	shl.b32 	%r381, %r380, %r379;
	shr.u32 	%r382, %r378, 3;
	add.s32 	%r384, %r218, %r382;
	atom.shared.or.b32 	%r385, [%r384], %r381;

BB7_45:
	add.s32 	%r1273, %r1273, 1;

BB7_46:
	setp.lt.u32	%p32, %r36, 4;
	@%p32 bra 	BB7_60;

BB7_47:
	shl.b32 	%r386, %r1273, 10;
	add.s32 	%r48, %r386, %r1;
	setp.ge.s32	%p33, %r48, %r3;
	@%p33 bra 	BB7_50;

	add.s32 	%r389, %r48, %r14;
	mul.wide.s32 	%rd39, %r389, 8;
	add.s64 	%rd38, %rd3, %rd39;
	// inline asm
	ld.global.cs.v2.u32 {%r387,%r388}, [%rd38];
	// inline asm
	or.b32  	%r390, %r387, %r388;
	setp.eq.s32	%p34, %r390, 0;
	@%p34 bra 	BB7_50;

	and.b32  	%r391, %r387, 131040;
	and.b32  	%r392, %r387, 31;
	mov.u32 	%r393, 1;
	shl.b32 	%r394, %r393, %r392;
	shr.u32 	%r395, %r391, 3;
	add.s32 	%r397, %r218, %r395;
	atom.shared.or.b32 	%r398, [%r397], %r394;

BB7_50:
	add.s32 	%r50, %r48, 1024;
	setp.ge.s32	%p35, %r50, %r3;
	@%p35 bra 	BB7_53;

	add.s32 	%r403, %r50, %r14;
	mul.wide.s32 	%rd41, %r403, 8;
	add.s64 	%rd40, %rd3, %rd41;
	// inline asm
	ld.global.cs.v2.u32 {%r401,%r402}, [%rd40];
	// inline asm
	or.b32  	%r404, %r401, %r402;
	setp.eq.s32	%p36, %r404, 0;
	@%p36 bra 	BB7_53;

	and.b32  	%r405, %r401, 131040;
	and.b32  	%r406, %r401, 31;
	mov.u32 	%r407, 1;
	shl.b32 	%r408, %r407, %r406;
	shr.u32 	%r409, %r405, 3;
	add.s32 	%r411, %r218, %r409;
	atom.shared.or.b32 	%r412, [%r411], %r408;

BB7_53:
	add.s32 	%r52, %r48, 2048;
	setp.ge.s32	%p37, %r52, %r3;
	@%p37 bra 	BB7_56;

	add.s32 	%r417, %r52, %r14;
	mul.wide.s32 	%rd43, %r417, 8;
	add.s64 	%rd42, %rd3, %rd43;
	// inline asm
	ld.global.cs.v2.u32 {%r415,%r416}, [%rd42];
	// inline asm
	or.b32  	%r418, %r415, %r416;
	setp.eq.s32	%p38, %r418, 0;
	@%p38 bra 	BB7_56;

	and.b32  	%r419, %r415, 131040;
	and.b32  	%r420, %r415, 31;
	mov.u32 	%r421, 1;
	shl.b32 	%r422, %r421, %r420;
	shr.u32 	%r423, %r419, 3;
	add.s32 	%r425, %r218, %r423;
	atom.shared.or.b32 	%r426, [%r425], %r422;

BB7_56:
	add.s32 	%r54, %r48, 3072;
	setp.ge.s32	%p39, %r54, %r3;
	@%p39 bra 	BB7_59;

	add.s32 	%r431, %r54, %r14;
	mul.wide.s32 	%rd45, %r431, 8;
	add.s64 	%rd44, %rd3, %rd45;
	// inline asm
	ld.global.cs.v2.u32 {%r429,%r430}, [%rd44];
	// inline asm
	or.b32  	%r432, %r429, %r430;
	setp.eq.s32	%p40, %r432, 0;
	@%p40 bra 	BB7_59;

	and.b32  	%r433, %r429, 131040;
	and.b32  	%r434, %r429, 31;
	mov.u32 	%r435, 1;
	shl.b32 	%r436, %r435, %r434;
	shr.u32 	%r437, %r433, 3;
	add.s32 	%r439, %r218, %r437;
	atom.shared.or.b32 	%r440, [%r439], %r436;

BB7_59:
	add.s32 	%r1273, %r1273, 4;
	setp.lt.s32	%p41, %r1273, %r9;
	@%p41 bra 	BB7_47;

BB7_60:
	setp.lt.s32	%p42, %r10, 1024;
	@%p42 bra 	BB7_90;

	mov.u32 	%r445, 1;
	max.s32 	%r57, %r11, %r445;
	and.b32  	%r444, %r57, 3;
	mov.u32 	%r1277, 0;
	setp.eq.s32	%p43, %r444, 0;
	@%p43 bra 	BB7_76;

	setp.eq.s32	%p44, %r444, 1;
	@%p44 bra 	BB7_72;

	setp.eq.s32	%p45, %r444, 2;
	@%p45 bra 	BB7_68;

	setp.ge.s32	%p46, %r1, %r4;
	@%p46 bra 	BB7_65;

	add.s32 	%r450, %r1, %r14;
	mul.wide.s32 	%rd47, %r450, 8;
	add.s64 	%rd46, %rd4, %rd47;
	// inline asm
	ld.global.cs.v2.u32 {%r447,%r448}, [%rd46];
	// inline asm
	or.b32  	%r451, %r447, %r448;
	setp.eq.s32	%p47, %r451, 0;
	mov.u32 	%r1277, %r445;
	@%p47 bra 	BB7_68;

	and.b32  	%r453, %r447, 131040;
	and.b32  	%r454, %r447, 31;
	mov.u32 	%r1277, 1;
	shl.b32 	%r455, %r1277, %r454;
	shr.u32 	%r456, %r453, 3;
	add.s32 	%r458, %r218, %r456;
	atom.shared.or.b32 	%r459, [%r458], %r455;
	bra.uni 	BB7_68;

BB7_65:
	mov.u32 	%r1277, %r445;

BB7_68:
	shl.b32 	%r460, %r1277, 10;
	add.s32 	%r60, %r460, %r1;
	setp.ge.s32	%p48, %r60, %r4;
	@%p48 bra 	BB7_71;

	add.s32 	%r463, %r60, %r14;
	mul.wide.s32 	%rd49, %r463, 8;
	add.s64 	%rd48, %rd4, %rd49;
	// inline asm
	ld.global.cs.v2.u32 {%r461,%r462}, [%rd48];
	// inline asm
	or.b32  	%r464, %r461, %r462;
	setp.eq.s32	%p49, %r464, 0;
	@%p49 bra 	BB7_71;

	and.b32  	%r465, %r461, 131040;
	and.b32  	%r466, %r461, 31;
	mov.u32 	%r467, 1;
	shl.b32 	%r468, %r467, %r466;
	shr.u32 	%r469, %r465, 3;
	add.s32 	%r471, %r218, %r469;
	atom.shared.or.b32 	%r472, [%r471], %r468;

BB7_71:
	add.s32 	%r1277, %r1277, 1;

BB7_72:
	shl.b32 	%r473, %r1277, 10;
	add.s32 	%r64, %r473, %r1;
	setp.ge.s32	%p50, %r64, %r4;
	@%p50 bra 	BB7_75;

	add.s32 	%r476, %r64, %r14;
	mul.wide.s32 	%rd51, %r476, 8;
	add.s64 	%rd50, %rd4, %rd51;
	// inline asm
	ld.global.cs.v2.u32 {%r474,%r475}, [%rd50];
	// inline asm
	or.b32  	%r477, %r474, %r475;
	setp.eq.s32	%p51, %r477, 0;
	@%p51 bra 	BB7_75;

	and.b32  	%r478, %r474, 131040;
	and.b32  	%r479, %r474, 31;
	mov.u32 	%r480, 1;
	shl.b32 	%r481, %r480, %r479;
	shr.u32 	%r482, %r478, 3;
	add.s32 	%r484, %r218, %r482;
	atom.shared.or.b32 	%r485, [%r484], %r481;

BB7_75:
	add.s32 	%r1277, %r1277, 1;

BB7_76:
	setp.lt.u32	%p52, %r57, 4;
	@%p52 bra 	BB7_90;

BB7_77:
	shl.b32 	%r486, %r1277, 10;
	add.s32 	%r69, %r486, %r1;
	setp.ge.s32	%p53, %r69, %r4;
	@%p53 bra 	BB7_80;

	add.s32 	%r489, %r69, %r14;
	mul.wide.s32 	%rd53, %r489, 8;
	add.s64 	%rd52, %rd4, %rd53;
	// inline asm
	ld.global.cs.v2.u32 {%r487,%r488}, [%rd52];
	// inline asm
	or.b32  	%r490, %r487, %r488;
	setp.eq.s32	%p54, %r490, 0;
	@%p54 bra 	BB7_80;

	and.b32  	%r491, %r487, 131040;
	and.b32  	%r492, %r487, 31;
	mov.u32 	%r493, 1;
	shl.b32 	%r494, %r493, %r492;
	shr.u32 	%r495, %r491, 3;
	add.s32 	%r497, %r218, %r495;
	atom.shared.or.b32 	%r498, [%r497], %r494;

BB7_80:
	add.s32 	%r71, %r69, 1024;
	setp.ge.s32	%p55, %r71, %r4;
	@%p55 bra 	BB7_83;

	add.s32 	%r503, %r71, %r14;
	mul.wide.s32 	%rd55, %r503, 8;
	add.s64 	%rd54, %rd4, %rd55;
	// inline asm
	ld.global.cs.v2.u32 {%r501,%r502}, [%rd54];
	// inline asm
	or.b32  	%r504, %r501, %r502;
	setp.eq.s32	%p56, %r504, 0;
	@%p56 bra 	BB7_83;

	and.b32  	%r505, %r501, 131040;
	and.b32  	%r506, %r501, 31;
	mov.u32 	%r507, 1;
	shl.b32 	%r508, %r507, %r506;
	shr.u32 	%r509, %r505, 3;
	add.s32 	%r511, %r218, %r509;
	atom.shared.or.b32 	%r512, [%r511], %r508;

BB7_83:
	add.s32 	%r73, %r69, 2048;
	setp.ge.s32	%p57, %r73, %r4;
	@%p57 bra 	BB7_86;

	add.s32 	%r517, %r73, %r14;
	mul.wide.s32 	%rd57, %r517, 8;
	add.s64 	%rd56, %rd4, %rd57;
	// inline asm
	ld.global.cs.v2.u32 {%r515,%r516}, [%rd56];
	// inline asm
	or.b32  	%r518, %r515, %r516;
	setp.eq.s32	%p58, %r518, 0;
	@%p58 bra 	BB7_86;

	and.b32  	%r519, %r515, 131040;
	and.b32  	%r520, %r515, 31;
	mov.u32 	%r521, 1;
	shl.b32 	%r522, %r521, %r520;
	shr.u32 	%r523, %r519, 3;
	add.s32 	%r525, %r218, %r523;
	atom.shared.or.b32 	%r526, [%r525], %r522;

BB7_86:
	add.s32 	%r75, %r69, 3072;
	setp.ge.s32	%p59, %r75, %r4;
	@%p59 bra 	BB7_89;

	add.s32 	%r531, %r75, %r14;
	mul.wide.s32 	%rd59, %r531, 8;
	add.s64 	%rd58, %rd4, %rd59;
	// inline asm
	ld.global.cs.v2.u32 {%r529,%r530}, [%rd58];
	// inline asm
	or.b32  	%r532, %r529, %r530;
	setp.eq.s32	%p60, %r532, 0;
	@%p60 bra 	BB7_89;

	and.b32  	%r533, %r529, 131040;
	and.b32  	%r534, %r529, 31;
	mov.u32 	%r535, 1;
	shl.b32 	%r536, %r535, %r534;
	shr.u32 	%r537, %r533, 3;
	add.s32 	%r539, %r218, %r537;
	atom.shared.or.b32 	%r540, [%r539], %r536;

BB7_89:
	add.s32 	%r1277, %r1277, 4;
	setp.lt.s32	%p61, %r1277, %r11;
	@%p61 bra 	BB7_77;

BB7_90:
	setp.lt.s32	%p62, %r12, 1024;
	@%p62 bra 	BB7_120;

	mov.u32 	%r545, 1;
	max.s32 	%r78, %r13, %r545;
	and.b32  	%r544, %r78, 3;
	mov.u32 	%r1281, 0;
	setp.eq.s32	%p63, %r544, 0;
	@%p63 bra 	BB7_106;

	setp.eq.s32	%p64, %r544, 1;
	@%p64 bra 	BB7_102;

	setp.eq.s32	%p65, %r544, 2;
	@%p65 bra 	BB7_98;

	setp.ge.s32	%p66, %r1, %r5;
	@%p66 bra 	BB7_95;

	add.s32 	%r550, %r1, %r14;
	mul.wide.s32 	%rd61, %r550, 8;
	add.s64 	%rd60, %rd5, %rd61;
	// inline asm
	ld.global.cs.v2.u32 {%r547,%r548}, [%rd60];
	// inline asm
	or.b32  	%r551, %r547, %r548;
	setp.eq.s32	%p67, %r551, 0;
	mov.u32 	%r1281, %r545;
	@%p67 bra 	BB7_98;

	and.b32  	%r553, %r547, 131040;
	and.b32  	%r554, %r547, 31;
	mov.u32 	%r1281, 1;
	shl.b32 	%r555, %r1281, %r554;
	shr.u32 	%r556, %r553, 3;
	add.s32 	%r558, %r218, %r556;
	atom.shared.or.b32 	%r559, [%r558], %r555;
	bra.uni 	BB7_98;

BB7_95:
	mov.u32 	%r1281, %r545;

BB7_98:
	shl.b32 	%r560, %r1281, 10;
	add.s32 	%r81, %r560, %r1;
	setp.ge.s32	%p68, %r81, %r5;
	@%p68 bra 	BB7_101;

	add.s32 	%r563, %r81, %r14;
	mul.wide.s32 	%rd63, %r563, 8;
	add.s64 	%rd62, %rd5, %rd63;
	// inline asm
	ld.global.cs.v2.u32 {%r561,%r562}, [%rd62];
	// inline asm
	or.b32  	%r564, %r561, %r562;
	setp.eq.s32	%p69, %r564, 0;
	@%p69 bra 	BB7_101;

	and.b32  	%r565, %r561, 131040;
	and.b32  	%r566, %r561, 31;
	mov.u32 	%r567, 1;
	shl.b32 	%r568, %r567, %r566;
	shr.u32 	%r569, %r565, 3;
	add.s32 	%r571, %r218, %r569;
	atom.shared.or.b32 	%r572, [%r571], %r568;

BB7_101:
	add.s32 	%r1281, %r1281, 1;

BB7_102:
	shl.b32 	%r573, %r1281, 10;
	add.s32 	%r85, %r573, %r1;
	setp.ge.s32	%p70, %r85, %r5;
	@%p70 bra 	BB7_105;

	add.s32 	%r576, %r85, %r14;
	mul.wide.s32 	%rd65, %r576, 8;
	add.s64 	%rd64, %rd5, %rd65;
	// inline asm
	ld.global.cs.v2.u32 {%r574,%r575}, [%rd64];
	// inline asm
	or.b32  	%r577, %r574, %r575;
	setp.eq.s32	%p71, %r577, 0;
	@%p71 bra 	BB7_105;

	and.b32  	%r578, %r574, 131040;
	and.b32  	%r579, %r574, 31;
	mov.u32 	%r580, 1;
	shl.b32 	%r581, %r580, %r579;
	shr.u32 	%r582, %r578, 3;
	add.s32 	%r584, %r218, %r582;
	atom.shared.or.b32 	%r585, [%r584], %r581;

BB7_105:
	add.s32 	%r1281, %r1281, 1;

BB7_106:
	setp.lt.u32	%p72, %r78, 4;
	@%p72 bra 	BB7_120;

BB7_107:
	shl.b32 	%r586, %r1281, 10;
	add.s32 	%r90, %r586, %r1;
	setp.ge.s32	%p73, %r90, %r5;
	@%p73 bra 	BB7_110;

	add.s32 	%r589, %r90, %r14;
	mul.wide.s32 	%rd67, %r589, 8;
	add.s64 	%rd66, %rd5, %rd67;
	// inline asm
	ld.global.cs.v2.u32 {%r587,%r588}, [%rd66];
	// inline asm
	or.b32  	%r590, %r587, %r588;
	setp.eq.s32	%p74, %r590, 0;
	@%p74 bra 	BB7_110;

	and.b32  	%r591, %r587, 131040;
	and.b32  	%r592, %r587, 31;
	mov.u32 	%r593, 1;
	shl.b32 	%r594, %r593, %r592;
	shr.u32 	%r595, %r591, 3;
	add.s32 	%r597, %r218, %r595;
	atom.shared.or.b32 	%r598, [%r597], %r594;

BB7_110:
	add.s32 	%r92, %r90, 1024;
	setp.ge.s32	%p75, %r92, %r5;
	@%p75 bra 	BB7_113;

	add.s32 	%r603, %r92, %r14;
	mul.wide.s32 	%rd69, %r603, 8;
	add.s64 	%rd68, %rd5, %rd69;
	// inline asm
	ld.global.cs.v2.u32 {%r601,%r602}, [%rd68];
	// inline asm
	or.b32  	%r604, %r601, %r602;
	setp.eq.s32	%p76, %r604, 0;
	@%p76 bra 	BB7_113;

	and.b32  	%r605, %r601, 131040;
	and.b32  	%r606, %r601, 31;
	mov.u32 	%r607, 1;
	shl.b32 	%r608, %r607, %r606;
	shr.u32 	%r609, %r605, 3;
	add.s32 	%r611, %r218, %r609;
	atom.shared.or.b32 	%r612, [%r611], %r608;

BB7_113:
	add.s32 	%r94, %r90, 2048;
	setp.ge.s32	%p77, %r94, %r5;
	@%p77 bra 	BB7_116;

	add.s32 	%r617, %r94, %r14;
	mul.wide.s32 	%rd71, %r617, 8;
	add.s64 	%rd70, %rd5, %rd71;
	// inline asm
	ld.global.cs.v2.u32 {%r615,%r616}, [%rd70];
	// inline asm
	or.b32  	%r618, %r615, %r616;
	setp.eq.s32	%p78, %r618, 0;
	@%p78 bra 	BB7_116;

	and.b32  	%r619, %r615, 131040;
	and.b32  	%r620, %r615, 31;
	mov.u32 	%r621, 1;
	shl.b32 	%r622, %r621, %r620;
	shr.u32 	%r623, %r619, 3;
	add.s32 	%r625, %r218, %r623;
	atom.shared.or.b32 	%r626, [%r625], %r622;

BB7_116:
	add.s32 	%r96, %r90, 3072;
	setp.ge.s32	%p79, %r96, %r5;
	@%p79 bra 	BB7_119;

	add.s32 	%r631, %r96, %r14;
	mul.wide.s32 	%rd73, %r631, 8;
	add.s64 	%rd72, %rd5, %rd73;
	// inline asm
	ld.global.cs.v2.u32 {%r629,%r630}, [%rd72];
	// inline asm
	or.b32  	%r632, %r629, %r630;
	setp.eq.s32	%p80, %r632, 0;
	@%p80 bra 	BB7_119;

	and.b32  	%r633, %r629, 131040;
	and.b32  	%r634, %r629, 31;
	mov.u32 	%r635, 1;
	shl.b32 	%r636, %r635, %r634;
	shr.u32 	%r637, %r633, 3;
	add.s32 	%r639, %r218, %r637;
	atom.shared.or.b32 	%r640, [%r639], %r636;

BB7_119:
	add.s32 	%r1281, %r1281, 4;
	setp.lt.s32	%p81, %r1281, %r13;
	@%p81 bra 	BB7_107;

BB7_120:
	setp.gt.s32	%p1, %r6, 1023;
	bar.sync 	0;
	@!%p1 bra 	BB7_157;
	bra.uni 	BB7_121;

BB7_121:
	add.s32 	%r99, %r215, -2;
	mov.u32 	%r645, 1;
	max.s32 	%r100, %r7, %r645;
	and.b32  	%r644, %r100, 3;
	mov.u32 	%r1285, 0;
	setp.eq.s32	%p82, %r644, 0;
	@%p82 bra 	BB7_139;

	setp.eq.s32	%p83, %r644, 1;
	@%p83 bra 	BB7_134;

	setp.eq.s32	%p84, %r644, 2;
	@%p84 bra 	BB7_129;

	setp.ge.s32	%p85, %r1, %r2;
	@%p85 bra 	BB7_125;

	add.s32 	%r650, %r1, %r14;
	mul.wide.s32 	%rd75, %r650, 8;
	add.s64 	%rd74, %rd2, %rd75;
	// inline asm
	ld.global.cs.v2.u32 {%r647,%r648}, [%rd74];
	// inline asm
	or.b32  	%r651, %r647, %r648;
	setp.eq.s32	%p86, %r651, 0;
	mov.u32 	%r1285, %r645;
	@%p86 bra 	BB7_129;

	and.b32  	%r653, %r647, 131040;
	and.b32  	%r654, %r647, 31;
	xor.b32  	%r655, %r654, 1;
	shr.u32 	%r656, %r653, 3;
	add.s32 	%r658, %r218, %r656;
	mov.u32 	%r1285, 1;
	shl.b32 	%r659, %r1285, %r655;
	ld.shared.u32 	%r660, [%r658];
	and.b32  	%r661, %r660, %r659;
	setp.eq.s32	%p87, %r661, 0;
	@%p87 bra 	BB7_129;

	bfe.u32 	%r665, %r648, 17, 12;
	mul.wide.u32 	%rd77, %r665, 4;
	add.s64 	%rd78, %rd1, %rd77;
	atom.global.add.u32 	%r666, [%rd78], 1;
	min.s32 	%r667, %r666, %r99;
	mad.lo.s32 	%r668, %r665, %r215, %r667;
	mul.wide.s32 	%rd79, %r668, 8;
	add.s64 	%rd76, %rd6, %rd79;
	// inline asm
	st.global.cg.v2.u32 [%rd76], {%r648, %r647};
	// inline asm
	bra.uni 	BB7_129;

BB7_125:
	mov.u32 	%r1285, %r645;

BB7_129:
	shl.b32 	%r669, %r1285, 10;
	add.s32 	%r104, %r669, %r1;
	setp.ge.s32	%p88, %r104, %r2;
	@%p88 bra 	BB7_133;

	add.s32 	%r672, %r104, %r14;
	mul.wide.s32 	%rd81, %r672, 8;
	add.s64 	%rd80, %rd2, %rd81;
	// inline asm
	ld.global.cs.v2.u32 {%r670,%r671}, [%rd80];
	// inline asm
	or.b32  	%r673, %r670, %r671;
	setp.eq.s32	%p89, %r673, 0;
	@%p89 bra 	BB7_133;

	and.b32  	%r674, %r670, 131040;
	and.b32  	%r675, %r670, 31;
	xor.b32  	%r676, %r675, 1;
	shr.u32 	%r677, %r674, 3;
	add.s32 	%r679, %r218, %r677;
	mov.u32 	%r680, 1;
	shl.b32 	%r681, %r680, %r676;
	ld.shared.u32 	%r682, [%r679];
	and.b32  	%r683, %r682, %r681;
	setp.eq.s32	%p90, %r683, 0;
	@%p90 bra 	BB7_133;

	bfe.u32 	%r686, %r671, 17, 12;
	mul.wide.u32 	%rd83, %r686, 4;
	add.s64 	%rd84, %rd1, %rd83;
	atom.global.add.u32 	%r687, [%rd84], 1;
	min.s32 	%r688, %r687, %r99;
	mad.lo.s32 	%r689, %r686, %r215, %r688;
	mul.wide.s32 	%rd85, %r689, 8;
	add.s64 	%rd82, %rd6, %rd85;
	// inline asm
	st.global.cg.v2.u32 [%rd82], {%r671, %r670};
	// inline asm

BB7_133:
	add.s32 	%r1285, %r1285, 1;

BB7_134:
	shl.b32 	%r690, %r1285, 10;
	add.s32 	%r109, %r690, %r1;
	setp.ge.s32	%p91, %r109, %r2;
	@%p91 bra 	BB7_138;

	add.s32 	%r693, %r109, %r14;
	mul.wide.s32 	%rd87, %r693, 8;
	add.s64 	%rd86, %rd2, %rd87;
	// inline asm
	ld.global.cs.v2.u32 {%r691,%r692}, [%rd86];
	// inline asm
	or.b32  	%r694, %r691, %r692;
	setp.eq.s32	%p92, %r694, 0;
	@%p92 bra 	BB7_138;

	and.b32  	%r695, %r691, 131040;
	and.b32  	%r696, %r691, 31;
	xor.b32  	%r697, %r696, 1;
	shr.u32 	%r698, %r695, 3;
	add.s32 	%r700, %r218, %r698;
	mov.u32 	%r701, 1;
	shl.b32 	%r702, %r701, %r697;
	ld.shared.u32 	%r703, [%r700];
	and.b32  	%r704, %r703, %r702;
	setp.eq.s32	%p93, %r704, 0;
	@%p93 bra 	BB7_138;

	bfe.u32 	%r707, %r692, 17, 12;
	mul.wide.u32 	%rd89, %r707, 4;
	add.s64 	%rd90, %rd1, %rd89;
	atom.global.add.u32 	%r708, [%rd90], 1;
	min.s32 	%r709, %r708, %r99;
	mad.lo.s32 	%r710, %r707, %r215, %r709;
	mul.wide.s32 	%rd91, %r710, 8;
	add.s64 	%rd88, %rd6, %rd91;
	// inline asm
	st.global.cg.v2.u32 [%rd88], {%r692, %r691};
	// inline asm

BB7_138:
	add.s32 	%r1285, %r1285, 1;

BB7_139:
	setp.lt.u32	%p94, %r100, 4;
	@%p94 bra 	BB7_157;

BB7_140:
	shl.b32 	%r711, %r1285, 10;
	add.s32 	%r115, %r711, %r1;
	setp.ge.s32	%p95, %r115, %r2;
	@%p95 bra 	BB7_144;

	add.s32 	%r714, %r115, %r14;
	mul.wide.s32 	%rd93, %r714, 8;
	add.s64 	%rd92, %rd2, %rd93;
	// inline asm
	ld.global.cs.v2.u32 {%r712,%r713}, [%rd92];
	// inline asm
	or.b32  	%r715, %r712, %r713;
	setp.eq.s32	%p96, %r715, 0;
	@%p96 bra 	BB7_144;

	and.b32  	%r716, %r712, 131040;
	and.b32  	%r717, %r712, 31;
	xor.b32  	%r718, %r717, 1;
	shr.u32 	%r719, %r716, 3;
	add.s32 	%r721, %r218, %r719;
	mov.u32 	%r722, 1;
	shl.b32 	%r723, %r722, %r718;
	ld.shared.u32 	%r724, [%r721];
	and.b32  	%r725, %r724, %r723;
	setp.eq.s32	%p97, %r725, 0;
	@%p97 bra 	BB7_144;

	bfe.u32 	%r728, %r713, 17, 12;
	mul.wide.u32 	%rd95, %r728, 4;
	add.s64 	%rd96, %rd1, %rd95;
	atom.global.add.u32 	%r729, [%rd96], 1;
	min.s32 	%r730, %r729, %r99;
	mad.lo.s32 	%r731, %r728, %r215, %r730;
	mul.wide.s32 	%rd97, %r731, 8;
	add.s64 	%rd94, %rd6, %rd97;
	// inline asm
	st.global.cg.v2.u32 [%rd94], {%r713, %r712};
	// inline asm

BB7_144:
	add.s32 	%r118, %r115, 1024;
	setp.ge.s32	%p98, %r118, %r2;
	@%p98 bra 	BB7_148;

	add.s32 	%r736, %r118, %r14;
	mul.wide.s32 	%rd99, %r736, 8;
	add.s64 	%rd98, %rd2, %rd99;
	// inline asm
	ld.global.cs.v2.u32 {%r734,%r735}, [%rd98];
	// inline asm
	or.b32  	%r737, %r734, %r735;
	setp.eq.s32	%p99, %r737, 0;
	@%p99 bra 	BB7_148;

	and.b32  	%r738, %r734, 131040;
	and.b32  	%r739, %r734, 31;
	xor.b32  	%r740, %r739, 1;
	shr.u32 	%r741, %r738, 3;
	add.s32 	%r743, %r218, %r741;
	mov.u32 	%r744, 1;
	shl.b32 	%r745, %r744, %r740;
	ld.shared.u32 	%r746, [%r743];
	and.b32  	%r747, %r746, %r745;
	setp.eq.s32	%p100, %r747, 0;
	@%p100 bra 	BB7_148;

	bfe.u32 	%r750, %r735, 17, 12;
	mul.wide.u32 	%rd101, %r750, 4;
	add.s64 	%rd102, %rd1, %rd101;
	atom.global.add.u32 	%r751, [%rd102], 1;
	min.s32 	%r752, %r751, %r99;
	mad.lo.s32 	%r753, %r750, %r215, %r752;
	mul.wide.s32 	%rd103, %r753, 8;
	add.s64 	%rd100, %rd6, %rd103;
	// inline asm
	st.global.cg.v2.u32 [%rd100], {%r735, %r734};
	// inline asm

BB7_148:
	add.s32 	%r121, %r115, 2048;
	setp.ge.s32	%p101, %r121, %r2;
	@%p101 bra 	BB7_152;

	add.s32 	%r758, %r121, %r14;
	mul.wide.s32 	%rd105, %r758, 8;
	add.s64 	%rd104, %rd2, %rd105;
	// inline asm
	ld.global.cs.v2.u32 {%r756,%r757}, [%rd104];
	// inline asm
	or.b32  	%r759, %r756, %r757;
	setp.eq.s32	%p102, %r759, 0;
	@%p102 bra 	BB7_152;

	and.b32  	%r760, %r756, 131040;
	and.b32  	%r761, %r756, 31;
	xor.b32  	%r762, %r761, 1;
	shr.u32 	%r763, %r760, 3;
	add.s32 	%r765, %r218, %r763;
	mov.u32 	%r766, 1;
	shl.b32 	%r767, %r766, %r762;
	ld.shared.u32 	%r768, [%r765];
	and.b32  	%r769, %r768, %r767;
	setp.eq.s32	%p103, %r769, 0;
	@%p103 bra 	BB7_152;

	bfe.u32 	%r772, %r757, 17, 12;
	mul.wide.u32 	%rd107, %r772, 4;
	add.s64 	%rd108, %rd1, %rd107;
	atom.global.add.u32 	%r773, [%rd108], 1;
	min.s32 	%r774, %r773, %r99;
	mad.lo.s32 	%r775, %r772, %r215, %r774;
	mul.wide.s32 	%rd109, %r775, 8;
	add.s64 	%rd106, %rd6, %rd109;
	// inline asm
	st.global.cg.v2.u32 [%rd106], {%r757, %r756};
	// inline asm

BB7_152:
	add.s32 	%r124, %r115, 3072;
	setp.ge.s32	%p104, %r124, %r2;
	@%p104 bra 	BB7_156;

	add.s32 	%r780, %r124, %r14;
	mul.wide.s32 	%rd111, %r780, 8;
	add.s64 	%rd110, %rd2, %rd111;
	// inline asm
	ld.global.cs.v2.u32 {%r778,%r779}, [%rd110];
	// inline asm
	or.b32  	%r781, %r778, %r779;
	setp.eq.s32	%p105, %r781, 0;
	@%p105 bra 	BB7_156;

	and.b32  	%r782, %r778, 131040;
	and.b32  	%r783, %r778, 31;
	xor.b32  	%r784, %r783, 1;
	shr.u32 	%r785, %r782, 3;
	add.s32 	%r787, %r218, %r785;
	mov.u32 	%r788, 1;
	shl.b32 	%r789, %r788, %r784;
	ld.shared.u32 	%r790, [%r787];
	and.b32  	%r791, %r790, %r789;
	setp.eq.s32	%p106, %r791, 0;
	@%p106 bra 	BB7_156;

	bfe.u32 	%r794, %r779, 17, 12;
	mul.wide.u32 	%rd113, %r794, 4;
	add.s64 	%rd114, %rd1, %rd113;
	atom.global.add.u32 	%r795, [%rd114], 1;
	min.s32 	%r796, %r795, %r99;
	mad.lo.s32 	%r797, %r794, %r215, %r796;
	mul.wide.s32 	%rd115, %r797, 8;
	add.s64 	%rd112, %rd6, %rd115;
	// inline asm
	st.global.cg.v2.u32 [%rd112], {%r779, %r778};
	// inline asm

BB7_156:
	add.s32 	%r1285, %r1285, 4;
	setp.lt.s32	%p107, %r1285, %r7;
	@%p107 bra 	BB7_140;

BB7_157:
	@%p22 bra 	BB7_194;

	add.s32 	%r128, %r215, -2;
	mov.u32 	%r802, 1;
	max.s32 	%r129, %r9, %r802;
	and.b32  	%r801, %r129, 3;
	mov.u32 	%r1289, 0;
	setp.eq.s32	%p109, %r801, 0;
	@%p109 bra 	BB7_176;

	setp.eq.s32	%p110, %r801, 1;
	@%p110 bra 	BB7_171;

	setp.eq.s32	%p111, %r801, 2;
	@%p111 bra 	BB7_166;

	setp.ge.s32	%p112, %r1, %r3;
	@%p112 bra 	BB7_162;

	add.s32 	%r807, %r1, %r14;
	mul.wide.s32 	%rd117, %r807, 8;
	add.s64 	%rd116, %rd3, %rd117;
	// inline asm
	ld.global.cs.v2.u32 {%r804,%r805}, [%rd116];
	// inline asm
	or.b32  	%r808, %r804, %r805;
	setp.eq.s32	%p113, %r808, 0;
	mov.u32 	%r1289, %r802;
	@%p113 bra 	BB7_166;

	and.b32  	%r810, %r804, 131040;
	and.b32  	%r811, %r804, 31;
	xor.b32  	%r812, %r811, 1;
	shr.u32 	%r813, %r810, 3;
	add.s32 	%r815, %r218, %r813;
	mov.u32 	%r1289, 1;
	shl.b32 	%r816, %r1289, %r812;
	ld.shared.u32 	%r817, [%r815];
	and.b32  	%r818, %r817, %r816;
	setp.eq.s32	%p114, %r818, 0;
	@%p114 bra 	BB7_166;

	bfe.u32 	%r822, %r805, 17, 12;
	mul.wide.u32 	%rd119, %r822, 4;
	add.s64 	%rd120, %rd1, %rd119;
	atom.global.add.u32 	%r823, [%rd120], 1;
	min.s32 	%r824, %r823, %r128;
	mad.lo.s32 	%r825, %r822, %r215, %r824;
	mul.wide.s32 	%rd121, %r825, 8;
	add.s64 	%rd118, %rd6, %rd121;
	// inline asm
	st.global.cg.v2.u32 [%rd118], {%r805, %r804};
	// inline asm
	bra.uni 	BB7_166;

BB7_162:
	mov.u32 	%r1289, %r802;

BB7_166:
	shl.b32 	%r826, %r1289, 10;
	add.s32 	%r133, %r826, %r1;
	setp.ge.s32	%p115, %r133, %r3;
	@%p115 bra 	BB7_170;

	add.s32 	%r829, %r133, %r14;
	mul.wide.s32 	%rd123, %r829, 8;
	add.s64 	%rd122, %rd3, %rd123;
	// inline asm
	ld.global.cs.v2.u32 {%r827,%r828}, [%rd122];
	// inline asm
	or.b32  	%r830, %r827, %r828;
	setp.eq.s32	%p116, %r830, 0;
	@%p116 bra 	BB7_170;

	and.b32  	%r831, %r827, 131040;
	and.b32  	%r832, %r827, 31;
	xor.b32  	%r833, %r832, 1;
	shr.u32 	%r834, %r831, 3;
	add.s32 	%r836, %r218, %r834;
	mov.u32 	%r837, 1;
	shl.b32 	%r838, %r837, %r833;
	ld.shared.u32 	%r839, [%r836];
	and.b32  	%r840, %r839, %r838;
	setp.eq.s32	%p117, %r840, 0;
	@%p117 bra 	BB7_170;

	bfe.u32 	%r843, %r828, 17, 12;
	mul.wide.u32 	%rd125, %r843, 4;
	add.s64 	%rd126, %rd1, %rd125;
	atom.global.add.u32 	%r844, [%rd126], 1;
	min.s32 	%r845, %r844, %r128;
	mad.lo.s32 	%r846, %r843, %r215, %r845;
	mul.wide.s32 	%rd127, %r846, 8;
	add.s64 	%rd124, %rd6, %rd127;
	// inline asm
	st.global.cg.v2.u32 [%rd124], {%r828, %r827};
	// inline asm

BB7_170:
	add.s32 	%r1289, %r1289, 1;

BB7_171:
	shl.b32 	%r847, %r1289, 10;
	add.s32 	%r138, %r847, %r1;
	setp.ge.s32	%p118, %r138, %r3;
	@%p118 bra 	BB7_175;

	add.s32 	%r850, %r138, %r14;
	mul.wide.s32 	%rd129, %r850, 8;
	add.s64 	%rd128, %rd3, %rd129;
	// inline asm
	ld.global.cs.v2.u32 {%r848,%r849}, [%rd128];
	// inline asm
	or.b32  	%r851, %r848, %r849;
	setp.eq.s32	%p119, %r851, 0;
	@%p119 bra 	BB7_175;

	and.b32  	%r852, %r848, 131040;
	and.b32  	%r853, %r848, 31;
	xor.b32  	%r854, %r853, 1;
	shr.u32 	%r855, %r852, 3;
	add.s32 	%r857, %r218, %r855;
	mov.u32 	%r858, 1;
	shl.b32 	%r859, %r858, %r854;
	ld.shared.u32 	%r860, [%r857];
	and.b32  	%r861, %r860, %r859;
	setp.eq.s32	%p120, %r861, 0;
	@%p120 bra 	BB7_175;

	bfe.u32 	%r864, %r849, 17, 12;
	mul.wide.u32 	%rd131, %r864, 4;
	add.s64 	%rd132, %rd1, %rd131;
	atom.global.add.u32 	%r865, [%rd132], 1;
	min.s32 	%r866, %r865, %r128;
	mad.lo.s32 	%r867, %r864, %r215, %r866;
	mul.wide.s32 	%rd133, %r867, 8;
	add.s64 	%rd130, %rd6, %rd133;
	// inline asm
	st.global.cg.v2.u32 [%rd130], {%r849, %r848};
	// inline asm

BB7_175:
	add.s32 	%r1289, %r1289, 1;

BB7_176:
	setp.lt.u32	%p121, %r129, 4;
	@%p121 bra 	BB7_194;

BB7_177:
	shl.b32 	%r868, %r1289, 10;
	add.s32 	%r144, %r868, %r1;
	setp.ge.s32	%p122, %r144, %r3;
	@%p122 bra 	BB7_181;

	add.s32 	%r871, %r144, %r14;
	mul.wide.s32 	%rd135, %r871, 8;
	add.s64 	%rd134, %rd3, %rd135;
	// inline asm
	ld.global.cs.v2.u32 {%r869,%r870}, [%rd134];
	// inline asm
	or.b32  	%r872, %r869, %r870;
	setp.eq.s32	%p123, %r872, 0;
	@%p123 bra 	BB7_181;

	and.b32  	%r873, %r869, 131040;
	and.b32  	%r874, %r869, 31;
	xor.b32  	%r875, %r874, 1;
	shr.u32 	%r876, %r873, 3;
	add.s32 	%r878, %r218, %r876;
	mov.u32 	%r879, 1;
	shl.b32 	%r880, %r879, %r875;
	ld.shared.u32 	%r881, [%r878];
	and.b32  	%r882, %r881, %r880;
	setp.eq.s32	%p124, %r882, 0;
	@%p124 bra 	BB7_181;

	bfe.u32 	%r885, %r870, 17, 12;
	mul.wide.u32 	%rd137, %r885, 4;
	add.s64 	%rd138, %rd1, %rd137;
	atom.global.add.u32 	%r886, [%rd138], 1;
	min.s32 	%r887, %r886, %r128;
	mad.lo.s32 	%r888, %r885, %r215, %r887;
	mul.wide.s32 	%rd139, %r888, 8;
	add.s64 	%rd136, %rd6, %rd139;
	// inline asm
	st.global.cg.v2.u32 [%rd136], {%r870, %r869};
	// inline asm

BB7_181:
	add.s32 	%r147, %r144, 1024;
	setp.ge.s32	%p125, %r147, %r3;
	@%p125 bra 	BB7_185;

	add.s32 	%r893, %r147, %r14;
	mul.wide.s32 	%rd141, %r893, 8;
	add.s64 	%rd140, %rd3, %rd141;
	// inline asm
	ld.global.cs.v2.u32 {%r891,%r892}, [%rd140];
	// inline asm
	or.b32  	%r894, %r891, %r892;
	setp.eq.s32	%p126, %r894, 0;
	@%p126 bra 	BB7_185;

	and.b32  	%r895, %r891, 131040;
	and.b32  	%r896, %r891, 31;
	xor.b32  	%r897, %r896, 1;
	shr.u32 	%r898, %r895, 3;
	add.s32 	%r900, %r218, %r898;
	mov.u32 	%r901, 1;
	shl.b32 	%r902, %r901, %r897;
	ld.shared.u32 	%r903, [%r900];
	and.b32  	%r904, %r903, %r902;
	setp.eq.s32	%p127, %r904, 0;
	@%p127 bra 	BB7_185;

	bfe.u32 	%r907, %r892, 17, 12;
	mul.wide.u32 	%rd143, %r907, 4;
	add.s64 	%rd144, %rd1, %rd143;
	atom.global.add.u32 	%r908, [%rd144], 1;
	min.s32 	%r909, %r908, %r128;
	mad.lo.s32 	%r910, %r907, %r215, %r909;
	mul.wide.s32 	%rd145, %r910, 8;
	add.s64 	%rd142, %rd6, %rd145;
	// inline asm
	st.global.cg.v2.u32 [%rd142], {%r892, %r891};
	// inline asm

BB7_185:
	add.s32 	%r150, %r144, 2048;
	setp.ge.s32	%p128, %r150, %r3;
	@%p128 bra 	BB7_189;

	add.s32 	%r915, %r150, %r14;
	mul.wide.s32 	%rd147, %r915, 8;
	add.s64 	%rd146, %rd3, %rd147;
	// inline asm
	ld.global.cs.v2.u32 {%r913,%r914}, [%rd146];
	// inline asm
	or.b32  	%r916, %r913, %r914;
	setp.eq.s32	%p129, %r916, 0;
	@%p129 bra 	BB7_189;

	and.b32  	%r917, %r913, 131040;
	and.b32  	%r918, %r913, 31;
	xor.b32  	%r919, %r918, 1;
	shr.u32 	%r920, %r917, 3;
	add.s32 	%r922, %r218, %r920;
	mov.u32 	%r923, 1;
	shl.b32 	%r924, %r923, %r919;
	ld.shared.u32 	%r925, [%r922];
	and.b32  	%r926, %r925, %r924;
	setp.eq.s32	%p130, %r926, 0;
	@%p130 bra 	BB7_189;

	bfe.u32 	%r929, %r914, 17, 12;
	mul.wide.u32 	%rd149, %r929, 4;
	add.s64 	%rd150, %rd1, %rd149;
	atom.global.add.u32 	%r930, [%rd150], 1;
	min.s32 	%r931, %r930, %r128;
	mad.lo.s32 	%r932, %r929, %r215, %r931;
	mul.wide.s32 	%rd151, %r932, 8;
	add.s64 	%rd148, %rd6, %rd151;
	// inline asm
	st.global.cg.v2.u32 [%rd148], {%r914, %r913};
	// inline asm

BB7_189:
	add.s32 	%r153, %r144, 3072;
	setp.ge.s32	%p131, %r153, %r3;
	@%p131 bra 	BB7_193;

	add.s32 	%r937, %r153, %r14;
	mul.wide.s32 	%rd153, %r937, 8;
	add.s64 	%rd152, %rd3, %rd153;
	// inline asm
	ld.global.cs.v2.u32 {%r935,%r936}, [%rd152];
	// inline asm
	or.b32  	%r938, %r935, %r936;
	setp.eq.s32	%p132, %r938, 0;
	@%p132 bra 	BB7_193;

	and.b32  	%r939, %r935, 131040;
	and.b32  	%r940, %r935, 31;
	xor.b32  	%r941, %r940, 1;
	shr.u32 	%r942, %r939, 3;
	add.s32 	%r944, %r218, %r942;
	mov.u32 	%r945, 1;
	shl.b32 	%r946, %r945, %r941;
	ld.shared.u32 	%r947, [%r944];
	and.b32  	%r948, %r947, %r946;
	setp.eq.s32	%p133, %r948, 0;
	@%p133 bra 	BB7_193;

	bfe.u32 	%r951, %r936, 17, 12;
	mul.wide.u32 	%rd155, %r951, 4;
	add.s64 	%rd156, %rd1, %rd155;
	atom.global.add.u32 	%r952, [%rd156], 1;
	min.s32 	%r953, %r952, %r128;
	mad.lo.s32 	%r954, %r951, %r215, %r953;
	mul.wide.s32 	%rd157, %r954, 8;
	add.s64 	%rd154, %rd6, %rd157;
	// inline asm
	st.global.cg.v2.u32 [%rd154], {%r936, %r935};
	// inline asm

BB7_193:
	add.s32 	%r1289, %r1289, 4;
	setp.lt.s32	%p134, %r1289, %r9;
	@%p134 bra 	BB7_177;

BB7_194:
	@%p42 bra 	BB7_231;

	add.s32 	%r157, %r215, -2;
	mov.u32 	%r959, 1;
	max.s32 	%r158, %r11, %r959;
	and.b32  	%r958, %r158, 3;
	mov.u32 	%r1293, 0;
	setp.eq.s32	%p136, %r958, 0;
	@%p136 bra 	BB7_213;

	setp.eq.s32	%p137, %r958, 1;
	@%p137 bra 	BB7_208;

	setp.eq.s32	%p138, %r958, 2;
	@%p138 bra 	BB7_203;

	setp.ge.s32	%p139, %r1, %r4;
	@%p139 bra 	BB7_199;

	add.s32 	%r964, %r1, %r14;
	mul.wide.s32 	%rd159, %r964, 8;
	add.s64 	%rd158, %rd4, %rd159;
	// inline asm
	ld.global.cs.v2.u32 {%r961,%r962}, [%rd158];
	// inline asm
	or.b32  	%r965, %r961, %r962;
	setp.eq.s32	%p140, %r965, 0;
	mov.u32 	%r1293, %r959;
	@%p140 bra 	BB7_203;

	and.b32  	%r967, %r961, 131040;
	and.b32  	%r968, %r961, 31;
	xor.b32  	%r969, %r968, 1;
	shr.u32 	%r970, %r967, 3;
	add.s32 	%r972, %r218, %r970;
	mov.u32 	%r1293, 1;
	shl.b32 	%r973, %r1293, %r969;
	ld.shared.u32 	%r974, [%r972];
	and.b32  	%r975, %r974, %r973;
	setp.eq.s32	%p141, %r975, 0;
	@%p141 bra 	BB7_203;

	bfe.u32 	%r979, %r962, 17, 12;
	mul.wide.u32 	%rd161, %r979, 4;
	add.s64 	%rd162, %rd1, %rd161;
	atom.global.add.u32 	%r980, [%rd162], 1;
	min.s32 	%r981, %r980, %r157;
	mad.lo.s32 	%r982, %r979, %r215, %r981;
	mul.wide.s32 	%rd163, %r982, 8;
	add.s64 	%rd160, %rd6, %rd163;
	// inline asm
	st.global.cg.v2.u32 [%rd160], {%r962, %r961};
	// inline asm
	bra.uni 	BB7_203;

BB7_199:
	mov.u32 	%r1293, %r959;

BB7_203:
	shl.b32 	%r983, %r1293, 10;
	add.s32 	%r162, %r983, %r1;
	setp.ge.s32	%p142, %r162, %r4;
	@%p142 bra 	BB7_207;

	add.s32 	%r986, %r162, %r14;
	mul.wide.s32 	%rd165, %r986, 8;
	add.s64 	%rd164, %rd4, %rd165;
	// inline asm
	ld.global.cs.v2.u32 {%r984,%r985}, [%rd164];
	// inline asm
	or.b32  	%r987, %r984, %r985;
	setp.eq.s32	%p143, %r987, 0;
	@%p143 bra 	BB7_207;

	and.b32  	%r988, %r984, 131040;
	and.b32  	%r989, %r984, 31;
	xor.b32  	%r990, %r989, 1;
	shr.u32 	%r991, %r988, 3;
	add.s32 	%r993, %r218, %r991;
	mov.u32 	%r994, 1;
	shl.b32 	%r995, %r994, %r990;
	ld.shared.u32 	%r996, [%r993];
	and.b32  	%r997, %r996, %r995;
	setp.eq.s32	%p144, %r997, 0;
	@%p144 bra 	BB7_207;

	bfe.u32 	%r1000, %r985, 17, 12;
	mul.wide.u32 	%rd167, %r1000, 4;
	add.s64 	%rd168, %rd1, %rd167;
	atom.global.add.u32 	%r1001, [%rd168], 1;
	min.s32 	%r1002, %r1001, %r157;
	mad.lo.s32 	%r1003, %r1000, %r215, %r1002;
	mul.wide.s32 	%rd169, %r1003, 8;
	add.s64 	%rd166, %rd6, %rd169;
	// inline asm
	st.global.cg.v2.u32 [%rd166], {%r985, %r984};
	// inline asm

BB7_207:
	add.s32 	%r1293, %r1293, 1;

BB7_208:
	shl.b32 	%r1004, %r1293, 10;
	add.s32 	%r167, %r1004, %r1;
	setp.ge.s32	%p145, %r167, %r4;
	@%p145 bra 	BB7_212;

	add.s32 	%r1007, %r167, %r14;
	mul.wide.s32 	%rd171, %r1007, 8;
	add.s64 	%rd170, %rd4, %rd171;
	// inline asm
	ld.global.cs.v2.u32 {%r1005,%r1006}, [%rd170];
	// inline asm
	or.b32  	%r1008, %r1005, %r1006;
	setp.eq.s32	%p146, %r1008, 0;
	@%p146 bra 	BB7_212;

	and.b32  	%r1009, %r1005, 131040;
	and.b32  	%r1010, %r1005, 31;
	xor.b32  	%r1011, %r1010, 1;
	shr.u32 	%r1012, %r1009, 3;
	add.s32 	%r1014, %r218, %r1012;
	mov.u32 	%r1015, 1;
	shl.b32 	%r1016, %r1015, %r1011;
	ld.shared.u32 	%r1017, [%r1014];
	and.b32  	%r1018, %r1017, %r1016;
	setp.eq.s32	%p147, %r1018, 0;
	@%p147 bra 	BB7_212;

	bfe.u32 	%r1021, %r1006, 17, 12;
	mul.wide.u32 	%rd173, %r1021, 4;
	add.s64 	%rd174, %rd1, %rd173;
	atom.global.add.u32 	%r1022, [%rd174], 1;
	min.s32 	%r1023, %r1022, %r157;
	mad.lo.s32 	%r1024, %r1021, %r215, %r1023;
	mul.wide.s32 	%rd175, %r1024, 8;
	add.s64 	%rd172, %rd6, %rd175;
	// inline asm
	st.global.cg.v2.u32 [%rd172], {%r1006, %r1005};
	// inline asm

BB7_212:
	add.s32 	%r1293, %r1293, 1;

BB7_213:
	setp.lt.u32	%p148, %r158, 4;
	@%p148 bra 	BB7_231;

BB7_214:
	shl.b32 	%r1025, %r1293, 10;
	add.s32 	%r173, %r1025, %r1;
	setp.ge.s32	%p149, %r173, %r4;
	@%p149 bra 	BB7_218;

	add.s32 	%r1028, %r173, %r14;
	mul.wide.s32 	%rd177, %r1028, 8;
	add.s64 	%rd176, %rd4, %rd177;
	// inline asm
	ld.global.cs.v2.u32 {%r1026,%r1027}, [%rd176];
	// inline asm
	or.b32  	%r1029, %r1026, %r1027;
	setp.eq.s32	%p150, %r1029, 0;
	@%p150 bra 	BB7_218;

	and.b32  	%r1030, %r1026, 131040;
	and.b32  	%r1031, %r1026, 31;
	xor.b32  	%r1032, %r1031, 1;
	shr.u32 	%r1033, %r1030, 3;
	add.s32 	%r1035, %r218, %r1033;
	mov.u32 	%r1036, 1;
	shl.b32 	%r1037, %r1036, %r1032;
	ld.shared.u32 	%r1038, [%r1035];
	and.b32  	%r1039, %r1038, %r1037;
	setp.eq.s32	%p151, %r1039, 0;
	@%p151 bra 	BB7_218;

	bfe.u32 	%r1042, %r1027, 17, 12;
	mul.wide.u32 	%rd179, %r1042, 4;
	add.s64 	%rd180, %rd1, %rd179;
	atom.global.add.u32 	%r1043, [%rd180], 1;
	min.s32 	%r1044, %r1043, %r157;
	mad.lo.s32 	%r1045, %r1042, %r215, %r1044;
	mul.wide.s32 	%rd181, %r1045, 8;
	add.s64 	%rd178, %rd6, %rd181;
	// inline asm
	st.global.cg.v2.u32 [%rd178], {%r1027, %r1026};
	// inline asm

BB7_218:
	add.s32 	%r176, %r173, 1024;
	setp.ge.s32	%p152, %r176, %r4;
	@%p152 bra 	BB7_222;

	add.s32 	%r1050, %r176, %r14;
	mul.wide.s32 	%rd183, %r1050, 8;
	add.s64 	%rd182, %rd4, %rd183;
	// inline asm
	ld.global.cs.v2.u32 {%r1048,%r1049}, [%rd182];
	// inline asm
	or.b32  	%r1051, %r1048, %r1049;
	setp.eq.s32	%p153, %r1051, 0;
	@%p153 bra 	BB7_222;

	and.b32  	%r1052, %r1048, 131040;
	and.b32  	%r1053, %r1048, 31;
	xor.b32  	%r1054, %r1053, 1;
	shr.u32 	%r1055, %r1052, 3;
	add.s32 	%r1057, %r218, %r1055;
	mov.u32 	%r1058, 1;
	shl.b32 	%r1059, %r1058, %r1054;
	ld.shared.u32 	%r1060, [%r1057];
	and.b32  	%r1061, %r1060, %r1059;
	setp.eq.s32	%p154, %r1061, 0;
	@%p154 bra 	BB7_222;

	bfe.u32 	%r1064, %r1049, 17, 12;
	mul.wide.u32 	%rd185, %r1064, 4;
	add.s64 	%rd186, %rd1, %rd185;
	atom.global.add.u32 	%r1065, [%rd186], 1;
	min.s32 	%r1066, %r1065, %r157;
	mad.lo.s32 	%r1067, %r1064, %r215, %r1066;
	mul.wide.s32 	%rd187, %r1067, 8;
	add.s64 	%rd184, %rd6, %rd187;
	// inline asm
	st.global.cg.v2.u32 [%rd184], {%r1049, %r1048};
	// inline asm

BB7_222:
	add.s32 	%r179, %r173, 2048;
	setp.ge.s32	%p155, %r179, %r4;
	@%p155 bra 	BB7_226;

	add.s32 	%r1072, %r179, %r14;
	mul.wide.s32 	%rd189, %r1072, 8;
	add.s64 	%rd188, %rd4, %rd189;
	// inline asm
	ld.global.cs.v2.u32 {%r1070,%r1071}, [%rd188];
	// inline asm
	or.b32  	%r1073, %r1070, %r1071;
	setp.eq.s32	%p156, %r1073, 0;
	@%p156 bra 	BB7_226;

	and.b32  	%r1074, %r1070, 131040;
	and.b32  	%r1075, %r1070, 31;
	xor.b32  	%r1076, %r1075, 1;
	shr.u32 	%r1077, %r1074, 3;
	add.s32 	%r1079, %r218, %r1077;
	mov.u32 	%r1080, 1;
	shl.b32 	%r1081, %r1080, %r1076;
	ld.shared.u32 	%r1082, [%r1079];
	and.b32  	%r1083, %r1082, %r1081;
	setp.eq.s32	%p157, %r1083, 0;
	@%p157 bra 	BB7_226;

	bfe.u32 	%r1086, %r1071, 17, 12;
	mul.wide.u32 	%rd191, %r1086, 4;
	add.s64 	%rd192, %rd1, %rd191;
	atom.global.add.u32 	%r1087, [%rd192], 1;
	min.s32 	%r1088, %r1087, %r157;
	mad.lo.s32 	%r1089, %r1086, %r215, %r1088;
	mul.wide.s32 	%rd193, %r1089, 8;
	add.s64 	%rd190, %rd6, %rd193;
	// inline asm
	st.global.cg.v2.u32 [%rd190], {%r1071, %r1070};
	// inline asm

BB7_226:
	add.s32 	%r182, %r173, 3072;
	setp.ge.s32	%p158, %r182, %r4;
	@%p158 bra 	BB7_230;

	add.s32 	%r1094, %r182, %r14;
	mul.wide.s32 	%rd195, %r1094, 8;
	add.s64 	%rd194, %rd4, %rd195;
	// inline asm
	ld.global.cs.v2.u32 {%r1092,%r1093}, [%rd194];
	// inline asm
	or.b32  	%r1095, %r1092, %r1093;
	setp.eq.s32	%p159, %r1095, 0;
	@%p159 bra 	BB7_230;

	and.b32  	%r1096, %r1092, 131040;
	and.b32  	%r1097, %r1092, 31;
	xor.b32  	%r1098, %r1097, 1;
	shr.u32 	%r1099, %r1096, 3;
	add.s32 	%r1101, %r218, %r1099;
	mov.u32 	%r1102, 1;
	shl.b32 	%r1103, %r1102, %r1098;
	ld.shared.u32 	%r1104, [%r1101];
	and.b32  	%r1105, %r1104, %r1103;
	setp.eq.s32	%p160, %r1105, 0;
	@%p160 bra 	BB7_230;

	bfe.u32 	%r1108, %r1093, 17, 12;
	mul.wide.u32 	%rd197, %r1108, 4;
	add.s64 	%rd198, %rd1, %rd197;
	atom.global.add.u32 	%r1109, [%rd198], 1;
	min.s32 	%r1110, %r1109, %r157;
	mad.lo.s32 	%r1111, %r1108, %r215, %r1110;
	mul.wide.s32 	%rd199, %r1111, 8;
	add.s64 	%rd196, %rd6, %rd199;
	// inline asm
	st.global.cg.v2.u32 [%rd196], {%r1093, %r1092};
	// inline asm

BB7_230:
	add.s32 	%r1293, %r1293, 4;
	setp.lt.s32	%p161, %r1293, %r11;
	@%p161 bra 	BB7_214;

BB7_231:
	@%p62 bra 	BB7_268;

	add.s32 	%r186, %r215, -2;
	mov.u32 	%r1116, 1;
	max.s32 	%r187, %r13, %r1116;
	and.b32  	%r1115, %r187, 3;
	setp.eq.s32	%p163, %r1115, 0;
	@%p163 bra 	BB7_250;

	setp.eq.s32	%p164, %r1115, 1;
	@%p164 bra 	BB7_245;

	setp.eq.s32	%p165, %r1115, 2;
	@%p165 bra 	BB7_240;

	setp.ge.s32	%p166, %r1, %r5;
	@%p166 bra 	BB7_236;

	add.s32 	%r1121, %r1, %r14;
	mul.wide.s32 	%rd201, %r1121, 8;
	add.s64 	%rd200, %rd5, %rd201;
	// inline asm
	ld.global.cs.v2.u32 {%r1118,%r1119}, [%rd200];
	// inline asm
	or.b32  	%r1122, %r1118, %r1119;
	setp.eq.s32	%p167, %r1122, 0;
	mov.u32 	%r1297, %r1116;
	@%p167 bra 	BB7_240;

	and.b32  	%r1124, %r1118, 131040;
	and.b32  	%r1125, %r1118, 31;
	xor.b32  	%r1126, %r1125, 1;
	shr.u32 	%r1127, %r1124, 3;
	add.s32 	%r1129, %r218, %r1127;
	mov.u32 	%r1297, 1;
	shl.b32 	%r1130, %r1297, %r1126;
	ld.shared.u32 	%r1131, [%r1129];
	and.b32  	%r1132, %r1131, %r1130;
	setp.eq.s32	%p168, %r1132, 0;
	@%p168 bra 	BB7_240;

	bfe.u32 	%r1136, %r1119, 17, 12;
	mul.wide.u32 	%rd203, %r1136, 4;
	add.s64 	%rd204, %rd1, %rd203;
	atom.global.add.u32 	%r1137, [%rd204], 1;
	min.s32 	%r1138, %r1137, %r186;
	mad.lo.s32 	%r1139, %r1136, %r215, %r1138;
	mul.wide.s32 	%rd205, %r1139, 8;
	add.s64 	%rd202, %rd6, %rd205;
	// inline asm
	st.global.cg.v2.u32 [%rd202], {%r1119, %r1118};
	// inline asm
	bra.uni 	BB7_240;

BB7_236:
	mov.u32 	%r1297, %r1116;

BB7_240:
	shl.b32 	%r1140, %r1297, 10;
	add.s32 	%r191, %r1140, %r1;
	setp.ge.s32	%p169, %r191, %r5;
	@%p169 bra 	BB7_244;

	add.s32 	%r1143, %r191, %r14;
	mul.wide.s32 	%rd207, %r1143, 8;
	add.s64 	%rd206, %rd5, %rd207;
	// inline asm
	ld.global.cs.v2.u32 {%r1141,%r1142}, [%rd206];
	// inline asm
	or.b32  	%r1144, %r1141, %r1142;
	setp.eq.s32	%p170, %r1144, 0;
	@%p170 bra 	BB7_244;

	and.b32  	%r1145, %r1141, 131040;
	and.b32  	%r1146, %r1141, 31;
	xor.b32  	%r1147, %r1146, 1;
	shr.u32 	%r1148, %r1145, 3;
	add.s32 	%r1150, %r218, %r1148;
	mov.u32 	%r1151, 1;
	shl.b32 	%r1152, %r1151, %r1147;
	ld.shared.u32 	%r1153, [%r1150];
	and.b32  	%r1154, %r1153, %r1152;
	setp.eq.s32	%p171, %r1154, 0;
	@%p171 bra 	BB7_244;

	bfe.u32 	%r1157, %r1142, 17, 12;
	mul.wide.u32 	%rd209, %r1157, 4;
	add.s64 	%rd210, %rd1, %rd209;
	atom.global.add.u32 	%r1158, [%rd210], 1;
	min.s32 	%r1159, %r1158, %r186;
	mad.lo.s32 	%r1160, %r1157, %r215, %r1159;
	mul.wide.s32 	%rd211, %r1160, 8;
	add.s64 	%rd208, %rd6, %rd211;
	// inline asm
	st.global.cg.v2.u32 [%rd208], {%r1142, %r1141};
	// inline asm

BB7_244:
	add.s32 	%r1297, %r1297, 1;

BB7_245:
	shl.b32 	%r1161, %r1297, 10;
	add.s32 	%r196, %r1161, %r1;
	setp.ge.s32	%p172, %r196, %r5;
	@%p172 bra 	BB7_249;

	add.s32 	%r1164, %r196, %r14;
	mul.wide.s32 	%rd213, %r1164, 8;
	add.s64 	%rd212, %rd5, %rd213;
	// inline asm
	ld.global.cs.v2.u32 {%r1162,%r1163}, [%rd212];
	// inline asm
	or.b32  	%r1165, %r1162, %r1163;
	setp.eq.s32	%p173, %r1165, 0;
	@%p173 bra 	BB7_249;

	and.b32  	%r1166, %r1162, 131040;
	and.b32  	%r1167, %r1162, 31;
	xor.b32  	%r1168, %r1167, 1;
	shr.u32 	%r1169, %r1166, 3;
	add.s32 	%r1171, %r218, %r1169;
	mov.u32 	%r1172, 1;
	shl.b32 	%r1173, %r1172, %r1168;
	ld.shared.u32 	%r1174, [%r1171];
	and.b32  	%r1175, %r1174, %r1173;
	setp.eq.s32	%p174, %r1175, 0;
	@%p174 bra 	BB7_249;

	bfe.u32 	%r1178, %r1163, 17, 12;
	mul.wide.u32 	%rd215, %r1178, 4;
	add.s64 	%rd216, %rd1, %rd215;
	atom.global.add.u32 	%r1179, [%rd216], 1;
	min.s32 	%r1180, %r1179, %r186;
	mad.lo.s32 	%r1181, %r1178, %r215, %r1180;
	mul.wide.s32 	%rd217, %r1181, 8;
	add.s64 	%rd214, %rd6, %rd217;
	// inline asm
	st.global.cg.v2.u32 [%rd214], {%r1163, %r1162};
	// inline asm

BB7_249:
	add.s32 	%r1297, %r1297, 1;

BB7_250:
	setp.lt.u32	%p175, %r187, 4;
	@%p175 bra 	BB7_268;

BB7_251:
	shl.b32 	%r1182, %r1297, 10;
	add.s32 	%r202, %r1182, %r1;
	setp.ge.s32	%p176, %r202, %r5;
	@%p176 bra 	BB7_255;

	add.s32 	%r1185, %r202, %r14;
	mul.wide.s32 	%rd219, %r1185, 8;
	add.s64 	%rd218, %rd5, %rd219;
	// inline asm
	ld.global.cs.v2.u32 {%r1183,%r1184}, [%rd218];
	// inline asm
	or.b32  	%r1186, %r1183, %r1184;
	setp.eq.s32	%p177, %r1186, 0;
	@%p177 bra 	BB7_255;

	and.b32  	%r1187, %r1183, 131040;
	and.b32  	%r1188, %r1183, 31;
	xor.b32  	%r1189, %r1188, 1;
	shr.u32 	%r1190, %r1187, 3;
	add.s32 	%r1192, %r218, %r1190;
	mov.u32 	%r1193, 1;
	shl.b32 	%r1194, %r1193, %r1189;
	ld.shared.u32 	%r1195, [%r1192];
	and.b32  	%r1196, %r1195, %r1194;
	setp.eq.s32	%p178, %r1196, 0;
	@%p178 bra 	BB7_255;

	bfe.u32 	%r1199, %r1184, 17, 12;
	mul.wide.u32 	%rd221, %r1199, 4;
	add.s64 	%rd222, %rd1, %rd221;
	atom.global.add.u32 	%r1200, [%rd222], 1;
	min.s32 	%r1201, %r1200, %r186;
	mad.lo.s32 	%r1202, %r1199, %r215, %r1201;
	mul.wide.s32 	%rd223, %r1202, 8;
	add.s64 	%rd220, %rd6, %rd223;
	// inline asm
	st.global.cg.v2.u32 [%rd220], {%r1184, %r1183};
	// inline asm

BB7_255:
	add.s32 	%r205, %r202, 1024;
	setp.ge.s32	%p179, %r205, %r5;
	@%p179 bra 	BB7_259;

	add.s32 	%r1207, %r205, %r14;
	mul.wide.s32 	%rd225, %r1207, 8;
	add.s64 	%rd224, %rd5, %rd225;
	// inline asm
	ld.global.cs.v2.u32 {%r1205,%r1206}, [%rd224];
	// inline asm
	or.b32  	%r1208, %r1205, %r1206;
	setp.eq.s32	%p180, %r1208, 0;
	@%p180 bra 	BB7_259;

	and.b32  	%r1209, %r1205, 131040;
	and.b32  	%r1210, %r1205, 31;
	xor.b32  	%r1211, %r1210, 1;
	shr.u32 	%r1212, %r1209, 3;
	add.s32 	%r1214, %r218, %r1212;
	mov.u32 	%r1215, 1;
	shl.b32 	%r1216, %r1215, %r1211;
	ld.shared.u32 	%r1217, [%r1214];
	and.b32  	%r1218, %r1217, %r1216;
	setp.eq.s32	%p181, %r1218, 0;
	@%p181 bra 	BB7_259;

	bfe.u32 	%r1221, %r1206, 17, 12;
	mul.wide.u32 	%rd227, %r1221, 4;
	add.s64 	%rd228, %rd1, %rd227;
	atom.global.add.u32 	%r1222, [%rd228], 1;
	min.s32 	%r1223, %r1222, %r186;
	mad.lo.s32 	%r1224, %r1221, %r215, %r1223;
	mul.wide.s32 	%rd229, %r1224, 8;
	add.s64 	%rd226, %rd6, %rd229;
	// inline asm
	st.global.cg.v2.u32 [%rd226], {%r1206, %r1205};
	// inline asm

BB7_259:
	add.s32 	%r208, %r202, 2048;
	setp.ge.s32	%p182, %r208, %r5;
	@%p182 bra 	BB7_263;

	add.s32 	%r1229, %r208, %r14;
	mul.wide.s32 	%rd231, %r1229, 8;
	add.s64 	%rd230, %rd5, %rd231;
	// inline asm
	ld.global.cs.v2.u32 {%r1227,%r1228}, [%rd230];
	// inline asm
	or.b32  	%r1230, %r1227, %r1228;
	setp.eq.s32	%p183, %r1230, 0;
	@%p183 bra 	BB7_263;

	and.b32  	%r1231, %r1227, 131040;
	and.b32  	%r1232, %r1227, 31;
	xor.b32  	%r1233, %r1232, 1;
	shr.u32 	%r1234, %r1231, 3;
	add.s32 	%r1236, %r218, %r1234;
	mov.u32 	%r1237, 1;
	shl.b32 	%r1238, %r1237, %r1233;
	ld.shared.u32 	%r1239, [%r1236];
	and.b32  	%r1240, %r1239, %r1238;
	setp.eq.s32	%p184, %r1240, 0;
	@%p184 bra 	BB7_263;

	bfe.u32 	%r1243, %r1228, 17, 12;
	mul.wide.u32 	%rd233, %r1243, 4;
	add.s64 	%rd234, %rd1, %rd233;
	atom.global.add.u32 	%r1244, [%rd234], 1;
	min.s32 	%r1245, %r1244, %r186;
	mad.lo.s32 	%r1246, %r1243, %r215, %r1245;
	mul.wide.s32 	%rd235, %r1246, 8;
	add.s64 	%rd232, %rd6, %rd235;
	// inline asm
	st.global.cg.v2.u32 [%rd232], {%r1228, %r1227};
	// inline asm

BB7_263:
	add.s32 	%r211, %r202, 3072;
	setp.ge.s32	%p185, %r211, %r5;
	@%p185 bra 	BB7_267;

	add.s32 	%r1251, %r211, %r14;
	mul.wide.s32 	%rd237, %r1251, 8;
	add.s64 	%rd236, %rd5, %rd237;
	// inline asm
	ld.global.cs.v2.u32 {%r1249,%r1250}, [%rd236];
	// inline asm
	or.b32  	%r1252, %r1249, %r1250;
	setp.eq.s32	%p186, %r1252, 0;
	@%p186 bra 	BB7_267;

	and.b32  	%r1253, %r1249, 131040;
	and.b32  	%r1254, %r1249, 31;
	xor.b32  	%r1255, %r1254, 1;
	shr.u32 	%r1256, %r1253, 3;
	add.s32 	%r1258, %r218, %r1256;
	mov.u32 	%r1259, 1;
	shl.b32 	%r1260, %r1259, %r1255;
	ld.shared.u32 	%r1261, [%r1258];
	and.b32  	%r1262, %r1261, %r1260;
	setp.eq.s32	%p187, %r1262, 0;
	@%p187 bra 	BB7_267;

	bfe.u32 	%r1265, %r1250, 17, 12;
	mul.wide.u32 	%rd239, %r1265, 4;
	add.s64 	%rd240, %rd1, %rd239;
	atom.global.add.u32 	%r1266, [%rd240], 1;
	min.s32 	%r1267, %r1266, %r186;
	mad.lo.s32 	%r1268, %r1265, %r215, %r1267;
	mul.wide.s32 	%rd241, %r1268, 8;
	add.s64 	%rd238, %rd6, %rd241;
	// inline asm
	st.global.cg.v2.u32 [%rd238], {%r1250, %r1249};
	// inline asm

BB7_267:
	add.s32 	%r1297, %r1297, 4;
	setp.lt.s32	%p188, %r1297, %r13;
	@%p188 bra 	BB7_251;

BB7_268:
	ret;
}

	// .globl	FluffyRound_B1
.visible .entry FluffyRound_B1(
	.param .u64 FluffyRound_B1_param_0,
	.param .u64 FluffyRound_B1_param_1,
	.param .u64 FluffyRound_B1_param_2,
	.param .u64 FluffyRound_B1_param_3,
	.param .u32 FluffyRound_B1_param_4,
	.param .u32 FluffyRound_B1_param_5,
	.param .u32 FluffyRound_B1_param_6
)
{
	.reg .pred 	%p<189>;
	.reg .b32 	%r<1276>;
	.reg .b64 	%rd<241>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_B1$__cuda_local_var_208097_30_non_const_ecounters[32768];

	ld.param.u64 	%rd4, [FluffyRound_B1_param_0];
	ld.param.u64 	%rd5, [FluffyRound_B1_param_1];
	ld.param.u64 	%rd6, [FluffyRound_B1_param_2];
	ld.param.u64 	%rd7, [FluffyRound_B1_param_3];
	ld.param.u32 	%r235, [FluffyRound_B1_param_5];
	ld.param.u32 	%r236, [FluffyRound_B1_param_6];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r237, %r1, 2;
	mov.u32 	%r238, FluffyRound_B1$__cuda_local_var_208097_30_non_const_ecounters;
	add.s32 	%r239, %r238, %r237;
	mov.u32 	%r1271, 0;
	st.shared.u32 	[%r239], %r1271;
	st.shared.u32 	[%r239+4096], %r1271;
	st.shared.u32 	[%r239+8192], %r1271;
	st.shared.u32 	[%r239+12288], %r1271;
	st.shared.u32 	[%r239+16384], %r1271;
	st.shared.u32 	[%r239+20480], %r1271;
	st.shared.u32 	[%r239+24576], %r1271;
	st.shared.u32 	[%r239+28672], %r1271;
	cvta.to.global.u64 	%rd3, %rd7;
	mov.u32 	%r241, %ctaid.x;
	add.s32 	%r242, %r241, %r236;
	cvta.to.global.u64 	%rd8, %rd6;
	mul.wide.s32 	%rd9, %r242, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%r243, [%rd10];
	mov.u32 	%r244, 34300;
	min.s32 	%r2, %r243, %r244;
	add.s32 	%r3, %r2, 1024;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 22;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r4, %r247, 10;
	add.s32 	%r248, %r242, 4096;
	mul.wide.s32 	%rd11, %r248, 4;
	add.s64 	%rd12, %rd8, %rd11;
	ld.global.u32 	%r249, [%rd12];
	min.s32 	%r5, %r249, %r244;
	add.s32 	%r6, %r5, 1024;
	shr.s32 	%r250, %r6, 31;
	shr.u32 	%r251, %r250, 22;
	add.s32 	%r252, %r6, %r251;
	shr.s32 	%r7, %r252, 10;
	add.s32 	%r253, %r242, 8192;
	mul.wide.s32 	%rd13, %r253, 4;
	add.s64 	%rd14, %rd8, %rd13;
	ld.global.u32 	%r254, [%rd14];
	min.s32 	%r8, %r254, %r244;
	add.s32 	%r9, %r8, 1024;
	shr.s32 	%r255, %r9, 31;
	shr.u32 	%r256, %r255, 22;
	add.s32 	%r257, %r9, %r256;
	shr.s32 	%r10, %r257, 10;
	add.s32 	%r258, %r242, 12288;
	mul.wide.s32 	%rd15, %r258, 4;
	add.s64 	%rd16, %rd8, %rd15;
	ld.global.u32 	%r259, [%rd16];
	min.s32 	%r11, %r259, %r244;
	add.s32 	%r12, %r11, 1024;
	shr.s32 	%r260, %r12, 31;
	shr.u32 	%r261, %r260, 22;
	add.s32 	%r262, %r12, %r261;
	shr.s32 	%r13, %r262, 10;
	shl.b32 	%r14, %r236, 2;
	add.s32 	%r263, %r241, %r14;
	mul.lo.s32 	%r15, %r263, 34304;
	add.s32 	%r16, %r15, 35127296;
	add.s32 	%r17, %r15, 70254592;
	add.s32 	%r18, %r15, 105381888;
	bar.sync 	0;
	setp.lt.s32	%p2, %r3, 1024;
	@%p2 bra 	BB8_31;

	mov.u32 	%r268, 1;
	max.s32 	%r19, %r4, %r268;
	and.b32  	%r267, %r19, 3;
	mov.u32 	%r1236, 0;
	setp.eq.s32	%p3, %r267, 0;
	@%p3 bra 	BB8_16;

	setp.eq.s32	%p4, %r267, 1;
	@%p4 bra 	BB8_12;

	setp.eq.s32	%p5, %r267, 2;
	@%p5 bra 	BB8_8;

	setp.ge.s32	%p6, %r1, %r2;
	@%p6 bra 	BB8_5;

	add.s32 	%r271, %r1, %r15;
	mul.wide.s32 	%rd17, %r271, 8;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.v2.u32 	{%r272, %r273}, [%rd18];
	or.b32  	%r275, %r272, %r273;
	setp.eq.s32	%p7, %r275, 0;
	mov.u32 	%r1236, %r268;
	@%p7 bra 	BB8_8;

	and.b32  	%r277, %r272, 131040;
	and.b32  	%r278, %r272, 31;
	mov.u32 	%r1236, 1;
	shl.b32 	%r279, %r1236, %r278;
	shr.u32 	%r280, %r277, 3;
	add.s32 	%r282, %r238, %r280;
	atom.shared.or.b32 	%r283, [%r282], %r279;
	bra.uni 	BB8_8;

BB8_5:
	mov.u32 	%r1236, %r268;

BB8_8:
	shl.b32 	%r284, %r1236, 10;
	add.s32 	%r22, %r284, %r1;
	setp.ge.s32	%p8, %r22, %r2;
	@%p8 bra 	BB8_11;

	add.s32 	%r285, %r22, %r15;
	mul.wide.s32 	%rd19, %r285, 8;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.v2.u32 	{%r286, %r287}, [%rd20];
	or.b32  	%r289, %r286, %r287;
	setp.eq.s32	%p9, %r289, 0;
	@%p9 bra 	BB8_11;

	and.b32  	%r290, %r286, 131040;
	and.b32  	%r291, %r286, 31;
	mov.u32 	%r292, 1;
	shl.b32 	%r293, %r292, %r291;
	shr.u32 	%r294, %r290, 3;
	add.s32 	%r296, %r238, %r294;
	atom.shared.or.b32 	%r297, [%r296], %r293;

BB8_11:
	add.s32 	%r1236, %r1236, 1;

BB8_12:
	shl.b32 	%r298, %r1236, 10;
	add.s32 	%r26, %r298, %r1;
	setp.ge.s32	%p10, %r26, %r2;
	@%p10 bra 	BB8_15;

	add.s32 	%r299, %r26, %r15;
	mul.wide.s32 	%rd21, %r299, 8;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.v2.u32 	{%r300, %r301}, [%rd22];
	or.b32  	%r303, %r300, %r301;
	setp.eq.s32	%p11, %r303, 0;
	@%p11 bra 	BB8_15;

	and.b32  	%r304, %r300, 131040;
	and.b32  	%r305, %r300, 31;
	mov.u32 	%r306, 1;
	shl.b32 	%r307, %r306, %r305;
	shr.u32 	%r308, %r304, 3;
	add.s32 	%r310, %r238, %r308;
	atom.shared.or.b32 	%r311, [%r310], %r307;

BB8_15:
	add.s32 	%r1236, %r1236, 1;

BB8_16:
	setp.lt.u32	%p12, %r19, 4;
	@%p12 bra 	BB8_31;

	mad.lo.s32 	%r1239, %r1236, 1024, %r1;

BB8_18:
	setp.ge.s32	%p13, %r1239, %r2;
	@%p13 bra 	BB8_21;

	add.s32 	%r312, %r1239, %r15;
	mul.wide.s32 	%rd23, %r312, 8;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.v2.u32 	{%r313, %r314}, [%rd24];
	or.b32  	%r316, %r313, %r314;
	setp.eq.s32	%p14, %r316, 0;
	@%p14 bra 	BB8_21;

	and.b32  	%r317, %r313, 131040;
	and.b32  	%r318, %r313, 31;
	mov.u32 	%r319, 1;
	shl.b32 	%r320, %r319, %r318;
	shr.u32 	%r321, %r317, 3;
	add.s32 	%r323, %r238, %r321;
	atom.shared.or.b32 	%r324, [%r323], %r320;

BB8_21:
	add.s32 	%r34, %r1239, 1024;
	setp.ge.s32	%p15, %r34, %r2;
	@%p15 bra 	BB8_24;

	add.s32 	%r325, %r34, %r15;
	mul.wide.s32 	%rd25, %r325, 8;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.v2.u32 	{%r326, %r327}, [%rd26];
	or.b32  	%r329, %r326, %r327;
	setp.eq.s32	%p16, %r329, 0;
	@%p16 bra 	BB8_24;

	and.b32  	%r330, %r326, 131040;
	and.b32  	%r331, %r326, 31;
	mov.u32 	%r332, 1;
	shl.b32 	%r333, %r332, %r331;
	shr.u32 	%r334, %r330, 3;
	add.s32 	%r336, %r238, %r334;
	atom.shared.or.b32 	%r337, [%r336], %r333;

BB8_24:
	add.s32 	%r36, %r1239, 2048;
	setp.ge.s32	%p17, %r36, %r2;
	@%p17 bra 	BB8_27;

	add.s32 	%r338, %r36, %r15;
	mul.wide.s32 	%rd27, %r338, 8;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.v2.u32 	{%r339, %r340}, [%rd28];
	or.b32  	%r342, %r339, %r340;
	setp.eq.s32	%p18, %r342, 0;
	@%p18 bra 	BB8_27;

	and.b32  	%r343, %r339, 131040;
	and.b32  	%r344, %r339, 31;
	mov.u32 	%r345, 1;
	shl.b32 	%r346, %r345, %r344;
	shr.u32 	%r347, %r343, 3;
	add.s32 	%r349, %r238, %r347;
	atom.shared.or.b32 	%r350, [%r349], %r346;

BB8_27:
	add.s32 	%r38, %r1239, 3072;
	setp.ge.s32	%p19, %r38, %r2;
	@%p19 bra 	BB8_30;

	add.s32 	%r351, %r38, %r15;
	mul.wide.s32 	%rd29, %r351, 8;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.v2.u32 	{%r352, %r353}, [%rd30];
	or.b32  	%r355, %r352, %r353;
	setp.eq.s32	%p20, %r355, 0;
	@%p20 bra 	BB8_30;

	and.b32  	%r356, %r352, 131040;
	and.b32  	%r357, %r352, 31;
	mov.u32 	%r358, 1;
	shl.b32 	%r359, %r358, %r357;
	shr.u32 	%r360, %r356, 3;
	add.s32 	%r362, %r238, %r360;
	atom.shared.or.b32 	%r363, [%r362], %r359;

BB8_30:
	add.s32 	%r1236, %r1236, 4;
	add.s32 	%r1239, %r1239, 4096;
	setp.lt.s32	%p21, %r1236, %r4;
	@%p21 bra 	BB8_18;

BB8_31:
	setp.lt.s32	%p22, %r6, 1024;
	@%p22 bra 	BB8_62;

	mov.u32 	%r368, 1;
	max.s32 	%r42, %r7, %r368;
	and.b32  	%r367, %r42, 3;
	mov.u32 	%r1241, 0;
	setp.eq.s32	%p23, %r367, 0;
	@%p23 bra 	BB8_47;

	setp.eq.s32	%p24, %r367, 1;
	@%p24 bra 	BB8_43;

	setp.eq.s32	%p25, %r367, 2;
	@%p25 bra 	BB8_39;

	setp.ge.s32	%p26, %r1, %r5;
	@%p26 bra 	BB8_36;

	add.s32 	%r371, %r1, %r16;
	mul.wide.s32 	%rd31, %r371, 8;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.v2.u32 	{%r372, %r373}, [%rd32];
	or.b32  	%r375, %r372, %r373;
	setp.eq.s32	%p27, %r375, 0;
	mov.u32 	%r1241, %r368;
	@%p27 bra 	BB8_39;

	and.b32  	%r377, %r372, 131040;
	and.b32  	%r378, %r372, 31;
	mov.u32 	%r1241, 1;
	shl.b32 	%r379, %r1241, %r378;
	shr.u32 	%r380, %r377, 3;
	add.s32 	%r382, %r238, %r380;
	atom.shared.or.b32 	%r383, [%r382], %r379;
	bra.uni 	BB8_39;

BB8_36:
	mov.u32 	%r1241, %r368;

BB8_39:
	shl.b32 	%r384, %r1241, 10;
	add.s32 	%r45, %r384, %r1;
	setp.ge.s32	%p28, %r45, %r5;
	@%p28 bra 	BB8_42;

	add.s32 	%r385, %r45, %r16;
	mul.wide.s32 	%rd33, %r385, 8;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.v2.u32 	{%r386, %r387}, [%rd34];
	or.b32  	%r389, %r386, %r387;
	setp.eq.s32	%p29, %r389, 0;
	@%p29 bra 	BB8_42;

	and.b32  	%r390, %r386, 131040;
	and.b32  	%r391, %r386, 31;
	mov.u32 	%r392, 1;
	shl.b32 	%r393, %r392, %r391;
	shr.u32 	%r394, %r390, 3;
	add.s32 	%r396, %r238, %r394;
	atom.shared.or.b32 	%r397, [%r396], %r393;

BB8_42:
	add.s32 	%r1241, %r1241, 1;

BB8_43:
	shl.b32 	%r398, %r1241, 10;
	add.s32 	%r49, %r398, %r1;
	setp.ge.s32	%p30, %r49, %r5;
	@%p30 bra 	BB8_46;

	add.s32 	%r399, %r49, %r16;
	mul.wide.s32 	%rd35, %r399, 8;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.v2.u32 	{%r400, %r401}, [%rd36];
	or.b32  	%r403, %r400, %r401;
	setp.eq.s32	%p31, %r403, 0;
	@%p31 bra 	BB8_46;

	and.b32  	%r404, %r400, 131040;
	and.b32  	%r405, %r400, 31;
	mov.u32 	%r406, 1;
	shl.b32 	%r407, %r406, %r405;
	shr.u32 	%r408, %r404, 3;
	add.s32 	%r410, %r238, %r408;
	atom.shared.or.b32 	%r411, [%r410], %r407;

BB8_46:
	add.s32 	%r1241, %r1241, 1;

BB8_47:
	setp.lt.u32	%p32, %r42, 4;
	@%p32 bra 	BB8_62;

	mad.lo.s32 	%r1244, %r1241, 1024, %r1;

BB8_49:
	setp.ge.s32	%p33, %r1244, %r5;
	@%p33 bra 	BB8_52;

	add.s32 	%r412, %r1244, %r16;
	mul.wide.s32 	%rd37, %r412, 8;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.v2.u32 	{%r413, %r414}, [%rd38];
	or.b32  	%r416, %r413, %r414;
	setp.eq.s32	%p34, %r416, 0;
	@%p34 bra 	BB8_52;

	and.b32  	%r417, %r413, 131040;
	and.b32  	%r418, %r413, 31;
	mov.u32 	%r419, 1;
	shl.b32 	%r420, %r419, %r418;
	shr.u32 	%r421, %r417, 3;
	add.s32 	%r423, %r238, %r421;
	atom.shared.or.b32 	%r424, [%r423], %r420;

BB8_52:
	add.s32 	%r57, %r1244, 1024;
	setp.ge.s32	%p35, %r57, %r5;
	@%p35 bra 	BB8_55;

	add.s32 	%r425, %r57, %r16;
	mul.wide.s32 	%rd39, %r425, 8;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.v2.u32 	{%r426, %r427}, [%rd40];
	or.b32  	%r429, %r426, %r427;
	setp.eq.s32	%p36, %r429, 0;
	@%p36 bra 	BB8_55;

	and.b32  	%r430, %r426, 131040;
	and.b32  	%r431, %r426, 31;
	mov.u32 	%r432, 1;
	shl.b32 	%r433, %r432, %r431;
	shr.u32 	%r434, %r430, 3;
	add.s32 	%r436, %r238, %r434;
	atom.shared.or.b32 	%r437, [%r436], %r433;

BB8_55:
	add.s32 	%r59, %r1244, 2048;
	setp.ge.s32	%p37, %r59, %r5;
	@%p37 bra 	BB8_58;

	add.s32 	%r438, %r59, %r16;
	mul.wide.s32 	%rd41, %r438, 8;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.v2.u32 	{%r439, %r440}, [%rd42];
	or.b32  	%r442, %r439, %r440;
	setp.eq.s32	%p38, %r442, 0;
	@%p38 bra 	BB8_58;

	and.b32  	%r443, %r439, 131040;
	and.b32  	%r444, %r439, 31;
	mov.u32 	%r445, 1;
	shl.b32 	%r446, %r445, %r444;
	shr.u32 	%r447, %r443, 3;
	add.s32 	%r449, %r238, %r447;
	atom.shared.or.b32 	%r450, [%r449], %r446;

BB8_58:
	add.s32 	%r61, %r1244, 3072;
	setp.ge.s32	%p39, %r61, %r5;
	@%p39 bra 	BB8_61;

	add.s32 	%r451, %r61, %r16;
	mul.wide.s32 	%rd43, %r451, 8;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.v2.u32 	{%r452, %r453}, [%rd44];
	or.b32  	%r455, %r452, %r453;
	setp.eq.s32	%p40, %r455, 0;
	@%p40 bra 	BB8_61;

	and.b32  	%r456, %r452, 131040;
	and.b32  	%r457, %r452, 31;
	mov.u32 	%r458, 1;
	shl.b32 	%r459, %r458, %r457;
	shr.u32 	%r460, %r456, 3;
	add.s32 	%r462, %r238, %r460;
	atom.shared.or.b32 	%r463, [%r462], %r459;

BB8_61:
	add.s32 	%r1241, %r1241, 4;
	add.s32 	%r1244, %r1244, 4096;
	setp.lt.s32	%p41, %r1241, %r7;
	@%p41 bra 	BB8_49;

BB8_62:
	setp.lt.s32	%p42, %r9, 1024;
	@%p42 bra 	BB8_93;

	mov.u32 	%r468, 1;
	max.s32 	%r65, %r10, %r468;
	and.b32  	%r467, %r65, 3;
	mov.u32 	%r1246, 0;
	setp.eq.s32	%p43, %r467, 0;
	@%p43 bra 	BB8_78;

	setp.eq.s32	%p44, %r467, 1;
	@%p44 bra 	BB8_74;

	setp.eq.s32	%p45, %r467, 2;
	@%p45 bra 	BB8_70;

	setp.ge.s32	%p46, %r1, %r8;
	@%p46 bra 	BB8_67;

	add.s32 	%r471, %r1, %r17;
	mul.wide.s32 	%rd45, %r471, 8;
	add.s64 	%rd46, %rd2, %rd45;
	ld.global.v2.u32 	{%r472, %r473}, [%rd46];
	or.b32  	%r475, %r472, %r473;
	setp.eq.s32	%p47, %r475, 0;
	mov.u32 	%r1246, %r468;
	@%p47 bra 	BB8_70;

	and.b32  	%r477, %r472, 131040;
	and.b32  	%r478, %r472, 31;
	mov.u32 	%r1246, 1;
	shl.b32 	%r479, %r1246, %r478;
	shr.u32 	%r480, %r477, 3;
	add.s32 	%r482, %r238, %r480;
	atom.shared.or.b32 	%r483, [%r482], %r479;
	bra.uni 	BB8_70;

BB8_67:
	mov.u32 	%r1246, %r468;

BB8_70:
	shl.b32 	%r484, %r1246, 10;
	add.s32 	%r68, %r484, %r1;
	setp.ge.s32	%p48, %r68, %r8;
	@%p48 bra 	BB8_73;

	add.s32 	%r485, %r68, %r17;
	mul.wide.s32 	%rd47, %r485, 8;
	add.s64 	%rd48, %rd2, %rd47;
	ld.global.v2.u32 	{%r486, %r487}, [%rd48];
	or.b32  	%r489, %r486, %r487;
	setp.eq.s32	%p49, %r489, 0;
	@%p49 bra 	BB8_73;

	and.b32  	%r490, %r486, 131040;
	and.b32  	%r491, %r486, 31;
	mov.u32 	%r492, 1;
	shl.b32 	%r493, %r492, %r491;
	shr.u32 	%r494, %r490, 3;
	add.s32 	%r496, %r238, %r494;
	atom.shared.or.b32 	%r497, [%r496], %r493;

BB8_73:
	add.s32 	%r1246, %r1246, 1;

BB8_74:
	shl.b32 	%r498, %r1246, 10;
	add.s32 	%r72, %r498, %r1;
	setp.ge.s32	%p50, %r72, %r8;
	@%p50 bra 	BB8_77;

	add.s32 	%r499, %r72, %r17;
	mul.wide.s32 	%rd49, %r499, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.v2.u32 	{%r500, %r501}, [%rd50];
	or.b32  	%r503, %r500, %r501;
	setp.eq.s32	%p51, %r503, 0;
	@%p51 bra 	BB8_77;

	and.b32  	%r504, %r500, 131040;
	and.b32  	%r505, %r500, 31;
	mov.u32 	%r506, 1;
	shl.b32 	%r507, %r506, %r505;
	shr.u32 	%r508, %r504, 3;
	add.s32 	%r510, %r238, %r508;
	atom.shared.or.b32 	%r511, [%r510], %r507;

BB8_77:
	add.s32 	%r1246, %r1246, 1;

BB8_78:
	setp.lt.u32	%p52, %r65, 4;
	@%p52 bra 	BB8_93;

	mad.lo.s32 	%r1249, %r1246, 1024, %r1;

BB8_80:
	setp.ge.s32	%p53, %r1249, %r8;
	@%p53 bra 	BB8_83;

	add.s32 	%r512, %r1249, %r17;
	mul.wide.s32 	%rd51, %r512, 8;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.v2.u32 	{%r513, %r514}, [%rd52];
	or.b32  	%r516, %r513, %r514;
	setp.eq.s32	%p54, %r516, 0;
	@%p54 bra 	BB8_83;

	and.b32  	%r517, %r513, 131040;
	and.b32  	%r518, %r513, 31;
	mov.u32 	%r519, 1;
	shl.b32 	%r520, %r519, %r518;
	shr.u32 	%r521, %r517, 3;
	add.s32 	%r523, %r238, %r521;
	atom.shared.or.b32 	%r524, [%r523], %r520;

BB8_83:
	add.s32 	%r80, %r1249, 1024;
	setp.ge.s32	%p55, %r80, %r8;
	@%p55 bra 	BB8_86;

	add.s32 	%r525, %r80, %r17;
	mul.wide.s32 	%rd53, %r525, 8;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.v2.u32 	{%r526, %r527}, [%rd54];
	or.b32  	%r529, %r526, %r527;
	setp.eq.s32	%p56, %r529, 0;
	@%p56 bra 	BB8_86;

	and.b32  	%r530, %r526, 131040;
	and.b32  	%r531, %r526, 31;
	mov.u32 	%r532, 1;
	shl.b32 	%r533, %r532, %r531;
	shr.u32 	%r534, %r530, 3;
	add.s32 	%r536, %r238, %r534;
	atom.shared.or.b32 	%r537, [%r536], %r533;

BB8_86:
	add.s32 	%r82, %r1249, 2048;
	setp.ge.s32	%p57, %r82, %r8;
	@%p57 bra 	BB8_89;

	add.s32 	%r538, %r82, %r17;
	mul.wide.s32 	%rd55, %r538, 8;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.v2.u32 	{%r539, %r540}, [%rd56];
	or.b32  	%r542, %r539, %r540;
	setp.eq.s32	%p58, %r542, 0;
	@%p58 bra 	BB8_89;

	and.b32  	%r543, %r539, 131040;
	and.b32  	%r544, %r539, 31;
	mov.u32 	%r545, 1;
	shl.b32 	%r546, %r545, %r544;
	shr.u32 	%r547, %r543, 3;
	add.s32 	%r549, %r238, %r547;
	atom.shared.or.b32 	%r550, [%r549], %r546;

BB8_89:
	add.s32 	%r84, %r1249, 3072;
	setp.ge.s32	%p59, %r84, %r8;
	@%p59 bra 	BB8_92;

	add.s32 	%r551, %r84, %r17;
	mul.wide.s32 	%rd57, %r551, 8;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.v2.u32 	{%r552, %r553}, [%rd58];
	or.b32  	%r555, %r552, %r553;
	setp.eq.s32	%p60, %r555, 0;
	@%p60 bra 	BB8_92;

	and.b32  	%r556, %r552, 131040;
	and.b32  	%r557, %r552, 31;
	mov.u32 	%r558, 1;
	shl.b32 	%r559, %r558, %r557;
	shr.u32 	%r560, %r556, 3;
	add.s32 	%r562, %r238, %r560;
	atom.shared.or.b32 	%r563, [%r562], %r559;

BB8_92:
	add.s32 	%r1246, %r1246, 4;
	add.s32 	%r1249, %r1249, 4096;
	setp.lt.s32	%p61, %r1246, %r10;
	@%p61 bra 	BB8_80;

BB8_93:
	setp.lt.s32	%p62, %r12, 1024;
	@%p62 bra 	BB8_124;

	mov.u32 	%r568, 1;
	max.s32 	%r88, %r13, %r568;
	and.b32  	%r567, %r88, 3;
	mov.u32 	%r1251, 0;
	setp.eq.s32	%p63, %r567, 0;
	@%p63 bra 	BB8_109;

	setp.eq.s32	%p64, %r567, 1;
	@%p64 bra 	BB8_105;

	setp.eq.s32	%p65, %r567, 2;
	@%p65 bra 	BB8_101;

	setp.ge.s32	%p66, %r1, %r11;
	@%p66 bra 	BB8_98;

	add.s32 	%r571, %r1, %r18;
	mul.wide.s32 	%rd59, %r571, 8;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.v2.u32 	{%r572, %r573}, [%rd60];
	or.b32  	%r575, %r572, %r573;
	setp.eq.s32	%p67, %r575, 0;
	mov.u32 	%r1251, %r568;
	@%p67 bra 	BB8_101;

	and.b32  	%r577, %r572, 131040;
	and.b32  	%r578, %r572, 31;
	mov.u32 	%r1251, 1;
	shl.b32 	%r579, %r1251, %r578;
	shr.u32 	%r580, %r577, 3;
	add.s32 	%r582, %r238, %r580;
	atom.shared.or.b32 	%r583, [%r582], %r579;
	bra.uni 	BB8_101;

BB8_98:
	mov.u32 	%r1251, %r568;

BB8_101:
	shl.b32 	%r584, %r1251, 10;
	add.s32 	%r91, %r584, %r1;
	setp.ge.s32	%p68, %r91, %r11;
	@%p68 bra 	BB8_104;

	add.s32 	%r585, %r91, %r18;
	mul.wide.s32 	%rd61, %r585, 8;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.v2.u32 	{%r586, %r587}, [%rd62];
	or.b32  	%r589, %r586, %r587;
	setp.eq.s32	%p69, %r589, 0;
	@%p69 bra 	BB8_104;

	and.b32  	%r590, %r586, 131040;
	and.b32  	%r591, %r586, 31;
	mov.u32 	%r592, 1;
	shl.b32 	%r593, %r592, %r591;
	shr.u32 	%r594, %r590, 3;
	add.s32 	%r596, %r238, %r594;
	atom.shared.or.b32 	%r597, [%r596], %r593;

BB8_104:
	add.s32 	%r1251, %r1251, 1;

BB8_105:
	shl.b32 	%r598, %r1251, 10;
	add.s32 	%r95, %r598, %r1;
	setp.ge.s32	%p70, %r95, %r11;
	@%p70 bra 	BB8_108;

	add.s32 	%r599, %r95, %r18;
	mul.wide.s32 	%rd63, %r599, 8;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.v2.u32 	{%r600, %r601}, [%rd64];
	or.b32  	%r603, %r600, %r601;
	setp.eq.s32	%p71, %r603, 0;
	@%p71 bra 	BB8_108;

	and.b32  	%r604, %r600, 131040;
	and.b32  	%r605, %r600, 31;
	mov.u32 	%r606, 1;
	shl.b32 	%r607, %r606, %r605;
	shr.u32 	%r608, %r604, 3;
	add.s32 	%r610, %r238, %r608;
	atom.shared.or.b32 	%r611, [%r610], %r607;

BB8_108:
	add.s32 	%r1251, %r1251, 1;

BB8_109:
	setp.lt.u32	%p72, %r88, 4;
	@%p72 bra 	BB8_124;

	mad.lo.s32 	%r1254, %r1251, 1024, %r1;

BB8_111:
	setp.ge.s32	%p73, %r1254, %r11;
	@%p73 bra 	BB8_114;

	add.s32 	%r612, %r1254, %r18;
	mul.wide.s32 	%rd65, %r612, 8;
	add.s64 	%rd66, %rd2, %rd65;
	ld.global.v2.u32 	{%r613, %r614}, [%rd66];
	or.b32  	%r616, %r613, %r614;
	setp.eq.s32	%p74, %r616, 0;
	@%p74 bra 	BB8_114;

	and.b32  	%r617, %r613, 131040;
	and.b32  	%r618, %r613, 31;
	mov.u32 	%r619, 1;
	shl.b32 	%r620, %r619, %r618;
	shr.u32 	%r621, %r617, 3;
	add.s32 	%r623, %r238, %r621;
	atom.shared.or.b32 	%r624, [%r623], %r620;

BB8_114:
	add.s32 	%r103, %r1254, 1024;
	setp.ge.s32	%p75, %r103, %r11;
	@%p75 bra 	BB8_117;

	add.s32 	%r625, %r103, %r18;
	mul.wide.s32 	%rd67, %r625, 8;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.v2.u32 	{%r626, %r627}, [%rd68];
	or.b32  	%r629, %r626, %r627;
	setp.eq.s32	%p76, %r629, 0;
	@%p76 bra 	BB8_117;

	and.b32  	%r630, %r626, 131040;
	and.b32  	%r631, %r626, 31;
	mov.u32 	%r632, 1;
	shl.b32 	%r633, %r632, %r631;
	shr.u32 	%r634, %r630, 3;
	add.s32 	%r636, %r238, %r634;
	atom.shared.or.b32 	%r637, [%r636], %r633;

BB8_117:
	add.s32 	%r105, %r1254, 2048;
	setp.ge.s32	%p77, %r105, %r11;
	@%p77 bra 	BB8_120;

	add.s32 	%r638, %r105, %r18;
	mul.wide.s32 	%rd69, %r638, 8;
	add.s64 	%rd70, %rd2, %rd69;
	ld.global.v2.u32 	{%r639, %r640}, [%rd70];
	or.b32  	%r642, %r639, %r640;
	setp.eq.s32	%p78, %r642, 0;
	@%p78 bra 	BB8_120;

	and.b32  	%r643, %r639, 131040;
	and.b32  	%r644, %r639, 31;
	mov.u32 	%r645, 1;
	shl.b32 	%r646, %r645, %r644;
	shr.u32 	%r647, %r643, 3;
	add.s32 	%r649, %r238, %r647;
	atom.shared.or.b32 	%r650, [%r649], %r646;

BB8_120:
	add.s32 	%r107, %r1254, 3072;
	setp.ge.s32	%p79, %r107, %r11;
	@%p79 bra 	BB8_123;

	add.s32 	%r651, %r107, %r18;
	mul.wide.s32 	%rd71, %r651, 8;
	add.s64 	%rd72, %rd2, %rd71;
	ld.global.v2.u32 	{%r652, %r653}, [%rd72];
	or.b32  	%r655, %r652, %r653;
	setp.eq.s32	%p80, %r655, 0;
	@%p80 bra 	BB8_123;

	and.b32  	%r656, %r652, 131040;
	and.b32  	%r657, %r652, 31;
	mov.u32 	%r658, 1;
	shl.b32 	%r659, %r658, %r657;
	shr.u32 	%r660, %r656, 3;
	add.s32 	%r662, %r238, %r660;
	atom.shared.or.b32 	%r663, [%r662], %r659;

BB8_123:
	add.s32 	%r1251, %r1251, 4;
	add.s32 	%r1254, %r1254, 4096;
	setp.lt.s32	%p81, %r1251, %r13;
	@%p81 bra 	BB8_111;

BB8_124:
	setp.gt.s32	%p1, %r3, 1023;
	bar.sync 	0;
	@!%p1 bra 	BB8_162;
	bra.uni 	BB8_125;

BB8_125:
	add.s32 	%r111, %r235, -1;
	mov.u32 	%r668, 1;
	max.s32 	%r112, %r4, %r668;
	and.b32  	%r667, %r112, 3;
	mov.u32 	%r1256, 0;
	setp.eq.s32	%p82, %r667, 0;
	@%p82 bra 	BB8_143;

	setp.eq.s32	%p83, %r667, 1;
	@%p83 bra 	BB8_138;

	setp.eq.s32	%p84, %r667, 2;
	@%p84 bra 	BB8_133;

	setp.ge.s32	%p85, %r1, %r2;
	@%p85 bra 	BB8_129;

	add.s32 	%r671, %r1, %r15;
	mul.wide.s32 	%rd73, %r671, 8;
	add.s64 	%rd74, %rd2, %rd73;
	ld.global.v2.u32 	{%r672, %r673}, [%rd74];
	or.b32  	%r674, %r672, %r673;
	setp.eq.s32	%p86, %r674, 0;
	mov.u32 	%r1256, %r668;
	@%p86 bra 	BB8_133;

	and.b32  	%r676, %r672, 131040;
	and.b32  	%r677, %r672, 31;
	xor.b32  	%r678, %r677, 1;
	shr.u32 	%r679, %r676, 3;
	add.s32 	%r681, %r238, %r679;
	mov.u32 	%r1256, 1;
	shl.b32 	%r682, %r1256, %r678;
	ld.shared.u32 	%r683, [%r681];
	and.b32  	%r684, %r683, %r682;
	setp.eq.s32	%p87, %r684, 0;
	@%p87 bra 	BB8_133;

	bfe.u32 	%r686, %r673, 17, 12;
	add.s32 	%r687, %r686, %r14;
	mul.wide.s32 	%rd75, %r687, 4;
	add.s64 	%rd76, %rd3, %rd75;
	atom.global.add.u32 	%r688, [%rd76], 1;
	min.s32 	%r689, %r688, %r111;
	mad.lo.s32 	%r690, %r686, %r235, %r689;
	mul.wide.s32 	%rd77, %r690, 8;
	add.s64 	%rd78, %rd1, %rd77;
	st.global.v2.u32 	[%rd78], {%r673, %r672};
	bra.uni 	BB8_133;

BB8_129:
	mov.u32 	%r1256, %r668;

BB8_133:
	shl.b32 	%r691, %r1256, 10;
	add.s32 	%r116, %r691, %r1;
	setp.ge.s32	%p88, %r116, %r2;
	@%p88 bra 	BB8_137;

	add.s32 	%r692, %r116, %r15;
	mul.wide.s32 	%rd79, %r692, 8;
	add.s64 	%rd80, %rd2, %rd79;
	ld.global.v2.u32 	{%r693, %r694}, [%rd80];
	or.b32  	%r695, %r693, %r694;
	setp.eq.s32	%p89, %r695, 0;
	@%p89 bra 	BB8_137;

	and.b32  	%r696, %r693, 131040;
	and.b32  	%r697, %r693, 31;
	xor.b32  	%r698, %r697, 1;
	shr.u32 	%r699, %r696, 3;
	add.s32 	%r701, %r238, %r699;
	mov.u32 	%r702, 1;
	shl.b32 	%r703, %r702, %r698;
	ld.shared.u32 	%r704, [%r701];
	and.b32  	%r705, %r704, %r703;
	setp.eq.s32	%p90, %r705, 0;
	@%p90 bra 	BB8_137;

	bfe.u32 	%r706, %r694, 17, 12;
	add.s32 	%r707, %r706, %r14;
	mul.wide.s32 	%rd81, %r707, 4;
	add.s64 	%rd82, %rd3, %rd81;
	atom.global.add.u32 	%r708, [%rd82], 1;
	min.s32 	%r709, %r708, %r111;
	mad.lo.s32 	%r710, %r706, %r235, %r709;
	mul.wide.s32 	%rd83, %r710, 8;
	add.s64 	%rd84, %rd1, %rd83;
	st.global.v2.u32 	[%rd84], {%r694, %r693};

BB8_137:
	add.s32 	%r1256, %r1256, 1;

BB8_138:
	shl.b32 	%r711, %r1256, 10;
	add.s32 	%r121, %r711, %r1;
	setp.ge.s32	%p91, %r121, %r2;
	@%p91 bra 	BB8_142;

	add.s32 	%r712, %r121, %r15;
	mul.wide.s32 	%rd85, %r712, 8;
	add.s64 	%rd86, %rd2, %rd85;
	ld.global.v2.u32 	{%r713, %r714}, [%rd86];
	or.b32  	%r715, %r713, %r714;
	setp.eq.s32	%p92, %r715, 0;
	@%p92 bra 	BB8_142;

	and.b32  	%r716, %r713, 131040;
	and.b32  	%r717, %r713, 31;
	xor.b32  	%r718, %r717, 1;
	shr.u32 	%r719, %r716, 3;
	add.s32 	%r721, %r238, %r719;
	mov.u32 	%r722, 1;
	shl.b32 	%r723, %r722, %r718;
	ld.shared.u32 	%r724, [%r721];
	and.b32  	%r725, %r724, %r723;
	setp.eq.s32	%p93, %r725, 0;
	@%p93 bra 	BB8_142;

	bfe.u32 	%r726, %r714, 17, 12;
	add.s32 	%r727, %r726, %r14;
	mul.wide.s32 	%rd87, %r727, 4;
	add.s64 	%rd88, %rd3, %rd87;
	atom.global.add.u32 	%r728, [%rd88], 1;
	min.s32 	%r729, %r728, %r111;
	mad.lo.s32 	%r730, %r726, %r235, %r729;
	mul.wide.s32 	%rd89, %r730, 8;
	add.s64 	%rd90, %rd1, %rd89;
	st.global.v2.u32 	[%rd90], {%r714, %r713};

BB8_142:
	add.s32 	%r1256, %r1256, 1;

BB8_143:
	setp.lt.u32	%p94, %r112, 4;
	@%p94 bra 	BB8_162;

	mad.lo.s32 	%r1259, %r1256, 1024, %r1;

BB8_145:
	setp.ge.s32	%p95, %r1259, %r2;
	@%p95 bra 	BB8_149;

	add.s32 	%r731, %r1259, %r15;
	mul.wide.s32 	%rd91, %r731, 8;
	add.s64 	%rd92, %rd2, %rd91;
	ld.global.v2.u32 	{%r732, %r733}, [%rd92];
	or.b32  	%r734, %r732, %r733;
	setp.eq.s32	%p96, %r734, 0;
	@%p96 bra 	BB8_149;

	and.b32  	%r735, %r732, 131040;
	and.b32  	%r736, %r732, 31;
	xor.b32  	%r737, %r736, 1;
	shr.u32 	%r738, %r735, 3;
	add.s32 	%r740, %r238, %r738;
	mov.u32 	%r741, 1;
	shl.b32 	%r742, %r741, %r737;
	ld.shared.u32 	%r743, [%r740];
	and.b32  	%r744, %r743, %r742;
	setp.eq.s32	%p97, %r744, 0;
	@%p97 bra 	BB8_149;

	bfe.u32 	%r745, %r733, 17, 12;
	add.s32 	%r746, %r745, %r14;
	mul.wide.s32 	%rd93, %r746, 4;
	add.s64 	%rd94, %rd3, %rd93;
	atom.global.add.u32 	%r747, [%rd94], 1;
	min.s32 	%r748, %r747, %r111;
	mad.lo.s32 	%r749, %r745, %r235, %r748;
	mul.wide.s32 	%rd95, %r749, 8;
	add.s64 	%rd96, %rd1, %rd95;
	st.global.v2.u32 	[%rd96], {%r733, %r732};

BB8_149:
	add.s32 	%r131, %r1259, 1024;
	setp.ge.s32	%p98, %r131, %r2;
	@%p98 bra 	BB8_153;

	add.s32 	%r750, %r131, %r15;
	mul.wide.s32 	%rd97, %r750, 8;
	add.s64 	%rd98, %rd2, %rd97;
	ld.global.v2.u32 	{%r751, %r752}, [%rd98];
	or.b32  	%r753, %r751, %r752;
	setp.eq.s32	%p99, %r753, 0;
	@%p99 bra 	BB8_153;

	and.b32  	%r754, %r751, 131040;
	and.b32  	%r755, %r751, 31;
	xor.b32  	%r756, %r755, 1;
	shr.u32 	%r757, %r754, 3;
	add.s32 	%r759, %r238, %r757;
	mov.u32 	%r760, 1;
	shl.b32 	%r761, %r760, %r756;
	ld.shared.u32 	%r762, [%r759];
	and.b32  	%r763, %r762, %r761;
	setp.eq.s32	%p100, %r763, 0;
	@%p100 bra 	BB8_153;

	bfe.u32 	%r764, %r752, 17, 12;
	add.s32 	%r765, %r764, %r14;
	mul.wide.s32 	%rd99, %r765, 4;
	add.s64 	%rd100, %rd3, %rd99;
	atom.global.add.u32 	%r766, [%rd100], 1;
	min.s32 	%r767, %r766, %r111;
	mad.lo.s32 	%r768, %r764, %r235, %r767;
	mul.wide.s32 	%rd101, %r768, 8;
	add.s64 	%rd102, %rd1, %rd101;
	st.global.v2.u32 	[%rd102], {%r752, %r751};

BB8_153:
	add.s32 	%r134, %r1259, 2048;
	setp.ge.s32	%p101, %r134, %r2;
	@%p101 bra 	BB8_157;

	add.s32 	%r769, %r134, %r15;
	mul.wide.s32 	%rd103, %r769, 8;
	add.s64 	%rd104, %rd2, %rd103;
	ld.global.v2.u32 	{%r770, %r771}, [%rd104];
	or.b32  	%r772, %r770, %r771;
	setp.eq.s32	%p102, %r772, 0;
	@%p102 bra 	BB8_157;

	and.b32  	%r773, %r770, 131040;
	and.b32  	%r774, %r770, 31;
	xor.b32  	%r775, %r774, 1;
	shr.u32 	%r776, %r773, 3;
	add.s32 	%r778, %r238, %r776;
	mov.u32 	%r779, 1;
	shl.b32 	%r780, %r779, %r775;
	ld.shared.u32 	%r781, [%r778];
	and.b32  	%r782, %r781, %r780;
	setp.eq.s32	%p103, %r782, 0;
	@%p103 bra 	BB8_157;

	bfe.u32 	%r783, %r771, 17, 12;
	add.s32 	%r784, %r783, %r14;
	mul.wide.s32 	%rd105, %r784, 4;
	add.s64 	%rd106, %rd3, %rd105;
	atom.global.add.u32 	%r785, [%rd106], 1;
	min.s32 	%r786, %r785, %r111;
	mad.lo.s32 	%r787, %r783, %r235, %r786;
	mul.wide.s32 	%rd107, %r787, 8;
	add.s64 	%rd108, %rd1, %rd107;
	st.global.v2.u32 	[%rd108], {%r771, %r770};

BB8_157:
	add.s32 	%r137, %r1259, 3072;
	setp.ge.s32	%p104, %r137, %r2;
	@%p104 bra 	BB8_161;

	add.s32 	%r788, %r137, %r15;
	mul.wide.s32 	%rd109, %r788, 8;
	add.s64 	%rd110, %rd2, %rd109;
	ld.global.v2.u32 	{%r789, %r790}, [%rd110];
	or.b32  	%r791, %r789, %r790;
	setp.eq.s32	%p105, %r791, 0;
	@%p105 bra 	BB8_161;

	and.b32  	%r792, %r789, 131040;
	and.b32  	%r793, %r789, 31;
	xor.b32  	%r794, %r793, 1;
	shr.u32 	%r795, %r792, 3;
	add.s32 	%r797, %r238, %r795;
	mov.u32 	%r798, 1;
	shl.b32 	%r799, %r798, %r794;
	ld.shared.u32 	%r800, [%r797];
	and.b32  	%r801, %r800, %r799;
	setp.eq.s32	%p106, %r801, 0;
	@%p106 bra 	BB8_161;

	bfe.u32 	%r802, %r790, 17, 12;
	add.s32 	%r803, %r802, %r14;
	mul.wide.s32 	%rd111, %r803, 4;
	add.s64 	%rd112, %rd3, %rd111;
	atom.global.add.u32 	%r804, [%rd112], 1;
	min.s32 	%r805, %r804, %r111;
	mad.lo.s32 	%r806, %r802, %r235, %r805;
	mul.wide.s32 	%rd113, %r806, 8;
	add.s64 	%rd114, %rd1, %rd113;
	st.global.v2.u32 	[%rd114], {%r790, %r789};

BB8_161:
	add.s32 	%r1256, %r1256, 4;
	add.s32 	%r1259, %r1259, 4096;
	setp.lt.s32	%p107, %r1256, %r4;
	@%p107 bra 	BB8_145;

BB8_162:
	@%p22 bra 	BB8_200;

	add.s32 	%r142, %r235, -1;
	mov.u32 	%r811, 1;
	max.s32 	%r143, %r7, %r811;
	and.b32  	%r810, %r143, 3;
	mov.u32 	%r1261, 0;
	setp.eq.s32	%p109, %r810, 0;
	@%p109 bra 	BB8_181;

	setp.eq.s32	%p110, %r810, 1;
	@%p110 bra 	BB8_176;

	setp.eq.s32	%p111, %r810, 2;
	@%p111 bra 	BB8_171;

	setp.ge.s32	%p112, %r1, %r5;
	@%p112 bra 	BB8_167;

	add.s32 	%r814, %r1, %r16;
	mul.wide.s32 	%rd115, %r814, 8;
	add.s64 	%rd116, %rd2, %rd115;
	ld.global.v2.u32 	{%r815, %r816}, [%rd116];
	or.b32  	%r817, %r815, %r816;
	setp.eq.s32	%p113, %r817, 0;
	mov.u32 	%r1261, %r811;
	@%p113 bra 	BB8_171;

	and.b32  	%r819, %r815, 131040;
	and.b32  	%r820, %r815, 31;
	xor.b32  	%r821, %r820, 1;
	shr.u32 	%r822, %r819, 3;
	add.s32 	%r824, %r238, %r822;
	mov.u32 	%r1261, 1;
	shl.b32 	%r825, %r1261, %r821;
	ld.shared.u32 	%r826, [%r824];
	and.b32  	%r827, %r826, %r825;
	setp.eq.s32	%p114, %r827, 0;
	@%p114 bra 	BB8_171;

	bfe.u32 	%r829, %r816, 17, 12;
	add.s32 	%r830, %r829, %r14;
	mul.wide.s32 	%rd117, %r830, 4;
	add.s64 	%rd118, %rd3, %rd117;
	atom.global.add.u32 	%r831, [%rd118], 1;
	min.s32 	%r832, %r831, %r142;
	mad.lo.s32 	%r833, %r829, %r235, %r832;
	mul.wide.s32 	%rd119, %r833, 8;
	add.s64 	%rd120, %rd1, %rd119;
	st.global.v2.u32 	[%rd120], {%r816, %r815};
	bra.uni 	BB8_171;

BB8_167:
	mov.u32 	%r1261, %r811;

BB8_171:
	shl.b32 	%r834, %r1261, 10;
	add.s32 	%r147, %r834, %r1;
	setp.ge.s32	%p115, %r147, %r5;
	@%p115 bra 	BB8_175;

	add.s32 	%r835, %r147, %r16;
	mul.wide.s32 	%rd121, %r835, 8;
	add.s64 	%rd122, %rd2, %rd121;
	ld.global.v2.u32 	{%r836, %r837}, [%rd122];
	or.b32  	%r838, %r836, %r837;
	setp.eq.s32	%p116, %r838, 0;
	@%p116 bra 	BB8_175;

	and.b32  	%r839, %r836, 131040;
	and.b32  	%r840, %r836, 31;
	xor.b32  	%r841, %r840, 1;
	shr.u32 	%r842, %r839, 3;
	add.s32 	%r844, %r238, %r842;
	mov.u32 	%r845, 1;
	shl.b32 	%r846, %r845, %r841;
	ld.shared.u32 	%r847, [%r844];
	and.b32  	%r848, %r847, %r846;
	setp.eq.s32	%p117, %r848, 0;
	@%p117 bra 	BB8_175;

	bfe.u32 	%r849, %r837, 17, 12;
	add.s32 	%r850, %r849, %r14;
	mul.wide.s32 	%rd123, %r850, 4;
	add.s64 	%rd124, %rd3, %rd123;
	atom.global.add.u32 	%r851, [%rd124], 1;
	min.s32 	%r852, %r851, %r142;
	mad.lo.s32 	%r853, %r849, %r235, %r852;
	mul.wide.s32 	%rd125, %r853, 8;
	add.s64 	%rd126, %rd1, %rd125;
	st.global.v2.u32 	[%rd126], {%r837, %r836};

BB8_175:
	add.s32 	%r1261, %r1261, 1;

BB8_176:
	shl.b32 	%r854, %r1261, 10;
	add.s32 	%r152, %r854, %r1;
	setp.ge.s32	%p118, %r152, %r5;
	@%p118 bra 	BB8_180;

	add.s32 	%r855, %r152, %r16;
	mul.wide.s32 	%rd127, %r855, 8;
	add.s64 	%rd128, %rd2, %rd127;
	ld.global.v2.u32 	{%r856, %r857}, [%rd128];
	or.b32  	%r858, %r856, %r857;
	setp.eq.s32	%p119, %r858, 0;
	@%p119 bra 	BB8_180;

	and.b32  	%r859, %r856, 131040;
	and.b32  	%r860, %r856, 31;
	xor.b32  	%r861, %r860, 1;
	shr.u32 	%r862, %r859, 3;
	add.s32 	%r864, %r238, %r862;
	mov.u32 	%r865, 1;
	shl.b32 	%r866, %r865, %r861;
	ld.shared.u32 	%r867, [%r864];
	and.b32  	%r868, %r867, %r866;
	setp.eq.s32	%p120, %r868, 0;
	@%p120 bra 	BB8_180;

	bfe.u32 	%r869, %r857, 17, 12;
	add.s32 	%r870, %r869, %r14;
	mul.wide.s32 	%rd129, %r870, 4;
	add.s64 	%rd130, %rd3, %rd129;
	atom.global.add.u32 	%r871, [%rd130], 1;
	min.s32 	%r872, %r871, %r142;
	mad.lo.s32 	%r873, %r869, %r235, %r872;
	mul.wide.s32 	%rd131, %r873, 8;
	add.s64 	%rd132, %rd1, %rd131;
	st.global.v2.u32 	[%rd132], {%r857, %r856};

BB8_180:
	add.s32 	%r1261, %r1261, 1;

BB8_181:
	setp.lt.u32	%p121, %r143, 4;
	@%p121 bra 	BB8_200;

	mad.lo.s32 	%r1264, %r1261, 1024, %r1;

BB8_183:
	setp.ge.s32	%p122, %r1264, %r5;
	@%p122 bra 	BB8_187;

	add.s32 	%r874, %r1264, %r16;
	mul.wide.s32 	%rd133, %r874, 8;
	add.s64 	%rd134, %rd2, %rd133;
	ld.global.v2.u32 	{%r875, %r876}, [%rd134];
	or.b32  	%r877, %r875, %r876;
	setp.eq.s32	%p123, %r877, 0;
	@%p123 bra 	BB8_187;

	and.b32  	%r878, %r875, 131040;
	and.b32  	%r879, %r875, 31;
	xor.b32  	%r880, %r879, 1;
	shr.u32 	%r881, %r878, 3;
	add.s32 	%r883, %r238, %r881;
	mov.u32 	%r884, 1;
	shl.b32 	%r885, %r884, %r880;
	ld.shared.u32 	%r886, [%r883];
	and.b32  	%r887, %r886, %r885;
	setp.eq.s32	%p124, %r887, 0;
	@%p124 bra 	BB8_187;

	bfe.u32 	%r888, %r876, 17, 12;
	add.s32 	%r889, %r888, %r14;
	mul.wide.s32 	%rd135, %r889, 4;
	add.s64 	%rd136, %rd3, %rd135;
	atom.global.add.u32 	%r890, [%rd136], 1;
	min.s32 	%r891, %r890, %r142;
	mad.lo.s32 	%r892, %r888, %r235, %r891;
	mul.wide.s32 	%rd137, %r892, 8;
	add.s64 	%rd138, %rd1, %rd137;
	st.global.v2.u32 	[%rd138], {%r876, %r875};

BB8_187:
	add.s32 	%r162, %r1264, 1024;
	setp.ge.s32	%p125, %r162, %r5;
	@%p125 bra 	BB8_191;

	add.s32 	%r893, %r162, %r16;
	mul.wide.s32 	%rd139, %r893, 8;
	add.s64 	%rd140, %rd2, %rd139;
	ld.global.v2.u32 	{%r894, %r895}, [%rd140];
	or.b32  	%r896, %r894, %r895;
	setp.eq.s32	%p126, %r896, 0;
	@%p126 bra 	BB8_191;

	and.b32  	%r897, %r894, 131040;
	and.b32  	%r898, %r894, 31;
	xor.b32  	%r899, %r898, 1;
	shr.u32 	%r900, %r897, 3;
	add.s32 	%r902, %r238, %r900;
	mov.u32 	%r903, 1;
	shl.b32 	%r904, %r903, %r899;
	ld.shared.u32 	%r905, [%r902];
	and.b32  	%r906, %r905, %r904;
	setp.eq.s32	%p127, %r906, 0;
	@%p127 bra 	BB8_191;

	bfe.u32 	%r907, %r895, 17, 12;
	add.s32 	%r908, %r907, %r14;
	mul.wide.s32 	%rd141, %r908, 4;
	add.s64 	%rd142, %rd3, %rd141;
	atom.global.add.u32 	%r909, [%rd142], 1;
	min.s32 	%r910, %r909, %r142;
	mad.lo.s32 	%r911, %r907, %r235, %r910;
	mul.wide.s32 	%rd143, %r911, 8;
	add.s64 	%rd144, %rd1, %rd143;
	st.global.v2.u32 	[%rd144], {%r895, %r894};

BB8_191:
	add.s32 	%r165, %r1264, 2048;
	setp.ge.s32	%p128, %r165, %r5;
	@%p128 bra 	BB8_195;

	add.s32 	%r912, %r165, %r16;
	mul.wide.s32 	%rd145, %r912, 8;
	add.s64 	%rd146, %rd2, %rd145;
	ld.global.v2.u32 	{%r913, %r914}, [%rd146];
	or.b32  	%r915, %r913, %r914;
	setp.eq.s32	%p129, %r915, 0;
	@%p129 bra 	BB8_195;

	and.b32  	%r916, %r913, 131040;
	and.b32  	%r917, %r913, 31;
	xor.b32  	%r918, %r917, 1;
	shr.u32 	%r919, %r916, 3;
	add.s32 	%r921, %r238, %r919;
	mov.u32 	%r922, 1;
	shl.b32 	%r923, %r922, %r918;
	ld.shared.u32 	%r924, [%r921];
	and.b32  	%r925, %r924, %r923;
	setp.eq.s32	%p130, %r925, 0;
	@%p130 bra 	BB8_195;

	bfe.u32 	%r926, %r914, 17, 12;
	add.s32 	%r927, %r926, %r14;
	mul.wide.s32 	%rd147, %r927, 4;
	add.s64 	%rd148, %rd3, %rd147;
	atom.global.add.u32 	%r928, [%rd148], 1;
	min.s32 	%r929, %r928, %r142;
	mad.lo.s32 	%r930, %r926, %r235, %r929;
	mul.wide.s32 	%rd149, %r930, 8;
	add.s64 	%rd150, %rd1, %rd149;
	st.global.v2.u32 	[%rd150], {%r914, %r913};

BB8_195:
	add.s32 	%r168, %r1264, 3072;
	setp.ge.s32	%p131, %r168, %r5;
	@%p131 bra 	BB8_199;

	add.s32 	%r931, %r168, %r16;
	mul.wide.s32 	%rd151, %r931, 8;
	add.s64 	%rd152, %rd2, %rd151;
	ld.global.v2.u32 	{%r932, %r933}, [%rd152];
	or.b32  	%r934, %r932, %r933;
	setp.eq.s32	%p132, %r934, 0;
	@%p132 bra 	BB8_199;

	and.b32  	%r935, %r932, 131040;
	and.b32  	%r936, %r932, 31;
	xor.b32  	%r937, %r936, 1;
	shr.u32 	%r938, %r935, 3;
	add.s32 	%r940, %r238, %r938;
	mov.u32 	%r941, 1;
	shl.b32 	%r942, %r941, %r937;
	ld.shared.u32 	%r943, [%r940];
	and.b32  	%r944, %r943, %r942;
	setp.eq.s32	%p133, %r944, 0;
	@%p133 bra 	BB8_199;

	bfe.u32 	%r945, %r933, 17, 12;
	add.s32 	%r946, %r945, %r14;
	mul.wide.s32 	%rd153, %r946, 4;
	add.s64 	%rd154, %rd3, %rd153;
	atom.global.add.u32 	%r947, [%rd154], 1;
	min.s32 	%r948, %r947, %r142;
	mad.lo.s32 	%r949, %r945, %r235, %r948;
	mul.wide.s32 	%rd155, %r949, 8;
	add.s64 	%rd156, %rd1, %rd155;
	st.global.v2.u32 	[%rd156], {%r933, %r932};

BB8_199:
	add.s32 	%r1261, %r1261, 4;
	add.s32 	%r1264, %r1264, 4096;
	setp.lt.s32	%p134, %r1261, %r7;
	@%p134 bra 	BB8_183;

BB8_200:
	@%p42 bra 	BB8_238;

	add.s32 	%r173, %r235, -1;
	mov.u32 	%r954, 1;
	max.s32 	%r174, %r10, %r954;
	and.b32  	%r953, %r174, 3;
	mov.u32 	%r1266, 0;
	setp.eq.s32	%p136, %r953, 0;
	@%p136 bra 	BB8_219;

	setp.eq.s32	%p137, %r953, 1;
	@%p137 bra 	BB8_214;

	setp.eq.s32	%p138, %r953, 2;
	@%p138 bra 	BB8_209;

	setp.ge.s32	%p139, %r1, %r8;
	@%p139 bra 	BB8_205;

	add.s32 	%r957, %r1, %r17;
	mul.wide.s32 	%rd157, %r957, 8;
	add.s64 	%rd158, %rd2, %rd157;
	ld.global.v2.u32 	{%r958, %r959}, [%rd158];
	or.b32  	%r960, %r958, %r959;
	setp.eq.s32	%p140, %r960, 0;
	mov.u32 	%r1266, %r954;
	@%p140 bra 	BB8_209;

	and.b32  	%r962, %r958, 131040;
	and.b32  	%r963, %r958, 31;
	xor.b32  	%r964, %r963, 1;
	shr.u32 	%r965, %r962, 3;
	add.s32 	%r967, %r238, %r965;
	mov.u32 	%r1266, 1;
	shl.b32 	%r968, %r1266, %r964;
	ld.shared.u32 	%r969, [%r967];
	and.b32  	%r970, %r969, %r968;
	setp.eq.s32	%p141, %r970, 0;
	@%p141 bra 	BB8_209;

	bfe.u32 	%r972, %r959, 17, 12;
	add.s32 	%r973, %r972, %r14;
	mul.wide.s32 	%rd159, %r973, 4;
	add.s64 	%rd160, %rd3, %rd159;
	atom.global.add.u32 	%r974, [%rd160], 1;
	min.s32 	%r975, %r974, %r173;
	mad.lo.s32 	%r976, %r972, %r235, %r975;
	mul.wide.s32 	%rd161, %r976, 8;
	add.s64 	%rd162, %rd1, %rd161;
	st.global.v2.u32 	[%rd162], {%r959, %r958};
	bra.uni 	BB8_209;

BB8_205:
	mov.u32 	%r1266, %r954;

BB8_209:
	shl.b32 	%r977, %r1266, 10;
	add.s32 	%r178, %r977, %r1;
	setp.ge.s32	%p142, %r178, %r8;
	@%p142 bra 	BB8_213;

	add.s32 	%r978, %r178, %r17;
	mul.wide.s32 	%rd163, %r978, 8;
	add.s64 	%rd164, %rd2, %rd163;
	ld.global.v2.u32 	{%r979, %r980}, [%rd164];
	or.b32  	%r981, %r979, %r980;
	setp.eq.s32	%p143, %r981, 0;
	@%p143 bra 	BB8_213;

	and.b32  	%r982, %r979, 131040;
	and.b32  	%r983, %r979, 31;
	xor.b32  	%r984, %r983, 1;
	shr.u32 	%r985, %r982, 3;
	add.s32 	%r987, %r238, %r985;
	mov.u32 	%r988, 1;
	shl.b32 	%r989, %r988, %r984;
	ld.shared.u32 	%r990, [%r987];
	and.b32  	%r991, %r990, %r989;
	setp.eq.s32	%p144, %r991, 0;
	@%p144 bra 	BB8_213;

	bfe.u32 	%r992, %r980, 17, 12;
	add.s32 	%r993, %r992, %r14;
	mul.wide.s32 	%rd165, %r993, 4;
	add.s64 	%rd166, %rd3, %rd165;
	atom.global.add.u32 	%r994, [%rd166], 1;
	min.s32 	%r995, %r994, %r173;
	mad.lo.s32 	%r996, %r992, %r235, %r995;
	mul.wide.s32 	%rd167, %r996, 8;
	add.s64 	%rd168, %rd1, %rd167;
	st.global.v2.u32 	[%rd168], {%r980, %r979};

BB8_213:
	add.s32 	%r1266, %r1266, 1;

BB8_214:
	shl.b32 	%r997, %r1266, 10;
	add.s32 	%r183, %r997, %r1;
	setp.ge.s32	%p145, %r183, %r8;
	@%p145 bra 	BB8_218;

	add.s32 	%r998, %r183, %r17;
	mul.wide.s32 	%rd169, %r998, 8;
	add.s64 	%rd170, %rd2, %rd169;
	ld.global.v2.u32 	{%r999, %r1000}, [%rd170];
	or.b32  	%r1001, %r999, %r1000;
	setp.eq.s32	%p146, %r1001, 0;
	@%p146 bra 	BB8_218;

	and.b32  	%r1002, %r999, 131040;
	and.b32  	%r1003, %r999, 31;
	xor.b32  	%r1004, %r1003, 1;
	shr.u32 	%r1005, %r1002, 3;
	add.s32 	%r1007, %r238, %r1005;
	mov.u32 	%r1008, 1;
	shl.b32 	%r1009, %r1008, %r1004;
	ld.shared.u32 	%r1010, [%r1007];
	and.b32  	%r1011, %r1010, %r1009;
	setp.eq.s32	%p147, %r1011, 0;
	@%p147 bra 	BB8_218;

	bfe.u32 	%r1012, %r1000, 17, 12;
	add.s32 	%r1013, %r1012, %r14;
	mul.wide.s32 	%rd171, %r1013, 4;
	add.s64 	%rd172, %rd3, %rd171;
	atom.global.add.u32 	%r1014, [%rd172], 1;
	min.s32 	%r1015, %r1014, %r173;
	mad.lo.s32 	%r1016, %r1012, %r235, %r1015;
	mul.wide.s32 	%rd173, %r1016, 8;
	add.s64 	%rd174, %rd1, %rd173;
	st.global.v2.u32 	[%rd174], {%r1000, %r999};

BB8_218:
	add.s32 	%r1266, %r1266, 1;

BB8_219:
	setp.lt.u32	%p148, %r174, 4;
	@%p148 bra 	BB8_238;

	mad.lo.s32 	%r1269, %r1266, 1024, %r1;

BB8_221:
	setp.ge.s32	%p149, %r1269, %r8;
	@%p149 bra 	BB8_225;

	add.s32 	%r1017, %r1269, %r17;
	mul.wide.s32 	%rd175, %r1017, 8;
	add.s64 	%rd176, %rd2, %rd175;
	ld.global.v2.u32 	{%r1018, %r1019}, [%rd176];
	or.b32  	%r1020, %r1018, %r1019;
	setp.eq.s32	%p150, %r1020, 0;
	@%p150 bra 	BB8_225;

	and.b32  	%r1021, %r1018, 131040;
	and.b32  	%r1022, %r1018, 31;
	xor.b32  	%r1023, %r1022, 1;
	shr.u32 	%r1024, %r1021, 3;
	add.s32 	%r1026, %r238, %r1024;
	mov.u32 	%r1027, 1;
	shl.b32 	%r1028, %r1027, %r1023;
	ld.shared.u32 	%r1029, [%r1026];
	and.b32  	%r1030, %r1029, %r1028;
	setp.eq.s32	%p151, %r1030, 0;
	@%p151 bra 	BB8_225;

	bfe.u32 	%r1031, %r1019, 17, 12;
	add.s32 	%r1032, %r1031, %r14;
	mul.wide.s32 	%rd177, %r1032, 4;
	add.s64 	%rd178, %rd3, %rd177;
	atom.global.add.u32 	%r1033, [%rd178], 1;
	min.s32 	%r1034, %r1033, %r173;
	mad.lo.s32 	%r1035, %r1031, %r235, %r1034;
	mul.wide.s32 	%rd179, %r1035, 8;
	add.s64 	%rd180, %rd1, %rd179;
	st.global.v2.u32 	[%rd180], {%r1019, %r1018};

BB8_225:
	add.s32 	%r193, %r1269, 1024;
	setp.ge.s32	%p152, %r193, %r8;
	@%p152 bra 	BB8_229;

	add.s32 	%r1036, %r193, %r17;
	mul.wide.s32 	%rd181, %r1036, 8;
	add.s64 	%rd182, %rd2, %rd181;
	ld.global.v2.u32 	{%r1037, %r1038}, [%rd182];
	or.b32  	%r1039, %r1037, %r1038;
	setp.eq.s32	%p153, %r1039, 0;
	@%p153 bra 	BB8_229;

	and.b32  	%r1040, %r1037, 131040;
	and.b32  	%r1041, %r1037, 31;
	xor.b32  	%r1042, %r1041, 1;
	shr.u32 	%r1043, %r1040, 3;
	add.s32 	%r1045, %r238, %r1043;
	mov.u32 	%r1046, 1;
	shl.b32 	%r1047, %r1046, %r1042;
	ld.shared.u32 	%r1048, [%r1045];
	and.b32  	%r1049, %r1048, %r1047;
	setp.eq.s32	%p154, %r1049, 0;
	@%p154 bra 	BB8_229;

	bfe.u32 	%r1050, %r1038, 17, 12;
	add.s32 	%r1051, %r1050, %r14;
	mul.wide.s32 	%rd183, %r1051, 4;
	add.s64 	%rd184, %rd3, %rd183;
	atom.global.add.u32 	%r1052, [%rd184], 1;
	min.s32 	%r1053, %r1052, %r173;
	mad.lo.s32 	%r1054, %r1050, %r235, %r1053;
	mul.wide.s32 	%rd185, %r1054, 8;
	add.s64 	%rd186, %rd1, %rd185;
	st.global.v2.u32 	[%rd186], {%r1038, %r1037};

BB8_229:
	add.s32 	%r196, %r1269, 2048;
	setp.ge.s32	%p155, %r196, %r8;
	@%p155 bra 	BB8_233;

	add.s32 	%r1055, %r196, %r17;
	mul.wide.s32 	%rd187, %r1055, 8;
	add.s64 	%rd188, %rd2, %rd187;
	ld.global.v2.u32 	{%r1056, %r1057}, [%rd188];
	or.b32  	%r1058, %r1056, %r1057;
	setp.eq.s32	%p156, %r1058, 0;
	@%p156 bra 	BB8_233;

	and.b32  	%r1059, %r1056, 131040;
	and.b32  	%r1060, %r1056, 31;
	xor.b32  	%r1061, %r1060, 1;
	shr.u32 	%r1062, %r1059, 3;
	add.s32 	%r1064, %r238, %r1062;
	mov.u32 	%r1065, 1;
	shl.b32 	%r1066, %r1065, %r1061;
	ld.shared.u32 	%r1067, [%r1064];
	and.b32  	%r1068, %r1067, %r1066;
	setp.eq.s32	%p157, %r1068, 0;
	@%p157 bra 	BB8_233;

	bfe.u32 	%r1069, %r1057, 17, 12;
	add.s32 	%r1070, %r1069, %r14;
	mul.wide.s32 	%rd189, %r1070, 4;
	add.s64 	%rd190, %rd3, %rd189;
	atom.global.add.u32 	%r1071, [%rd190], 1;
	min.s32 	%r1072, %r1071, %r173;
	mad.lo.s32 	%r1073, %r1069, %r235, %r1072;
	mul.wide.s32 	%rd191, %r1073, 8;
	add.s64 	%rd192, %rd1, %rd191;
	st.global.v2.u32 	[%rd192], {%r1057, %r1056};

BB8_233:
	add.s32 	%r199, %r1269, 3072;
	setp.ge.s32	%p158, %r199, %r8;
	@%p158 bra 	BB8_237;

	add.s32 	%r1074, %r199, %r17;
	mul.wide.s32 	%rd193, %r1074, 8;
	add.s64 	%rd194, %rd2, %rd193;
	ld.global.v2.u32 	{%r1075, %r1076}, [%rd194];
	or.b32  	%r1077, %r1075, %r1076;
	setp.eq.s32	%p159, %r1077, 0;
	@%p159 bra 	BB8_237;

	and.b32  	%r1078, %r1075, 131040;
	and.b32  	%r1079, %r1075, 31;
	xor.b32  	%r1080, %r1079, 1;
	shr.u32 	%r1081, %r1078, 3;
	add.s32 	%r1083, %r238, %r1081;
	mov.u32 	%r1084, 1;
	shl.b32 	%r1085, %r1084, %r1080;
	ld.shared.u32 	%r1086, [%r1083];
	and.b32  	%r1087, %r1086, %r1085;
	setp.eq.s32	%p160, %r1087, 0;
	@%p160 bra 	BB8_237;

	bfe.u32 	%r1088, %r1076, 17, 12;
	add.s32 	%r1089, %r1088, %r14;
	mul.wide.s32 	%rd195, %r1089, 4;
	add.s64 	%rd196, %rd3, %rd195;
	atom.global.add.u32 	%r1090, [%rd196], 1;
	min.s32 	%r1091, %r1090, %r173;
	mad.lo.s32 	%r1092, %r1088, %r235, %r1091;
	mul.wide.s32 	%rd197, %r1092, 8;
	add.s64 	%rd198, %rd1, %rd197;
	st.global.v2.u32 	[%rd198], {%r1076, %r1075};

BB8_237:
	add.s32 	%r1266, %r1266, 4;
	add.s32 	%r1269, %r1269, 4096;
	setp.lt.s32	%p161, %r1266, %r10;
	@%p161 bra 	BB8_221;

BB8_238:
	@%p62 bra 	BB8_276;

	add.s32 	%r204, %r235, -1;
	mov.u32 	%r1097, 1;
	max.s32 	%r205, %r13, %r1097;
	and.b32  	%r1096, %r205, 3;
	setp.eq.s32	%p163, %r1096, 0;
	@%p163 bra 	BB8_257;

	setp.eq.s32	%p164, %r1096, 1;
	@%p164 bra 	BB8_252;

	setp.eq.s32	%p165, %r1096, 2;
	@%p165 bra 	BB8_247;

	setp.ge.s32	%p166, %r1, %r11;
	@%p166 bra 	BB8_243;

	add.s32 	%r1100, %r1, %r18;
	mul.wide.s32 	%rd199, %r1100, 8;
	add.s64 	%rd200, %rd2, %rd199;
	ld.global.v2.u32 	{%r1101, %r1102}, [%rd200];
	or.b32  	%r1103, %r1101, %r1102;
	setp.eq.s32	%p167, %r1103, 0;
	mov.u32 	%r1271, %r1097;
	@%p167 bra 	BB8_247;

	and.b32  	%r1105, %r1101, 131040;
	and.b32  	%r1106, %r1101, 31;
	xor.b32  	%r1107, %r1106, 1;
	shr.u32 	%r1108, %r1105, 3;
	add.s32 	%r1110, %r238, %r1108;
	mov.u32 	%r1271, 1;
	shl.b32 	%r1111, %r1271, %r1107;
	ld.shared.u32 	%r1112, [%r1110];
	and.b32  	%r1113, %r1112, %r1111;
	setp.eq.s32	%p168, %r1113, 0;
	@%p168 bra 	BB8_247;

	bfe.u32 	%r1115, %r1102, 17, 12;
	add.s32 	%r1116, %r1115, %r14;
	mul.wide.s32 	%rd201, %r1116, 4;
	add.s64 	%rd202, %rd3, %rd201;
	atom.global.add.u32 	%r1117, [%rd202], 1;
	min.s32 	%r1118, %r1117, %r204;
	mad.lo.s32 	%r1119, %r1115, %r235, %r1118;
	mul.wide.s32 	%rd203, %r1119, 8;
	add.s64 	%rd204, %rd1, %rd203;
	st.global.v2.u32 	[%rd204], {%r1102, %r1101};
	bra.uni 	BB8_247;

BB8_243:
	mov.u32 	%r1271, %r1097;

BB8_247:
	shl.b32 	%r1120, %r1271, 10;
	add.s32 	%r209, %r1120, %r1;
	setp.ge.s32	%p169, %r209, %r11;
	@%p169 bra 	BB8_251;

	add.s32 	%r1121, %r209, %r18;
	mul.wide.s32 	%rd205, %r1121, 8;
	add.s64 	%rd206, %rd2, %rd205;
	ld.global.v2.u32 	{%r1122, %r1123}, [%rd206];
	or.b32  	%r1124, %r1122, %r1123;
	setp.eq.s32	%p170, %r1124, 0;
	@%p170 bra 	BB8_251;

	and.b32  	%r1125, %r1122, 131040;
	and.b32  	%r1126, %r1122, 31;
	xor.b32  	%r1127, %r1126, 1;
	shr.u32 	%r1128, %r1125, 3;
	add.s32 	%r1130, %r238, %r1128;
	mov.u32 	%r1131, 1;
	shl.b32 	%r1132, %r1131, %r1127;
	ld.shared.u32 	%r1133, [%r1130];
	and.b32  	%r1134, %r1133, %r1132;
	setp.eq.s32	%p171, %r1134, 0;
	@%p171 bra 	BB8_251;

	bfe.u32 	%r1135, %r1123, 17, 12;
	add.s32 	%r1136, %r1135, %r14;
	mul.wide.s32 	%rd207, %r1136, 4;
	add.s64 	%rd208, %rd3, %rd207;
	atom.global.add.u32 	%r1137, [%rd208], 1;
	min.s32 	%r1138, %r1137, %r204;
	mad.lo.s32 	%r1139, %r1135, %r235, %r1138;
	mul.wide.s32 	%rd209, %r1139, 8;
	add.s64 	%rd210, %rd1, %rd209;
	st.global.v2.u32 	[%rd210], {%r1123, %r1122};

BB8_251:
	add.s32 	%r1271, %r1271, 1;

BB8_252:
	shl.b32 	%r1140, %r1271, 10;
	add.s32 	%r214, %r1140, %r1;
	setp.ge.s32	%p172, %r214, %r11;
	@%p172 bra 	BB8_256;

	add.s32 	%r1141, %r214, %r18;
	mul.wide.s32 	%rd211, %r1141, 8;
	add.s64 	%rd212, %rd2, %rd211;
	ld.global.v2.u32 	{%r1142, %r1143}, [%rd212];
	or.b32  	%r1144, %r1142, %r1143;
	setp.eq.s32	%p173, %r1144, 0;
	@%p173 bra 	BB8_256;

	and.b32  	%r1145, %r1142, 131040;
	and.b32  	%r1146, %r1142, 31;
	xor.b32  	%r1147, %r1146, 1;
	shr.u32 	%r1148, %r1145, 3;
	add.s32 	%r1150, %r238, %r1148;
	mov.u32 	%r1151, 1;
	shl.b32 	%r1152, %r1151, %r1147;
	ld.shared.u32 	%r1153, [%r1150];
	and.b32  	%r1154, %r1153, %r1152;
	setp.eq.s32	%p174, %r1154, 0;
	@%p174 bra 	BB8_256;

	bfe.u32 	%r1155, %r1143, 17, 12;
	add.s32 	%r1156, %r1155, %r14;
	mul.wide.s32 	%rd213, %r1156, 4;
	add.s64 	%rd214, %rd3, %rd213;
	atom.global.add.u32 	%r1157, [%rd214], 1;
	min.s32 	%r1158, %r1157, %r204;
	mad.lo.s32 	%r1159, %r1155, %r235, %r1158;
	mul.wide.s32 	%rd215, %r1159, 8;
	add.s64 	%rd216, %rd1, %rd215;
	st.global.v2.u32 	[%rd216], {%r1143, %r1142};

BB8_256:
	add.s32 	%r1271, %r1271, 1;

BB8_257:
	setp.lt.u32	%p175, %r205, 4;
	@%p175 bra 	BB8_276;

	mad.lo.s32 	%r1274, %r1271, 1024, %r1;

BB8_259:
	setp.ge.s32	%p176, %r1274, %r11;
	@%p176 bra 	BB8_263;

	add.s32 	%r1160, %r1274, %r18;
	mul.wide.s32 	%rd217, %r1160, 8;
	add.s64 	%rd218, %rd2, %rd217;
	ld.global.v2.u32 	{%r1161, %r1162}, [%rd218];
	or.b32  	%r1163, %r1161, %r1162;
	setp.eq.s32	%p177, %r1163, 0;
	@%p177 bra 	BB8_263;

	and.b32  	%r1164, %r1161, 131040;
	and.b32  	%r1165, %r1161, 31;
	xor.b32  	%r1166, %r1165, 1;
	shr.u32 	%r1167, %r1164, 3;
	add.s32 	%r1169, %r238, %r1167;
	mov.u32 	%r1170, 1;
	shl.b32 	%r1171, %r1170, %r1166;
	ld.shared.u32 	%r1172, [%r1169];
	and.b32  	%r1173, %r1172, %r1171;
	setp.eq.s32	%p178, %r1173, 0;
	@%p178 bra 	BB8_263;

	bfe.u32 	%r1174, %r1162, 17, 12;
	add.s32 	%r1175, %r1174, %r14;
	mul.wide.s32 	%rd219, %r1175, 4;
	add.s64 	%rd220, %rd3, %rd219;
	atom.global.add.u32 	%r1176, [%rd220], 1;
	min.s32 	%r1177, %r1176, %r204;
	mad.lo.s32 	%r1178, %r1174, %r235, %r1177;
	mul.wide.s32 	%rd221, %r1178, 8;
	add.s64 	%rd222, %rd1, %rd221;
	st.global.v2.u32 	[%rd222], {%r1162, %r1161};

BB8_263:
	add.s32 	%r224, %r1274, 1024;
	setp.ge.s32	%p179, %r224, %r11;
	@%p179 bra 	BB8_267;

	add.s32 	%r1179, %r224, %r18;
	mul.wide.s32 	%rd223, %r1179, 8;
	add.s64 	%rd224, %rd2, %rd223;
	ld.global.v2.u32 	{%r1180, %r1181}, [%rd224];
	or.b32  	%r1182, %r1180, %r1181;
	setp.eq.s32	%p180, %r1182, 0;
	@%p180 bra 	BB8_267;

	and.b32  	%r1183, %r1180, 131040;
	and.b32  	%r1184, %r1180, 31;
	xor.b32  	%r1185, %r1184, 1;
	shr.u32 	%r1186, %r1183, 3;
	add.s32 	%r1188, %r238, %r1186;
	mov.u32 	%r1189, 1;
	shl.b32 	%r1190, %r1189, %r1185;
	ld.shared.u32 	%r1191, [%r1188];
	and.b32  	%r1192, %r1191, %r1190;
	setp.eq.s32	%p181, %r1192, 0;
	@%p181 bra 	BB8_267;

	bfe.u32 	%r1193, %r1181, 17, 12;
	add.s32 	%r1194, %r1193, %r14;
	mul.wide.s32 	%rd225, %r1194, 4;
	add.s64 	%rd226, %rd3, %rd225;
	atom.global.add.u32 	%r1195, [%rd226], 1;
	min.s32 	%r1196, %r1195, %r204;
	mad.lo.s32 	%r1197, %r1193, %r235, %r1196;
	mul.wide.s32 	%rd227, %r1197, 8;
	add.s64 	%rd228, %rd1, %rd227;
	st.global.v2.u32 	[%rd228], {%r1181, %r1180};

BB8_267:
	add.s32 	%r227, %r1274, 2048;
	setp.ge.s32	%p182, %r227, %r11;
	@%p182 bra 	BB8_271;

	add.s32 	%r1198, %r227, %r18;
	mul.wide.s32 	%rd229, %r1198, 8;
	add.s64 	%rd230, %rd2, %rd229;
	ld.global.v2.u32 	{%r1199, %r1200}, [%rd230];
	or.b32  	%r1201, %r1199, %r1200;
	setp.eq.s32	%p183, %r1201, 0;
	@%p183 bra 	BB8_271;

	and.b32  	%r1202, %r1199, 131040;
	and.b32  	%r1203, %r1199, 31;
	xor.b32  	%r1204, %r1203, 1;
	shr.u32 	%r1205, %r1202, 3;
	add.s32 	%r1207, %r238, %r1205;
	mov.u32 	%r1208, 1;
	shl.b32 	%r1209, %r1208, %r1204;
	ld.shared.u32 	%r1210, [%r1207];
	and.b32  	%r1211, %r1210, %r1209;
	setp.eq.s32	%p184, %r1211, 0;
	@%p184 bra 	BB8_271;

	bfe.u32 	%r1212, %r1200, 17, 12;
	add.s32 	%r1213, %r1212, %r14;
	mul.wide.s32 	%rd231, %r1213, 4;
	add.s64 	%rd232, %rd3, %rd231;
	atom.global.add.u32 	%r1214, [%rd232], 1;
	min.s32 	%r1215, %r1214, %r204;
	mad.lo.s32 	%r1216, %r1212, %r235, %r1215;
	mul.wide.s32 	%rd233, %r1216, 8;
	add.s64 	%rd234, %rd1, %rd233;
	st.global.v2.u32 	[%rd234], {%r1200, %r1199};

BB8_271:
	add.s32 	%r230, %r1274, 3072;
	setp.ge.s32	%p185, %r230, %r11;
	@%p185 bra 	BB8_275;

	add.s32 	%r1217, %r230, %r18;
	mul.wide.s32 	%rd235, %r1217, 8;
	add.s64 	%rd236, %rd2, %rd235;
	ld.global.v2.u32 	{%r1218, %r1219}, [%rd236];
	or.b32  	%r1220, %r1218, %r1219;
	setp.eq.s32	%p186, %r1220, 0;
	@%p186 bra 	BB8_275;

	and.b32  	%r1221, %r1218, 131040;
	and.b32  	%r1222, %r1218, 31;
	xor.b32  	%r1223, %r1222, 1;
	shr.u32 	%r1224, %r1221, 3;
	add.s32 	%r1226, %r238, %r1224;
	mov.u32 	%r1227, 1;
	shl.b32 	%r1228, %r1227, %r1223;
	ld.shared.u32 	%r1229, [%r1226];
	and.b32  	%r1230, %r1229, %r1228;
	setp.eq.s32	%p187, %r1230, 0;
	@%p187 bra 	BB8_275;

	bfe.u32 	%r1231, %r1219, 17, 12;
	add.s32 	%r1232, %r1231, %r14;
	mul.wide.s32 	%rd237, %r1232, 4;
	add.s64 	%rd238, %rd3, %rd237;
	atom.global.add.u32 	%r1233, [%rd238], 1;
	min.s32 	%r1234, %r1233, %r204;
	mad.lo.s32 	%r1235, %r1231, %r235, %r1234;
	mul.wide.s32 	%rd239, %r1235, 8;
	add.s64 	%rd240, %rd1, %rd239;
	st.global.v2.u32 	[%rd240], {%r1219, %r1218};

BB8_275:
	add.s32 	%r1271, %r1271, 4;
	add.s32 	%r1274, %r1274, 4096;
	setp.lt.s32	%p188, %r1271, %r13;
	@%p188 bra 	BB8_259;

BB8_276:
	ret;
}

	// .globl	FluffyRound_B2
.visible .entry FluffyRound_B2(
	.param .u64 FluffyRound_B2_param_0,
	.param .u64 FluffyRound_B2_param_1,
	.param .u64 FluffyRound_B2_param_2,
	.param .u64 FluffyRound_B2_param_3,
	.param .u32 FluffyRound_B2_param_4,
	.param .u32 FluffyRound_B2_param_5,
	.param .u32 FluffyRound_B2_param_6,
	.param .u64 FluffyRound_B2_param_7
)
{
	.reg .pred 	%p<39>;
	.reg .b32 	%r<315>;
	.reg .b64 	%rd<78>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_B2$__cuda_local_var_208230_30_non_const_ecounters[32768];

	ld.param.u64 	%rd4, [FluffyRound_B2_param_0];
	ld.param.u64 	%rd6, [FluffyRound_B2_param_1];
	ld.param.u64 	%rd7, [FluffyRound_B2_param_2];
	ld.param.u64 	%rd8, [FluffyRound_B2_param_3];
	ld.param.u32 	%r63, [FluffyRound_B2_param_4];
	ld.param.u32 	%r61, [FluffyRound_B2_param_5];
	ld.param.u32 	%r62, [FluffyRound_B2_param_6];
	ld.param.u64 	%rd5, [FluffyRound_B2_param_7];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r1, %ctaid.x;
	cvta.to.global.u64 	%rd2, %rd7;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.u32 	%r64, [%rd10];
	min.s32 	%r2, %r64, %r63;
	add.s32 	%r3, %r2, 1024;
	shr.s32 	%r65, %r3, 31;
	shr.u32 	%r66, %r65, 22;
	add.s32 	%r67, %r3, %r66;
	shr.s32 	%r4, %r67, 10;
	mov.u32 	%r5, %tid.x;
	shl.b32 	%r68, %r5, 2;
	mov.u32 	%r69, FluffyRound_B2$__cuda_local_var_208230_30_non_const_ecounters;
	add.s32 	%r70, %r69, %r68;
	mov.u32 	%r71, 0;
	st.shared.u32 	[%r70], %r71;
	add.s32 	%r6, %r5, 1024;
	st.shared.u32 	[%r70+4096], %r71;
	add.s32 	%r7, %r5, 2048;
	st.shared.u32 	[%r70+8192], %r71;
	add.s32 	%r8, %r5, 3072;
	st.shared.u32 	[%r70+12288], %r71;
	st.shared.u32 	[%r70+16384], %r71;
	st.shared.u32 	[%r70+20480], %r71;
	st.shared.u32 	[%r70+24576], %r71;
	st.shared.u32 	[%r70+28672], %r71;
	cvta.to.global.u64 	%rd3, %rd8;
	mul.lo.s32 	%r9, %r1, %r63;
	bar.sync 	0;
	setp.gt.s32	%p1, %r3, 1023;
	mov.pred 	%p38, 0;
	setp.lt.s32	%p4, %r3, 1024;
	@%p4 bra 	BB9_23;

	mov.u32 	%r76, 1;
	max.s32 	%r10, %r4, %r76;
	and.b32  	%r75, %r10, 3;
	mov.u32 	%r307, 0;
	setp.eq.s32	%p5, %r75, 0;
	@%p5 bra 	BB9_12;

	setp.eq.s32	%p6, %r75, 1;
	@%p6 bra 	BB9_9;

	setp.eq.s32	%p7, %r75, 2;
	@%p7 bra 	BB9_6;

	setp.ge.s32	%p8, %r5, %r2;
	mov.u32 	%r307, %r76;
	@%p8 bra 	BB9_6;

	add.s32 	%r81, %r5, %r9;
	mul.wide.s32 	%rd12, %r81, 8;
	add.s64 	%rd11, %rd4, %rd12;
	// inline asm
	ld.global.nc.v2.u32 {%r78,%r79}, [%rd11];
	// inline asm
	and.b32  	%r82, %r78, 131040;
	and.b32  	%r83, %r78, 31;
	mov.u32 	%r307, 1;
	shl.b32 	%r84, %r307, %r83;
	shr.u32 	%r85, %r82, 3;
	add.s32 	%r87, %r69, %r85;
	atom.shared.or.b32 	%r88, [%r87], %r84;

BB9_6:
	shl.b32 	%r89, %r307, 10;
	add.s32 	%r12, %r89, %r5;
	setp.ge.s32	%p9, %r12, %r2;
	@%p9 bra 	BB9_8;

	add.s32 	%r92, %r12, %r9;
	mul.wide.s32 	%rd14, %r92, 8;
	add.s64 	%rd13, %rd4, %rd14;
	// inline asm
	ld.global.nc.v2.u32 {%r90,%r91}, [%rd13];
	// inline asm
	and.b32  	%r93, %r90, 131040;
	and.b32  	%r94, %r90, 31;
	mov.u32 	%r95, 1;
	shl.b32 	%r96, %r95, %r94;
	shr.u32 	%r97, %r93, 3;
	add.s32 	%r99, %r69, %r97;
	atom.shared.or.b32 	%r100, [%r99], %r96;

BB9_8:
	add.s32 	%r307, %r307, 1;

BB9_9:
	shl.b32 	%r101, %r307, 10;
	add.s32 	%r15, %r101, %r5;
	setp.ge.s32	%p10, %r15, %r2;
	@%p10 bra 	BB9_11;

	add.s32 	%r104, %r15, %r9;
	mul.wide.s32 	%rd16, %r104, 8;
	add.s64 	%rd15, %rd4, %rd16;
	// inline asm
	ld.global.nc.v2.u32 {%r102,%r103}, [%rd15];
	// inline asm
	and.b32  	%r105, %r102, 131040;
	and.b32  	%r106, %r102, 31;
	mov.u32 	%r107, 1;
	shl.b32 	%r108, %r107, %r106;
	shr.u32 	%r109, %r105, 3;
	add.s32 	%r111, %r69, %r109;
	atom.shared.or.b32 	%r112, [%r111], %r108;

BB9_11:
	add.s32 	%r307, %r307, 1;

BB9_12:
	setp.lt.u32	%p11, %r10, 4;
	@%p11 bra 	BB9_22;

BB9_13:
	shl.b32 	%r113, %r307, 10;
	add.s32 	%r19, %r113, %r5;
	setp.ge.s32	%p12, %r19, %r2;
	@%p12 bra 	BB9_15;

	add.s32 	%r116, %r19, %r9;
	mul.wide.s32 	%rd18, %r116, 8;
	add.s64 	%rd17, %rd4, %rd18;
	// inline asm
	ld.global.nc.v2.u32 {%r114,%r115}, [%rd17];
	// inline asm
	and.b32  	%r117, %r114, 131040;
	and.b32  	%r118, %r114, 31;
	mov.u32 	%r119, 1;
	shl.b32 	%r120, %r119, %r118;
	shr.u32 	%r121, %r117, 3;
	add.s32 	%r123, %r69, %r121;
	atom.shared.or.b32 	%r124, [%r123], %r120;

BB9_15:
	add.s32 	%r20, %r19, 1024;
	setp.ge.s32	%p13, %r20, %r2;
	@%p13 bra 	BB9_17;

	add.s32 	%r129, %r20, %r9;
	mul.wide.s32 	%rd20, %r129, 8;
	add.s64 	%rd19, %rd4, %rd20;
	// inline asm
	ld.global.nc.v2.u32 {%r127,%r128}, [%rd19];
	// inline asm
	and.b32  	%r130, %r127, 131040;
	and.b32  	%r131, %r127, 31;
	mov.u32 	%r132, 1;
	shl.b32 	%r133, %r132, %r131;
	shr.u32 	%r134, %r130, 3;
	add.s32 	%r136, %r69, %r134;
	atom.shared.or.b32 	%r137, [%r136], %r133;

BB9_17:
	add.s32 	%r21, %r19, 2048;
	setp.ge.s32	%p14, %r21, %r2;
	@%p14 bra 	BB9_19;

	add.s32 	%r142, %r21, %r9;
	mul.wide.s32 	%rd22, %r142, 8;
	add.s64 	%rd21, %rd4, %rd22;
	// inline asm
	ld.global.nc.v2.u32 {%r140,%r141}, [%rd21];
	// inline asm
	and.b32  	%r143, %r140, 131040;
	and.b32  	%r144, %r140, 31;
	mov.u32 	%r145, 1;
	shl.b32 	%r146, %r145, %r144;
	shr.u32 	%r147, %r143, 3;
	add.s32 	%r149, %r69, %r147;
	atom.shared.or.b32 	%r150, [%r149], %r146;

BB9_19:
	add.s32 	%r22, %r19, 3072;
	setp.ge.s32	%p15, %r22, %r2;
	@%p15 bra 	BB9_21;

	add.s32 	%r155, %r22, %r9;
	mul.wide.s32 	%rd24, %r155, 8;
	add.s64 	%rd23, %rd4, %rd24;
	// inline asm
	ld.global.nc.v2.u32 {%r153,%r154}, [%rd23];
	// inline asm
	and.b32  	%r156, %r153, 131040;
	and.b32  	%r157, %r153, 31;
	mov.u32 	%r158, 1;
	shl.b32 	%r159, %r158, %r157;
	shr.u32 	%r160, %r156, 3;
	add.s32 	%r162, %r69, %r160;
	atom.shared.or.b32 	%r163, [%r162], %r159;

BB9_21:
	add.s32 	%r307, %r307, 4;
	setp.lt.s32	%p16, %r307, %r4;
	@%p16 bra 	BB9_13;

BB9_22:
	mov.pred 	%p38, %p1;

BB9_23:
	bar.sync 	0;
	@!%p38 bra 	BB9_52;
	bra.uni 	BB9_24;

BB9_24:
	add.s32 	%r24, %r61, -1;
	mov.u32 	%r168, 1;
	max.s32 	%r25, %r4, %r168;
	and.b32  	%r167, %r25, 3;
	setp.eq.s32	%p17, %r167, 0;
	mov.u32 	%r314, %r71;
	@%p17 bra 	BB9_38;

	setp.eq.s32	%p18, %r167, 1;
	mov.u32 	%r312, %r71;
	@%p18 bra 	BB9_34;

	setp.eq.s32	%p19, %r167, 2;
	mov.u32 	%r311, %r71;
	@%p19 bra 	BB9_30;

	setp.ge.s32	%p20, %r5, %r2;
	mov.u32 	%r311, %r168;
	@%p20 bra 	BB9_30;

	add.s32 	%r173, %r5, %r9;
	mul.wide.s32 	%rd26, %r173, 8;
	add.s64 	%rd25, %rd4, %rd26;
	// inline asm
	ld.global.nc.v2.u32 {%r170,%r171}, [%rd25];
	// inline asm
	and.b32  	%r174, %r170, 131040;
	and.b32  	%r175, %r170, 31;
	xor.b32  	%r176, %r175, 1;
	shr.u32 	%r177, %r174, 3;
	add.s32 	%r179, %r69, %r177;
	mov.u32 	%r311, 1;
	shl.b32 	%r180, %r311, %r176;
	ld.shared.u32 	%r181, [%r179];
	and.b32  	%r182, %r180, %r181;
	setp.eq.s32	%p21, %r182, 0;
	@%p21 bra 	BB9_30;

	bfe.u32 	%r184, %r171, 17, 12;
	mul.wide.u32 	%rd27, %r184, 4;
	add.s64 	%rd28, %rd3, %rd27;
	atom.global.add.u32 	%r185, [%rd28], 1;
	min.s32 	%r186, %r185, %r24;
	mad.lo.s32 	%r187, %r184, %r61, %r186;
	mul.wide.s32 	%rd29, %r187, 8;
	add.s64 	%rd30, %rd1, %rd29;
	st.global.v2.u32 	[%rd30], {%r171, %r170};

BB9_30:
	shl.b32 	%r188, %r311, 10;
	add.s32 	%r30, %r188, %r5;
	setp.ge.s32	%p22, %r30, %r2;
	@%p22 bra 	BB9_33;

	add.s32 	%r191, %r30, %r9;
	mul.wide.s32 	%rd32, %r191, 8;
	add.s64 	%rd31, %rd4, %rd32;
	// inline asm
	ld.global.nc.v2.u32 {%r189,%r190}, [%rd31];
	// inline asm
	and.b32  	%r192, %r189, 131040;
	and.b32  	%r193, %r189, 31;
	xor.b32  	%r194, %r193, 1;
	shr.u32 	%r195, %r192, 3;
	add.s32 	%r197, %r69, %r195;
	mov.u32 	%r198, 1;
	shl.b32 	%r199, %r198, %r194;
	ld.shared.u32 	%r200, [%r197];
	and.b32  	%r201, %r199, %r200;
	setp.eq.s32	%p23, %r201, 0;
	@%p23 bra 	BB9_33;

	bfe.u32 	%r202, %r190, 17, 12;
	mul.wide.u32 	%rd33, %r202, 4;
	add.s64 	%rd34, %rd3, %rd33;
	atom.global.add.u32 	%r203, [%rd34], 1;
	min.s32 	%r204, %r203, %r24;
	mad.lo.s32 	%r205, %r202, %r61, %r204;
	mul.wide.s32 	%rd35, %r205, 8;
	add.s64 	%rd36, %rd1, %rd35;
	st.global.v2.u32 	[%rd36], {%r190, %r189};

BB9_33:
	add.s32 	%r312, %r311, 1;

BB9_34:
	shl.b32 	%r206, %r312, 10;
	add.s32 	%r36, %r206, %r5;
	setp.ge.s32	%p24, %r36, %r2;
	@%p24 bra 	BB9_37;

	add.s32 	%r209, %r36, %r9;
	mul.wide.s32 	%rd38, %r209, 8;
	add.s64 	%rd37, %rd4, %rd38;
	// inline asm
	ld.global.nc.v2.u32 {%r207,%r208}, [%rd37];
	// inline asm
	and.b32  	%r210, %r207, 131040;
	and.b32  	%r211, %r207, 31;
	xor.b32  	%r212, %r211, 1;
	shr.u32 	%r213, %r210, 3;
	add.s32 	%r215, %r69, %r213;
	mov.u32 	%r216, 1;
	shl.b32 	%r217, %r216, %r212;
	ld.shared.u32 	%r218, [%r215];
	and.b32  	%r219, %r217, %r218;
	setp.eq.s32	%p25, %r219, 0;
	@%p25 bra 	BB9_37;

	bfe.u32 	%r220, %r208, 17, 12;
	mul.wide.u32 	%rd39, %r220, 4;
	add.s64 	%rd40, %rd3, %rd39;
	atom.global.add.u32 	%r221, [%rd40], 1;
	min.s32 	%r222, %r221, %r24;
	mad.lo.s32 	%r223, %r220, %r61, %r222;
	mul.wide.s32 	%rd41, %r223, 8;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.v2.u32 	[%rd42], {%r208, %r207};

BB9_37:
	add.s32 	%r314, %r312, 1;

BB9_38:
	setp.lt.u32	%p26, %r25, 4;
	@%p26 bra 	BB9_52;

BB9_39:
	shl.b32 	%r224, %r314, 10;
	add.s32 	%r43, %r224, %r5;
	setp.ge.s32	%p27, %r43, %r2;
	@%p27 bra 	BB9_42;

	add.s32 	%r227, %r43, %r9;
	mul.wide.s32 	%rd44, %r227, 8;
	add.s64 	%rd43, %rd4, %rd44;
	// inline asm
	ld.global.nc.v2.u32 {%r225,%r226}, [%rd43];
	// inline asm
	and.b32  	%r228, %r225, 131040;
	and.b32  	%r229, %r225, 31;
	xor.b32  	%r230, %r229, 1;
	shr.u32 	%r231, %r228, 3;
	add.s32 	%r233, %r69, %r231;
	mov.u32 	%r234, 1;
	shl.b32 	%r235, %r234, %r230;
	ld.shared.u32 	%r236, [%r233];
	and.b32  	%r237, %r235, %r236;
	setp.eq.s32	%p28, %r237, 0;
	@%p28 bra 	BB9_42;

	bfe.u32 	%r238, %r226, 17, 12;
	mul.wide.u32 	%rd45, %r238, 4;
	add.s64 	%rd46, %rd3, %rd45;
	atom.global.add.u32 	%r239, [%rd46], 1;
	min.s32 	%r240, %r239, %r24;
	mad.lo.s32 	%r241, %r238, %r61, %r240;
	mul.wide.s32 	%rd47, %r241, 8;
	add.s64 	%rd48, %rd1, %rd47;
	st.global.v2.u32 	[%rd48], {%r226, %r225};

BB9_42:
	add.s32 	%r47, %r43, 1024;
	setp.ge.s32	%p29, %r47, %r2;
	@%p29 bra 	BB9_45;

	add.s32 	%r246, %r47, %r9;
	mul.wide.s32 	%rd50, %r246, 8;
	add.s64 	%rd49, %rd4, %rd50;
	// inline asm
	ld.global.nc.v2.u32 {%r244,%r245}, [%rd49];
	// inline asm
	and.b32  	%r247, %r244, 131040;
	and.b32  	%r248, %r244, 31;
	xor.b32  	%r249, %r248, 1;
	shr.u32 	%r250, %r247, 3;
	add.s32 	%r252, %r69, %r250;
	mov.u32 	%r253, 1;
	shl.b32 	%r254, %r253, %r249;
	ld.shared.u32 	%r255, [%r252];
	and.b32  	%r256, %r254, %r255;
	setp.eq.s32	%p30, %r256, 0;
	@%p30 bra 	BB9_45;

	bfe.u32 	%r257, %r245, 17, 12;
	mul.wide.u32 	%rd51, %r257, 4;
	add.s64 	%rd52, %rd3, %rd51;
	atom.global.add.u32 	%r258, [%rd52], 1;
	min.s32 	%r259, %r258, %r24;
	mad.lo.s32 	%r260, %r257, %r61, %r259;
	mul.wide.s32 	%rd53, %r260, 8;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.v2.u32 	[%rd54], {%r245, %r244};

BB9_45:
	add.s32 	%r51, %r43, 2048;
	setp.ge.s32	%p31, %r51, %r2;
	@%p31 bra 	BB9_48;

	add.s32 	%r265, %r51, %r9;
	mul.wide.s32 	%rd56, %r265, 8;
	add.s64 	%rd55, %rd4, %rd56;
	// inline asm
	ld.global.nc.v2.u32 {%r263,%r264}, [%rd55];
	// inline asm
	and.b32  	%r266, %r263, 131040;
	and.b32  	%r267, %r263, 31;
	xor.b32  	%r268, %r267, 1;
	shr.u32 	%r269, %r266, 3;
	add.s32 	%r271, %r69, %r269;
	mov.u32 	%r272, 1;
	shl.b32 	%r273, %r272, %r268;
	ld.shared.u32 	%r274, [%r271];
	and.b32  	%r275, %r273, %r274;
	setp.eq.s32	%p32, %r275, 0;
	@%p32 bra 	BB9_48;

	bfe.u32 	%r276, %r264, 17, 12;
	mul.wide.u32 	%rd57, %r276, 4;
	add.s64 	%rd58, %rd3, %rd57;
	atom.global.add.u32 	%r277, [%rd58], 1;
	min.s32 	%r278, %r277, %r24;
	mad.lo.s32 	%r279, %r276, %r61, %r278;
	mul.wide.s32 	%rd59, %r279, 8;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.v2.u32 	[%rd60], {%r264, %r263};

BB9_48:
	add.s32 	%r55, %r43, 3072;
	setp.ge.s32	%p33, %r55, %r2;
	@%p33 bra 	BB9_51;

	add.s32 	%r284, %r55, %r9;
	mul.wide.s32 	%rd62, %r284, 8;
	add.s64 	%rd61, %rd4, %rd62;
	// inline asm
	ld.global.nc.v2.u32 {%r282,%r283}, [%rd61];
	// inline asm
	and.b32  	%r285, %r282, 131040;
	and.b32  	%r286, %r282, 31;
	xor.b32  	%r287, %r286, 1;
	shr.u32 	%r288, %r285, 3;
	add.s32 	%r290, %r69, %r288;
	mov.u32 	%r291, 1;
	shl.b32 	%r292, %r291, %r287;
	ld.shared.u32 	%r293, [%r290];
	and.b32  	%r294, %r292, %r293;
	setp.eq.s32	%p34, %r294, 0;
	@%p34 bra 	BB9_51;

	bfe.u32 	%r295, %r283, 17, 12;
	mul.wide.u32 	%rd63, %r295, 4;
	add.s64 	%rd64, %rd3, %rd63;
	atom.global.add.u32 	%r296, [%rd64], 1;
	min.s32 	%r297, %r296, %r24;
	mad.lo.s32 	%r298, %r295, %r61, %r297;
	mul.wide.s32 	%rd65, %r298, 8;
	add.s64 	%rd66, %rd1, %rd65;
	st.global.v2.u32 	[%rd66], {%r283, %r282};

BB9_51:
	add.s32 	%r314, %r314, 4;
	setp.lt.s32	%p35, %r314, %r4;
	@%p35 bra 	BB9_39;

BB9_52:
	bar.sync 	0;
	setp.ne.s32	%p36, %r5, 0;
	@%p36 bra 	BB9_54;

	cvta.to.global.u64 	%rd67, %rd5;
	shl.b32 	%r299, %r1, 2;
	add.s32 	%r301, %r69, %r299;
	mul.wide.s32 	%rd68, %r62, 4;
	add.s64 	%rd69, %rd67, %rd68;
	atom.global.add.u32 	%r302, [%rd69], 1;
	st.shared.u32 	[%r301], %r302;

BB9_54:
	shl.b32 	%r303, %r1, 2;
	add.s32 	%r60, %r69, %r303;
	bar.sync 	0;
	ld.shared.u32 	%r305, [%r60];
	setp.ne.s32	%p37, %r305, 4095;
	@%p37 bra 	BB9_56;

	mul.wide.s32 	%rd70, %r5, 4;
	add.s64 	%rd71, %rd2, %rd70;
	st.global.u32 	[%rd71], %r71;
	mul.wide.s32 	%rd72, %r6, 4;
	add.s64 	%rd73, %rd2, %rd72;
	st.global.u32 	[%rd73], %r71;
	mul.wide.s32 	%rd74, %r7, 4;
	add.s64 	%rd75, %rd2, %rd74;
	st.global.u32 	[%rd75], %r71;
	mul.wide.s32 	%rd76, %r8, 4;
	add.s64 	%rd77, %rd2, %rd76;
	st.global.u32 	[%rd77], %r71;

BB9_56:
	ret;
}

	// .globl	FluffyRound_B3
.visible .entry FluffyRound_B3(
	.param .u64 FluffyRound_B3_param_0,
	.param .u64 FluffyRound_B3_param_1,
	.param .u64 FluffyRound_B3_param_2,
	.param .u64 FluffyRound_B3_param_3,
	.param .u64 FluffyRound_B3_param_4,
	.param .u64 FluffyRound_B3_param_5,
	.param .u64 FluffyRound_B3_param_6,
	.param .u32 FluffyRound_B3_param_7,
	.param .u32 FluffyRound_B3_param_8
)
{
	.reg .pred 	%p<133>;
	.reg .b32 	%r<1153>;
	.reg .b64 	%rd<247>;
	// demoted variable
	.shared .align 4 .b8 FluffyRound_B3$__cuda_local_var_208291_30_non_const_ecounters[32768];

	ld.param.u64 	%rd7, [FluffyRound_B3_param_0];
	ld.param.u64 	%rd8, [FluffyRound_B3_param_1];
	ld.param.u64 	%rd9, [FluffyRound_B3_param_2];
	ld.param.u64 	%rd10, [FluffyRound_B3_param_3];
	ld.param.u64 	%rd11, [FluffyRound_B3_param_4];
	ld.param.u64 	%rd12, [FluffyRound_B3_param_5];
	ld.param.u64 	%rd13, [FluffyRound_B3_param_6];
	ld.param.u32 	%r216, [FluffyRound_B3_param_7];
	ld.param.u32 	%r215, [FluffyRound_B3_param_8];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd9;
	cvta.to.global.u64 	%rd3, %rd7;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r217, %r1, 2;
	mov.u32 	%r218, FluffyRound_B3$__cuda_local_var_208291_30_non_const_ecounters;
	add.s32 	%r219, %r218, %r217;
	mov.u32 	%r1149, 0;
	st.shared.u32 	[%r219], %r1149;
	st.shared.u32 	[%r219+4096], %r1149;
	st.shared.u32 	[%r219+8192], %r1149;
	st.shared.u32 	[%r219+12288], %r1149;
	st.shared.u32 	[%r219+16384], %r1149;
	st.shared.u32 	[%r219+20480], %r1149;
	st.shared.u32 	[%r219+24576], %r1149;
	st.shared.u32 	[%r219+28672], %r1149;
	cvta.to.global.u64 	%rd4, %rd11;
	cvta.to.global.u64 	%rd5, %rd10;
	cvta.to.global.u64 	%rd6, %rd8;
	mov.u32 	%r221, %ctaid.x;
	cvta.to.global.u64 	%rd14, %rd12;
	mul.wide.s32 	%rd15, %r221, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u32 	%r222, [%rd16];
	min.s32 	%r2, %r222, %r216;
	add.s32 	%r223, %r221, 4096;
	mul.wide.s32 	%rd17, %r223, 4;
	add.s64 	%rd18, %rd14, %rd17;
	ld.global.u32 	%r224, [%rd18];
	min.s32 	%r3, %r224, %r216;
	add.s32 	%r225, %r221, 8192;
	mul.wide.s32 	%rd19, %r225, 4;
	add.s64 	%rd20, %rd14, %rd19;
	ld.global.u32 	%r226, [%rd20];
	min.s32 	%r4, %r226, %r216;
	add.s32 	%r227, %r221, 12288;
	mul.wide.s32 	%rd21, %r227, 4;
	add.s64 	%rd22, %rd14, %rd21;
	ld.global.u32 	%r228, [%rd22];
	min.s32 	%r5, %r228, %r216;
	add.s32 	%r6, %r2, 1024;
	shr.s32 	%r229, %r6, 31;
	shr.u32 	%r230, %r229, 22;
	add.s32 	%r231, %r6, %r230;
	shr.s32 	%r7, %r231, 10;
	add.s32 	%r8, %r3, 1024;
	shr.s32 	%r232, %r8, 31;
	shr.u32 	%r233, %r232, 22;
	add.s32 	%r234, %r8, %r233;
	shr.s32 	%r9, %r234, 10;
	add.s32 	%r10, %r4, 1024;
	shr.s32 	%r235, %r10, 31;
	shr.u32 	%r236, %r235, 22;
	add.s32 	%r237, %r10, %r236;
	shr.s32 	%r11, %r237, 10;
	add.s32 	%r12, %r5, 1024;
	shr.s32 	%r238, %r12, 31;
	shr.u32 	%r239, %r238, 22;
	add.s32 	%r240, %r12, %r239;
	shr.s32 	%r13, %r240, 10;
	mul.lo.s32 	%r14, %r221, %r216;
	bar.sync 	0;
	setp.lt.s32	%p2, %r6, 1024;
	@%p2 bra 	BB10_22;

	mov.u32 	%r245, 1;
	max.s32 	%r15, %r7, %r245;
	and.b32  	%r244, %r15, 3;
	mov.u32 	%r1121, 0;
	setp.eq.s32	%p3, %r244, 0;
	@%p3 bra 	BB10_12;

	setp.eq.s32	%p4, %r244, 1;
	@%p4 bra 	BB10_9;

	setp.eq.s32	%p5, %r244, 2;
	@%p5 bra 	BB10_6;

	setp.ge.s32	%p6, %r1, %r2;
	mov.u32 	%r1121, %r245;
	@%p6 bra 	BB10_6;

	add.s32 	%r248, %r1, %r14;
	mul.wide.s32 	%rd23, %r248, 8;
	add.s64 	%rd24, %rd3, %rd23;
	ld.global.u32 	%r249, [%rd24];
	and.b32  	%r250, %r249, 131040;
	and.b32  	%r251, %r249, 31;
	mov.u32 	%r1121, 1;
	shl.b32 	%r252, %r1121, %r251;
	shr.u32 	%r253, %r250, 3;
	add.s32 	%r255, %r218, %r253;
	atom.shared.or.b32 	%r256, [%r255], %r252;

BB10_6:
	shl.b32 	%r257, %r1121, 10;
	add.s32 	%r17, %r257, %r1;
	setp.ge.s32	%p7, %r17, %r2;
	@%p7 bra 	BB10_8;

	add.s32 	%r258, %r17, %r14;
	mul.wide.s32 	%rd25, %r258, 8;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.u32 	%r259, [%rd26];
	and.b32  	%r260, %r259, 131040;
	and.b32  	%r261, %r259, 31;
	mov.u32 	%r262, 1;
	shl.b32 	%r263, %r262, %r261;
	shr.u32 	%r264, %r260, 3;
	add.s32 	%r266, %r218, %r264;
	atom.shared.or.b32 	%r267, [%r266], %r263;

BB10_8:
	add.s32 	%r1121, %r1121, 1;

BB10_9:
	shl.b32 	%r268, %r1121, 10;
	add.s32 	%r20, %r268, %r1;
	setp.ge.s32	%p8, %r20, %r2;
	@%p8 bra 	BB10_11;

	add.s32 	%r269, %r20, %r14;
	mul.wide.s32 	%rd27, %r269, 8;
	add.s64 	%rd28, %rd3, %rd27;
	ld.global.u32 	%r270, [%rd28];
	and.b32  	%r271, %r270, 131040;
	and.b32  	%r272, %r270, 31;
	mov.u32 	%r273, 1;
	shl.b32 	%r274, %r273, %r272;
	shr.u32 	%r275, %r271, 3;
	add.s32 	%r277, %r218, %r275;
	atom.shared.or.b32 	%r278, [%r277], %r274;

BB10_11:
	add.s32 	%r1121, %r1121, 1;

BB10_12:
	setp.lt.u32	%p9, %r15, 4;
	@%p9 bra 	BB10_22;

BB10_13:
	shl.b32 	%r279, %r1121, 10;
	add.s32 	%r24, %r279, %r1;
	setp.ge.s32	%p10, %r24, %r2;
	@%p10 bra 	BB10_15;

	add.s32 	%r280, %r24, %r14;
	mul.wide.s32 	%rd29, %r280, 8;
	add.s64 	%rd30, %rd3, %rd29;
	ld.global.u32 	%r281, [%rd30];
	and.b32  	%r282, %r281, 131040;
	and.b32  	%r283, %r281, 31;
	mov.u32 	%r284, 1;
	shl.b32 	%r285, %r284, %r283;
	shr.u32 	%r286, %r282, 3;
	add.s32 	%r288, %r218, %r286;
	atom.shared.or.b32 	%r289, [%r288], %r285;

BB10_15:
	add.s32 	%r25, %r24, 1024;
	setp.ge.s32	%p11, %r25, %r2;
	@%p11 bra 	BB10_17;

	add.s32 	%r292, %r25, %r14;
	mul.wide.s32 	%rd31, %r292, 8;
	add.s64 	%rd32, %rd3, %rd31;
	ld.global.u32 	%r293, [%rd32];
	and.b32  	%r294, %r293, 131040;
	and.b32  	%r295, %r293, 31;
	mov.u32 	%r296, 1;
	shl.b32 	%r297, %r296, %r295;
	shr.u32 	%r298, %r294, 3;
	add.s32 	%r300, %r218, %r298;
	atom.shared.or.b32 	%r301, [%r300], %r297;

BB10_17:
	add.s32 	%r26, %r24, 2048;
	setp.ge.s32	%p12, %r26, %r2;
	@%p12 bra 	BB10_19;

	add.s32 	%r304, %r26, %r14;
	mul.wide.s32 	%rd33, %r304, 8;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u32 	%r305, [%rd34];
	and.b32  	%r306, %r305, 131040;
	and.b32  	%r307, %r305, 31;
	mov.u32 	%r308, 1;
	shl.b32 	%r309, %r308, %r307;
	shr.u32 	%r310, %r306, 3;
	add.s32 	%r312, %r218, %r310;
	atom.shared.or.b32 	%r313, [%r312], %r309;

BB10_19:
	add.s32 	%r27, %r24, 3072;
	setp.ge.s32	%p13, %r27, %r2;
	@%p13 bra 	BB10_21;

	add.s32 	%r316, %r27, %r14;
	mul.wide.s32 	%rd35, %r316, 8;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u32 	%r317, [%rd36];
	and.b32  	%r318, %r317, 131040;
	and.b32  	%r319, %r317, 31;
	mov.u32 	%r320, 1;
	shl.b32 	%r321, %r320, %r319;
	shr.u32 	%r322, %r318, 3;
	add.s32 	%r324, %r218, %r322;
	atom.shared.or.b32 	%r325, [%r324], %r321;

BB10_21:
	add.s32 	%r1121, %r1121, 4;
	setp.lt.s32	%p14, %r1121, %r7;
	@%p14 bra 	BB10_13;

BB10_22:
	setp.lt.s32	%p15, %r8, 1024;
	@%p15 bra 	BB10_44;

	mov.u32 	%r330, 1;
	max.s32 	%r29, %r9, %r330;
	and.b32  	%r329, %r29, 3;
	mov.u32 	%r1125, 0;
	setp.eq.s32	%p16, %r329, 0;
	@%p16 bra 	BB10_34;

	setp.eq.s32	%p17, %r329, 1;
	@%p17 bra 	BB10_31;

	setp.eq.s32	%p18, %r329, 2;
	@%p18 bra 	BB10_28;

	setp.ge.s32	%p19, %r1, %r3;
	mov.u32 	%r1125, %r330;
	@%p19 bra 	BB10_28;

	add.s32 	%r333, %r1, %r14;
	mul.wide.s32 	%rd37, %r333, 8;
	add.s64 	%rd38, %rd6, %rd37;
	ld.global.u32 	%r334, [%rd38];
	and.b32  	%r335, %r334, 131040;
	and.b32  	%r336, %r334, 31;
	mov.u32 	%r1125, 1;
	shl.b32 	%r337, %r1125, %r336;
	shr.u32 	%r338, %r335, 3;
	add.s32 	%r340, %r218, %r338;
	atom.shared.or.b32 	%r341, [%r340], %r337;

BB10_28:
	shl.b32 	%r342, %r1125, 10;
	add.s32 	%r31, %r342, %r1;
	setp.ge.s32	%p20, %r31, %r3;
	@%p20 bra 	BB10_30;

	add.s32 	%r343, %r31, %r14;
	mul.wide.s32 	%rd39, %r343, 8;
	add.s64 	%rd40, %rd6, %rd39;
	ld.global.u32 	%r344, [%rd40];
	and.b32  	%r345, %r344, 131040;
	and.b32  	%r346, %r344, 31;
	mov.u32 	%r347, 1;
	shl.b32 	%r348, %r347, %r346;
	shr.u32 	%r349, %r345, 3;
	add.s32 	%r351, %r218, %r349;
	atom.shared.or.b32 	%r352, [%r351], %r348;

BB10_30:
	add.s32 	%r1125, %r1125, 1;

BB10_31:
	shl.b32 	%r353, %r1125, 10;
	add.s32 	%r34, %r353, %r1;
	setp.ge.s32	%p21, %r34, %r3;
	@%p21 bra 	BB10_33;

	add.s32 	%r354, %r34, %r14;
	mul.wide.s32 	%rd41, %r354, 8;
	add.s64 	%rd42, %rd6, %rd41;
	ld.global.u32 	%r355, [%rd42];
	and.b32  	%r356, %r355, 131040;
	and.b32  	%r357, %r355, 31;
	mov.u32 	%r358, 1;
	shl.b32 	%r359, %r358, %r357;
	shr.u32 	%r360, %r356, 3;
	add.s32 	%r362, %r218, %r360;
	atom.shared.or.b32 	%r363, [%r362], %r359;

BB10_33:
	add.s32 	%r1125, %r1125, 1;

BB10_34:
	setp.lt.u32	%p22, %r29, 4;
	@%p22 bra 	BB10_44;

BB10_35:
	shl.b32 	%r364, %r1125, 10;
	add.s32 	%r38, %r364, %r1;
	setp.ge.s32	%p23, %r38, %r3;
	@%p23 bra 	BB10_37;

	add.s32 	%r365, %r38, %r14;
	mul.wide.s32 	%rd43, %r365, 8;
	add.s64 	%rd44, %rd6, %rd43;
	ld.global.u32 	%r366, [%rd44];
	and.b32  	%r367, %r366, 131040;
	and.b32  	%r368, %r366, 31;
	mov.u32 	%r369, 1;
	shl.b32 	%r370, %r369, %r368;
	shr.u32 	%r371, %r367, 3;
	add.s32 	%r373, %r218, %r371;
	atom.shared.or.b32 	%r374, [%r373], %r370;

BB10_37:
	add.s32 	%r39, %r38, 1024;
	setp.ge.s32	%p24, %r39, %r3;
	@%p24 bra 	BB10_39;

	add.s32 	%r377, %r39, %r14;
	mul.wide.s32 	%rd45, %r377, 8;
	add.s64 	%rd46, %rd6, %rd45;
	ld.global.u32 	%r378, [%rd46];
	and.b32  	%r379, %r378, 131040;
	and.b32  	%r380, %r378, 31;
	mov.u32 	%r381, 1;
	shl.b32 	%r382, %r381, %r380;
	shr.u32 	%r383, %r379, 3;
	add.s32 	%r385, %r218, %r383;
	atom.shared.or.b32 	%r386, [%r385], %r382;

BB10_39:
	add.s32 	%r40, %r38, 2048;
	setp.ge.s32	%p25, %r40, %r3;
	@%p25 bra 	BB10_41;

	add.s32 	%r389, %r40, %r14;
	mul.wide.s32 	%rd47, %r389, 8;
	add.s64 	%rd48, %rd6, %rd47;
	ld.global.u32 	%r390, [%rd48];
	and.b32  	%r391, %r390, 131040;
	and.b32  	%r392, %r390, 31;
	mov.u32 	%r393, 1;
	shl.b32 	%r394, %r393, %r392;
	shr.u32 	%r395, %r391, 3;
	add.s32 	%r397, %r218, %r395;
	atom.shared.or.b32 	%r398, [%r397], %r394;

BB10_41:
	add.s32 	%r41, %r38, 3072;
	setp.ge.s32	%p26, %r41, %r3;
	@%p26 bra 	BB10_43;

	add.s32 	%r401, %r41, %r14;
	mul.wide.s32 	%rd49, %r401, 8;
	add.s64 	%rd50, %rd6, %rd49;
	ld.global.u32 	%r402, [%rd50];
	and.b32  	%r403, %r402, 131040;
	and.b32  	%r404, %r402, 31;
	mov.u32 	%r405, 1;
	shl.b32 	%r406, %r405, %r404;
	shr.u32 	%r407, %r403, 3;
	add.s32 	%r409, %r218, %r407;
	atom.shared.or.b32 	%r410, [%r409], %r406;

BB10_43:
	add.s32 	%r1125, %r1125, 4;
	setp.lt.s32	%p27, %r1125, %r9;
	@%p27 bra 	BB10_35;

BB10_44:
	setp.lt.s32	%p28, %r10, 1024;
	@%p28 bra 	BB10_66;

	mov.u32 	%r415, 1;
	max.s32 	%r43, %r11, %r415;
	and.b32  	%r414, %r43, 3;
	mov.u32 	%r1129, 0;
	setp.eq.s32	%p29, %r414, 0;
	@%p29 bra 	BB10_56;

	setp.eq.s32	%p30, %r414, 1;
	@%p30 bra 	BB10_53;

	setp.eq.s32	%p31, %r414, 2;
	@%p31 bra 	BB10_50;

	setp.ge.s32	%p32, %r1, %r4;
	mov.u32 	%r1129, %r415;
	@%p32 bra 	BB10_50;

	add.s32 	%r418, %r1, %r14;
	mul.wide.s32 	%rd51, %r418, 8;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.u32 	%r419, [%rd52];
	and.b32  	%r420, %r419, 131040;
	and.b32  	%r421, %r419, 31;
	mov.u32 	%r1129, 1;
	shl.b32 	%r422, %r1129, %r421;
	shr.u32 	%r423, %r420, 3;
	add.s32 	%r425, %r218, %r423;
	atom.shared.or.b32 	%r426, [%r425], %r422;

BB10_50:
	shl.b32 	%r427, %r1129, 10;
	add.s32 	%r45, %r427, %r1;
	setp.ge.s32	%p33, %r45, %r4;
	@%p33 bra 	BB10_52;

	add.s32 	%r428, %r45, %r14;
	mul.wide.s32 	%rd53, %r428, 8;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u32 	%r429, [%rd54];
	and.b32  	%r430, %r429, 131040;
	and.b32  	%r431, %r429, 31;
	mov.u32 	%r432, 1;
	shl.b32 	%r433, %r432, %r431;
	shr.u32 	%r434, %r430, 3;
	add.s32 	%r436, %r218, %r434;
	atom.shared.or.b32 	%r437, [%r436], %r433;

BB10_52:
	add.s32 	%r1129, %r1129, 1;

BB10_53:
	shl.b32 	%r438, %r1129, 10;
	add.s32 	%r48, %r438, %r1;
	setp.ge.s32	%p34, %r48, %r4;
	@%p34 bra 	BB10_55;

	add.s32 	%r439, %r48, %r14;
	mul.wide.s32 	%rd55, %r439, 8;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r440, [%rd56];
	and.b32  	%r441, %r440, 131040;
	and.b32  	%r442, %r440, 31;
	mov.u32 	%r443, 1;
	shl.b32 	%r444, %r443, %r442;
	shr.u32 	%r445, %r441, 3;
	add.s32 	%r447, %r218, %r445;
	atom.shared.or.b32 	%r448, [%r447], %r444;

BB10_55:
	add.s32 	%r1129, %r1129, 1;

BB10_56:
	setp.lt.u32	%p35, %r43, 4;
	@%p35 bra 	BB10_66;

BB10_57:
	shl.b32 	%r449, %r1129, 10;
	add.s32 	%r52, %r449, %r1;
	setp.ge.s32	%p36, %r52, %r4;
	@%p36 bra 	BB10_59;

	add.s32 	%r450, %r52, %r14;
	mul.wide.s32 	%rd57, %r450, 8;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u32 	%r451, [%rd58];
	and.b32  	%r452, %r451, 131040;
	and.b32  	%r453, %r451, 31;
	mov.u32 	%r454, 1;
	shl.b32 	%r455, %r454, %r453;
	shr.u32 	%r456, %r452, 3;
	add.s32 	%r458, %r218, %r456;
	atom.shared.or.b32 	%r459, [%r458], %r455;

BB10_59:
	add.s32 	%r53, %r52, 1024;
	setp.ge.s32	%p37, %r53, %r4;
	@%p37 bra 	BB10_61;

	add.s32 	%r462, %r53, %r14;
	mul.wide.s32 	%rd59, %r462, 8;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.u32 	%r463, [%rd60];
	and.b32  	%r464, %r463, 131040;
	and.b32  	%r465, %r463, 31;
	mov.u32 	%r466, 1;
	shl.b32 	%r467, %r466, %r465;
	shr.u32 	%r468, %r464, 3;
	add.s32 	%r470, %r218, %r468;
	atom.shared.or.b32 	%r471, [%r470], %r467;

BB10_61:
	add.s32 	%r54, %r52, 2048;
	setp.ge.s32	%p38, %r54, %r4;
	@%p38 bra 	BB10_63;

	add.s32 	%r474, %r54, %r14;
	mul.wide.s32 	%rd61, %r474, 8;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u32 	%r475, [%rd62];
	and.b32  	%r476, %r475, 131040;
	and.b32  	%r477, %r475, 31;
	mov.u32 	%r478, 1;
	shl.b32 	%r479, %r478, %r477;
	shr.u32 	%r480, %r476, 3;
	add.s32 	%r482, %r218, %r480;
	atom.shared.or.b32 	%r483, [%r482], %r479;

BB10_63:
	add.s32 	%r55, %r52, 3072;
	setp.ge.s32	%p39, %r55, %r4;
	@%p39 bra 	BB10_65;

	add.s32 	%r486, %r55, %r14;
	mul.wide.s32 	%rd63, %r486, 8;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.u32 	%r487, [%rd64];
	and.b32  	%r488, %r487, 131040;
	and.b32  	%r489, %r487, 31;
	mov.u32 	%r490, 1;
	shl.b32 	%r491, %r490, %r489;
	shr.u32 	%r492, %r488, 3;
	add.s32 	%r494, %r218, %r492;
	atom.shared.or.b32 	%r495, [%r494], %r491;

BB10_65:
	add.s32 	%r1129, %r1129, 4;
	setp.lt.s32	%p40, %r1129, %r11;
	@%p40 bra 	BB10_57;

BB10_66:
	setp.lt.s32	%p41, %r12, 1024;
	@%p41 bra 	BB10_88;

	mov.u32 	%r500, 1;
	max.s32 	%r57, %r13, %r500;
	and.b32  	%r499, %r57, 3;
	mov.u32 	%r1133, 0;
	setp.eq.s32	%p42, %r499, 0;
	@%p42 bra 	BB10_78;

	setp.eq.s32	%p43, %r499, 1;
	@%p43 bra 	BB10_75;

	setp.eq.s32	%p44, %r499, 2;
	@%p44 bra 	BB10_72;

	setp.ge.s32	%p45, %r1, %r5;
	mov.u32 	%r1133, %r500;
	@%p45 bra 	BB10_72;

	add.s32 	%r503, %r1, %r14;
	mul.wide.s32 	%rd65, %r503, 8;
	add.s64 	%rd66, %rd5, %rd65;
	ld.global.u32 	%r504, [%rd66];
	and.b32  	%r505, %r504, 131040;
	and.b32  	%r506, %r504, 31;
	mov.u32 	%r1133, 1;
	shl.b32 	%r507, %r1133, %r506;
	shr.u32 	%r508, %r505, 3;
	add.s32 	%r510, %r218, %r508;
	atom.shared.or.b32 	%r511, [%r510], %r507;

BB10_72:
	shl.b32 	%r512, %r1133, 10;
	add.s32 	%r59, %r512, %r1;
	setp.ge.s32	%p46, %r59, %r5;
	@%p46 bra 	BB10_74;

	add.s32 	%r513, %r59, %r14;
	mul.wide.s32 	%rd67, %r513, 8;
	add.s64 	%rd68, %rd5, %rd67;
	ld.global.u32 	%r514, [%rd68];
	and.b32  	%r515, %r514, 131040;
	and.b32  	%r516, %r514, 31;
	mov.u32 	%r517, 1;
	shl.b32 	%r518, %r517, %r516;
	shr.u32 	%r519, %r515, 3;
	add.s32 	%r521, %r218, %r519;
	atom.shared.or.b32 	%r522, [%r521], %r518;

BB10_74:
	add.s32 	%r1133, %r1133, 1;

BB10_75:
	shl.b32 	%r523, %r1133, 10;
	add.s32 	%r62, %r523, %r1;
	setp.ge.s32	%p47, %r62, %r5;
	@%p47 bra 	BB10_77;

	add.s32 	%r524, %r62, %r14;
	mul.wide.s32 	%rd69, %r524, 8;
	add.s64 	%rd70, %rd5, %rd69;
	ld.global.u32 	%r525, [%rd70];
	and.b32  	%r526, %r525, 131040;
	and.b32  	%r527, %r525, 31;
	mov.u32 	%r528, 1;
	shl.b32 	%r529, %r528, %r527;
	shr.u32 	%r530, %r526, 3;
	add.s32 	%r532, %r218, %r530;
	atom.shared.or.b32 	%r533, [%r532], %r529;

BB10_77:
	add.s32 	%r1133, %r1133, 1;

BB10_78:
	setp.lt.u32	%p48, %r57, 4;
	@%p48 bra 	BB10_88;

BB10_79:
	shl.b32 	%r534, %r1133, 10;
	add.s32 	%r66, %r534, %r1;
	setp.ge.s32	%p49, %r66, %r5;
	@%p49 bra 	BB10_81;

	add.s32 	%r535, %r66, %r14;
	mul.wide.s32 	%rd71, %r535, 8;
	add.s64 	%rd72, %rd5, %rd71;
	ld.global.u32 	%r536, [%rd72];
	and.b32  	%r537, %r536, 131040;
	and.b32  	%r538, %r536, 31;
	mov.u32 	%r539, 1;
	shl.b32 	%r540, %r539, %r538;
	shr.u32 	%r541, %r537, 3;
	add.s32 	%r543, %r218, %r541;
	atom.shared.or.b32 	%r544, [%r543], %r540;

BB10_81:
	add.s32 	%r67, %r66, 1024;
	setp.ge.s32	%p50, %r67, %r5;
	@%p50 bra 	BB10_83;

	add.s32 	%r547, %r67, %r14;
	mul.wide.s32 	%rd73, %r547, 8;
	add.s64 	%rd74, %rd5, %rd73;
	ld.global.u32 	%r548, [%rd74];
	and.b32  	%r549, %r548, 131040;
	and.b32  	%r550, %r548, 31;
	mov.u32 	%r551, 1;
	shl.b32 	%r552, %r551, %r550;
	shr.u32 	%r553, %r549, 3;
	add.s32 	%r555, %r218, %r553;
	atom.shared.or.b32 	%r556, [%r555], %r552;

BB10_83:
	add.s32 	%r68, %r66, 2048;
	setp.ge.s32	%p51, %r68, %r5;
	@%p51 bra 	BB10_85;

	add.s32 	%r559, %r68, %r14;
	mul.wide.s32 	%rd75, %r559, 8;
	add.s64 	%rd76, %rd5, %rd75;
	ld.global.u32 	%r560, [%rd76];
	and.b32  	%r561, %r560, 131040;
	and.b32  	%r562, %r560, 31;
	mov.u32 	%r563, 1;
	shl.b32 	%r564, %r563, %r562;
	shr.u32 	%r565, %r561, 3;
	add.s32 	%r567, %r218, %r565;
	atom.shared.or.b32 	%r568, [%r567], %r564;

BB10_85:
	add.s32 	%r69, %r66, 3072;
	setp.ge.s32	%p52, %r69, %r5;
	@%p52 bra 	BB10_87;

	add.s32 	%r571, %r69, %r14;
	mul.wide.s32 	%rd77, %r571, 8;
	add.s64 	%rd78, %rd5, %rd77;
	ld.global.u32 	%r572, [%rd78];
	and.b32  	%r573, %r572, 131040;
	and.b32  	%r574, %r572, 31;
	mov.u32 	%r575, 1;
	shl.b32 	%r576, %r575, %r574;
	shr.u32 	%r577, %r573, 3;
	add.s32 	%r579, %r218, %r577;
	atom.shared.or.b32 	%r580, [%r579], %r576;

BB10_87:
	add.s32 	%r1133, %r1133, 4;
	setp.lt.s32	%p53, %r1133, %r13;
	@%p53 bra 	BB10_79;

BB10_88:
	setp.gt.s32	%p1, %r6, 1023;
	bar.sync 	0;
	@!%p1 bra 	BB10_117;
	bra.uni 	BB10_89;

BB10_89:
	add.s32 	%r71, %r215, -1;
	mov.u32 	%r585, 1;
	max.s32 	%r72, %r7, %r585;
	and.b32  	%r584, %r72, 3;
	mov.u32 	%r1137, 0;
	setp.eq.s32	%p54, %r584, 0;
	@%p54 bra 	BB10_103;

	setp.eq.s32	%p55, %r584, 1;
	@%p55 bra 	BB10_99;

	setp.eq.s32	%p56, %r584, 2;
	@%p56 bra 	BB10_95;

	setp.ge.s32	%p57, %r1, %r2;
	mov.u32 	%r1137, %r585;
	@%p57 bra 	BB10_95;

	add.s32 	%r588, %r1, %r14;
	mul.wide.s32 	%rd79, %r588, 8;
	add.s64 	%rd80, %rd3, %rd79;
	ld.global.v2.u32 	{%r589, %r590}, [%rd80];
	and.b32  	%r591, %r589, 131040;
	and.b32  	%r592, %r589, 31;
	xor.b32  	%r593, %r592, 1;
	shr.u32 	%r594, %r591, 3;
	add.s32 	%r596, %r218, %r594;
	mov.u32 	%r1137, 1;
	shl.b32 	%r597, %r1137, %r593;
	ld.shared.u32 	%r598, [%r596];
	and.b32  	%r599, %r597, %r598;
	setp.eq.s32	%p58, %r599, 0;
	@%p58 bra 	BB10_95;

	bfe.u32 	%r601, %r590, 17, 12;
	mul.wide.u32 	%rd81, %r601, 4;
	add.s64 	%rd82, %rd1, %rd81;
	atom.global.add.u32 	%r602, [%rd82], 1;
	min.s32 	%r603, %r602, %r71;
	mad.lo.s32 	%r604, %r601, %r215, %r603;
	mul.wide.s32 	%rd83, %r604, 8;
	add.s64 	%rd84, %rd4, %rd83;
	st.global.v2.u32 	[%rd84], {%r590, %r589};

BB10_95:
	shl.b32 	%r605, %r1137, 10;
	add.s32 	%r77, %r605, %r1;
	setp.ge.s32	%p59, %r77, %r2;
	@%p59 bra 	BB10_98;

	add.s32 	%r606, %r77, %r14;
	mul.wide.s32 	%rd85, %r606, 8;
	add.s64 	%rd86, %rd3, %rd85;
	ld.global.v2.u32 	{%r607, %r608}, [%rd86];
	and.b32  	%r609, %r607, 131040;
	and.b32  	%r610, %r607, 31;
	xor.b32  	%r611, %r610, 1;
	shr.u32 	%r612, %r609, 3;
	add.s32 	%r614, %r218, %r612;
	mov.u32 	%r615, 1;
	shl.b32 	%r616, %r615, %r611;
	ld.shared.u32 	%r617, [%r614];
	and.b32  	%r618, %r616, %r617;
	setp.eq.s32	%p60, %r618, 0;
	@%p60 bra 	BB10_98;

	bfe.u32 	%r619, %r608, 17, 12;
	mul.wide.u32 	%rd87, %r619, 4;
	add.s64 	%rd88, %rd1, %rd87;
	atom.global.add.u32 	%r620, [%rd88], 1;
	min.s32 	%r621, %r620, %r71;
	mad.lo.s32 	%r622, %r619, %r215, %r621;
	mul.wide.s32 	%rd89, %r622, 8;
	add.s64 	%rd90, %rd4, %rd89;
	st.global.v2.u32 	[%rd90], {%r608, %r607};

BB10_98:
	add.s32 	%r1137, %r1137, 1;

BB10_99:
	shl.b32 	%r623, %r1137, 10;
	add.s32 	%r83, %r623, %r1;
	setp.ge.s32	%p61, %r83, %r2;
	@%p61 bra 	BB10_102;

	add.s32 	%r624, %r83, %r14;
	mul.wide.s32 	%rd91, %r624, 8;
	add.s64 	%rd92, %rd3, %rd91;
	ld.global.v2.u32 	{%r625, %r626}, [%rd92];
	and.b32  	%r627, %r625, 131040;
	and.b32  	%r628, %r625, 31;
	xor.b32  	%r629, %r628, 1;
	shr.u32 	%r630, %r627, 3;
	add.s32 	%r632, %r218, %r630;
	mov.u32 	%r633, 1;
	shl.b32 	%r634, %r633, %r629;
	ld.shared.u32 	%r635, [%r632];
	and.b32  	%r636, %r634, %r635;
	setp.eq.s32	%p62, %r636, 0;
	@%p62 bra 	BB10_102;

	bfe.u32 	%r637, %r626, 17, 12;
	mul.wide.u32 	%rd93, %r637, 4;
	add.s64 	%rd94, %rd1, %rd93;
	atom.global.add.u32 	%r638, [%rd94], 1;
	min.s32 	%r639, %r638, %r71;
	mad.lo.s32 	%r640, %r637, %r215, %r639;
	mul.wide.s32 	%rd95, %r640, 8;
	add.s64 	%rd96, %rd4, %rd95;
	st.global.v2.u32 	[%rd96], {%r626, %r625};

BB10_102:
	add.s32 	%r1137, %r1137, 1;

BB10_103:
	setp.lt.u32	%p63, %r72, 4;
	@%p63 bra 	BB10_117;

BB10_104:
	shl.b32 	%r641, %r1137, 10;
	add.s32 	%r90, %r641, %r1;
	setp.ge.s32	%p64, %r90, %r2;
	@%p64 bra 	BB10_107;

	add.s32 	%r642, %r90, %r14;
	mul.wide.s32 	%rd97, %r642, 8;
	add.s64 	%rd98, %rd3, %rd97;
	ld.global.v2.u32 	{%r643, %r644}, [%rd98];
	and.b32  	%r645, %r643, 131040;
	and.b32  	%r646, %r643, 31;
	xor.b32  	%r647, %r646, 1;
	shr.u32 	%r648, %r645, 3;
	add.s32 	%r650, %r218, %r648;
	mov.u32 	%r651, 1;
	shl.b32 	%r652, %r651, %r647;
	ld.shared.u32 	%r653, [%r650];
	and.b32  	%r654, %r652, %r653;
	setp.eq.s32	%p65, %r654, 0;
	@%p65 bra 	BB10_107;

	bfe.u32 	%r655, %r644, 17, 12;
	mul.wide.u32 	%rd99, %r655, 4;
	add.s64 	%rd100, %rd1, %rd99;
	atom.global.add.u32 	%r656, [%rd100], 1;
	min.s32 	%r657, %r656, %r71;
	mad.lo.s32 	%r658, %r655, %r215, %r657;
	mul.wide.s32 	%rd101, %r658, 8;
	add.s64 	%rd102, %rd4, %rd101;
	st.global.v2.u32 	[%rd102], {%r644, %r643};

BB10_107:
	add.s32 	%r94, %r90, 1024;
	setp.ge.s32	%p66, %r94, %r2;
	@%p66 bra 	BB10_110;

	add.s32 	%r661, %r94, %r14;
	mul.wide.s32 	%rd103, %r661, 8;
	add.s64 	%rd104, %rd3, %rd103;
	ld.global.v2.u32 	{%r662, %r663}, [%rd104];
	and.b32  	%r664, %r662, 131040;
	and.b32  	%r665, %r662, 31;
	xor.b32  	%r666, %r665, 1;
	shr.u32 	%r667, %r664, 3;
	add.s32 	%r669, %r218, %r667;
	mov.u32 	%r670, 1;
	shl.b32 	%r671, %r670, %r666;
	ld.shared.u32 	%r672, [%r669];
	and.b32  	%r673, %r671, %r672;
	setp.eq.s32	%p67, %r673, 0;
	@%p67 bra 	BB10_110;

	bfe.u32 	%r674, %r663, 17, 12;
	mul.wide.u32 	%rd105, %r674, 4;
	add.s64 	%rd106, %rd1, %rd105;
	atom.global.add.u32 	%r675, [%rd106], 1;
	min.s32 	%r676, %r675, %r71;
	mad.lo.s32 	%r677, %r674, %r215, %r676;
	mul.wide.s32 	%rd107, %r677, 8;
	add.s64 	%rd108, %rd4, %rd107;
	st.global.v2.u32 	[%rd108], {%r663, %r662};

BB10_110:
	add.s32 	%r98, %r90, 2048;
	setp.ge.s32	%p68, %r98, %r2;
	@%p68 bra 	BB10_113;

	add.s32 	%r680, %r98, %r14;
	mul.wide.s32 	%rd109, %r680, 8;
	add.s64 	%rd110, %rd3, %rd109;
	ld.global.v2.u32 	{%r681, %r682}, [%rd110];
	and.b32  	%r683, %r681, 131040;
	and.b32  	%r684, %r681, 31;
	xor.b32  	%r685, %r684, 1;
	shr.u32 	%r686, %r683, 3;
	add.s32 	%r688, %r218, %r686;
	mov.u32 	%r689, 1;
	shl.b32 	%r690, %r689, %r685;
	ld.shared.u32 	%r691, [%r688];
	and.b32  	%r692, %r690, %r691;
	setp.eq.s32	%p69, %r692, 0;
	@%p69 bra 	BB10_113;

	bfe.u32 	%r693, %r682, 17, 12;
	mul.wide.u32 	%rd111, %r693, 4;
	add.s64 	%rd112, %rd1, %rd111;
	atom.global.add.u32 	%r694, [%rd112], 1;
	min.s32 	%r695, %r694, %r71;
	mad.lo.s32 	%r696, %r693, %r215, %r695;
	mul.wide.s32 	%rd113, %r696, 8;
	add.s64 	%rd114, %rd4, %rd113;
	st.global.v2.u32 	[%rd114], {%r682, %r681};

BB10_113:
	add.s32 	%r102, %r90, 3072;
	setp.ge.s32	%p70, %r102, %r2;
	@%p70 bra 	BB10_116;

	add.s32 	%r699, %r102, %r14;
	mul.wide.s32 	%rd115, %r699, 8;
	add.s64 	%rd116, %rd3, %rd115;
	ld.global.v2.u32 	{%r700, %r701}, [%rd116];
	and.b32  	%r702, %r700, 131040;
	and.b32  	%r703, %r700, 31;
	xor.b32  	%r704, %r703, 1;
	shr.u32 	%r705, %r702, 3;
	add.s32 	%r707, %r218, %r705;
	mov.u32 	%r708, 1;
	shl.b32 	%r709, %r708, %r704;
	ld.shared.u32 	%r710, [%r707];
	and.b32  	%r711, %r709, %r710;
	setp.eq.s32	%p71, %r711, 0;
	@%p71 bra 	BB10_116;

	bfe.u32 	%r712, %r701, 17, 12;
	mul.wide.u32 	%rd117, %r712, 4;
	add.s64 	%rd118, %rd1, %rd117;
	atom.global.add.u32 	%r713, [%rd118], 1;
	min.s32 	%r714, %r713, %r71;
	mad.lo.s32 	%r715, %r712, %r215, %r714;
	mul.wide.s32 	%rd119, %r715, 8;
	add.s64 	%rd120, %rd4, %rd119;
	st.global.v2.u32 	[%rd120], {%r701, %r700};

BB10_116:
	add.s32 	%r1137, %r1137, 4;
	setp.lt.s32	%p72, %r1137, %r7;
	@%p72 bra 	BB10_104;

BB10_117:
	@%p15 bra 	BB10_146;

	add.s32 	%r107, %r215, -1;
	mov.u32 	%r720, 1;
	max.s32 	%r108, %r9, %r720;
	and.b32  	%r719, %r108, 3;
	mov.u32 	%r1141, 0;
	setp.eq.s32	%p74, %r719, 0;
	@%p74 bra 	BB10_132;

	setp.eq.s32	%p75, %r719, 1;
	@%p75 bra 	BB10_128;

	setp.eq.s32	%p76, %r719, 2;
	@%p76 bra 	BB10_124;

	setp.ge.s32	%p77, %r1, %r3;
	mov.u32 	%r1141, %r720;
	@%p77 bra 	BB10_124;

	add.s32 	%r723, %r1, %r14;
	mul.wide.s32 	%rd121, %r723, 8;
	add.s64 	%rd122, %rd6, %rd121;
	ld.global.v2.u32 	{%r724, %r725}, [%rd122];
	and.b32  	%r726, %r724, 131040;
	and.b32  	%r727, %r724, 31;
	xor.b32  	%r728, %r727, 1;
	shr.u32 	%r729, %r726, 3;
	add.s32 	%r731, %r218, %r729;
	mov.u32 	%r1141, 1;
	shl.b32 	%r732, %r1141, %r728;
	ld.shared.u32 	%r733, [%r731];
	and.b32  	%r734, %r732, %r733;
	setp.eq.s32	%p78, %r734, 0;
	@%p78 bra 	BB10_124;

	bfe.u32 	%r736, %r725, 17, 12;
	mul.wide.u32 	%rd123, %r736, 4;
	add.s64 	%rd124, %rd1, %rd123;
	atom.global.add.u32 	%r737, [%rd124], 1;
	min.s32 	%r738, %r737, %r107;
	mad.lo.s32 	%r739, %r736, %r215, %r738;
	mul.wide.s32 	%rd125, %r739, 8;
	add.s64 	%rd126, %rd4, %rd125;
	st.global.v2.u32 	[%rd126], {%r725, %r724};

BB10_124:
	shl.b32 	%r740, %r1141, 10;
	add.s32 	%r113, %r740, %r1;
	setp.ge.s32	%p79, %r113, %r3;
	@%p79 bra 	BB10_127;

	add.s32 	%r741, %r113, %r14;
	mul.wide.s32 	%rd127, %r741, 8;
	add.s64 	%rd128, %rd6, %rd127;
	ld.global.v2.u32 	{%r742, %r743}, [%rd128];
	and.b32  	%r744, %r742, 131040;
	and.b32  	%r745, %r742, 31;
	xor.b32  	%r746, %r745, 1;
	shr.u32 	%r747, %r744, 3;
	add.s32 	%r749, %r218, %r747;
	mov.u32 	%r750, 1;
	shl.b32 	%r751, %r750, %r746;
	ld.shared.u32 	%r752, [%r749];
	and.b32  	%r753, %r751, %r752;
	setp.eq.s32	%p80, %r753, 0;
	@%p80 bra 	BB10_127;

	bfe.u32 	%r754, %r743, 17, 12;
	mul.wide.u32 	%rd129, %r754, 4;
	add.s64 	%rd130, %rd1, %rd129;
	atom.global.add.u32 	%r755, [%rd130], 1;
	min.s32 	%r756, %r755, %r107;
	mad.lo.s32 	%r757, %r754, %r215, %r756;
	mul.wide.s32 	%rd131, %r757, 8;
	add.s64 	%rd132, %rd4, %rd131;
	st.global.v2.u32 	[%rd132], {%r743, %r742};

BB10_127:
	add.s32 	%r1141, %r1141, 1;

BB10_128:
	shl.b32 	%r758, %r1141, 10;
	add.s32 	%r119, %r758, %r1;
	setp.ge.s32	%p81, %r119, %r3;
	@%p81 bra 	BB10_131;

	add.s32 	%r759, %r119, %r14;
	mul.wide.s32 	%rd133, %r759, 8;
	add.s64 	%rd134, %rd6, %rd133;
	ld.global.v2.u32 	{%r760, %r761}, [%rd134];
	and.b32  	%r762, %r760, 131040;
	and.b32  	%r763, %r760, 31;
	xor.b32  	%r764, %r763, 1;
	shr.u32 	%r765, %r762, 3;
	add.s32 	%r767, %r218, %r765;
	mov.u32 	%r768, 1;
	shl.b32 	%r769, %r768, %r764;
	ld.shared.u32 	%r770, [%r767];
	and.b32  	%r771, %r769, %r770;
	setp.eq.s32	%p82, %r771, 0;
	@%p82 bra 	BB10_131;

	bfe.u32 	%r772, %r761, 17, 12;
	mul.wide.u32 	%rd135, %r772, 4;
	add.s64 	%rd136, %rd1, %rd135;
	atom.global.add.u32 	%r773, [%rd136], 1;
	min.s32 	%r774, %r773, %r107;
	mad.lo.s32 	%r775, %r772, %r215, %r774;
	mul.wide.s32 	%rd137, %r775, 8;
	add.s64 	%rd138, %rd4, %rd137;
	st.global.v2.u32 	[%rd138], {%r761, %r760};

BB10_131:
	add.s32 	%r1141, %r1141, 1;

BB10_132:
	setp.lt.u32	%p83, %r108, 4;
	@%p83 bra 	BB10_146;

BB10_133:
	shl.b32 	%r776, %r1141, 10;
	add.s32 	%r126, %r776, %r1;
	setp.ge.s32	%p84, %r126, %r3;
	@%p84 bra 	BB10_136;

	add.s32 	%r777, %r126, %r14;
	mul.wide.s32 	%rd139, %r777, 8;
	add.s64 	%rd140, %rd6, %rd139;
	ld.global.v2.u32 	{%r778, %r779}, [%rd140];
	and.b32  	%r780, %r778, 131040;
	and.b32  	%r781, %r778, 31;
	xor.b32  	%r782, %r781, 1;
	shr.u32 	%r783, %r780, 3;
	add.s32 	%r785, %r218, %r783;
	mov.u32 	%r786, 1;
	shl.b32 	%r787, %r786, %r782;
	ld.shared.u32 	%r788, [%r785];
	and.b32  	%r789, %r787, %r788;
	setp.eq.s32	%p85, %r789, 0;
	@%p85 bra 	BB10_136;

	bfe.u32 	%r790, %r779, 17, 12;
	mul.wide.u32 	%rd141, %r790, 4;
	add.s64 	%rd142, %rd1, %rd141;
	atom.global.add.u32 	%r791, [%rd142], 1;
	min.s32 	%r792, %r791, %r107;
	mad.lo.s32 	%r793, %r790, %r215, %r792;
	mul.wide.s32 	%rd143, %r793, 8;
	add.s64 	%rd144, %rd4, %rd143;
	st.global.v2.u32 	[%rd144], {%r779, %r778};

BB10_136:
	add.s32 	%r130, %r126, 1024;
	setp.ge.s32	%p86, %r130, %r3;
	@%p86 bra 	BB10_139;

	add.s32 	%r796, %r130, %r14;
	mul.wide.s32 	%rd145, %r796, 8;
	add.s64 	%rd146, %rd6, %rd145;
	ld.global.v2.u32 	{%r797, %r798}, [%rd146];
	and.b32  	%r799, %r797, 131040;
	and.b32  	%r800, %r797, 31;
	xor.b32  	%r801, %r800, 1;
	shr.u32 	%r802, %r799, 3;
	add.s32 	%r804, %r218, %r802;
	mov.u32 	%r805, 1;
	shl.b32 	%r806, %r805, %r801;
	ld.shared.u32 	%r807, [%r804];
	and.b32  	%r808, %r806, %r807;
	setp.eq.s32	%p87, %r808, 0;
	@%p87 bra 	BB10_139;

	bfe.u32 	%r809, %r798, 17, 12;
	mul.wide.u32 	%rd147, %r809, 4;
	add.s64 	%rd148, %rd1, %rd147;
	atom.global.add.u32 	%r810, [%rd148], 1;
	min.s32 	%r811, %r810, %r107;
	mad.lo.s32 	%r812, %r809, %r215, %r811;
	mul.wide.s32 	%rd149, %r812, 8;
	add.s64 	%rd150, %rd4, %rd149;
	st.global.v2.u32 	[%rd150], {%r798, %r797};

BB10_139:
	add.s32 	%r134, %r126, 2048;
	setp.ge.s32	%p88, %r134, %r3;
	@%p88 bra 	BB10_142;

	add.s32 	%r815, %r134, %r14;
	mul.wide.s32 	%rd151, %r815, 8;
	add.s64 	%rd152, %rd6, %rd151;
	ld.global.v2.u32 	{%r816, %r817}, [%rd152];
	and.b32  	%r818, %r816, 131040;
	and.b32  	%r819, %r816, 31;
	xor.b32  	%r820, %r819, 1;
	shr.u32 	%r821, %r818, 3;
	add.s32 	%r823, %r218, %r821;
	mov.u32 	%r824, 1;
	shl.b32 	%r825, %r824, %r820;
	ld.shared.u32 	%r826, [%r823];
	and.b32  	%r827, %r825, %r826;
	setp.eq.s32	%p89, %r827, 0;
	@%p89 bra 	BB10_142;

	bfe.u32 	%r828, %r817, 17, 12;
	mul.wide.u32 	%rd153, %r828, 4;
	add.s64 	%rd154, %rd1, %rd153;
	atom.global.add.u32 	%r829, [%rd154], 1;
	min.s32 	%r830, %r829, %r107;
	mad.lo.s32 	%r831, %r828, %r215, %r830;
	mul.wide.s32 	%rd155, %r831, 8;
	add.s64 	%rd156, %rd4, %rd155;
	st.global.v2.u32 	[%rd156], {%r817, %r816};

BB10_142:
	add.s32 	%r138, %r126, 3072;
	setp.ge.s32	%p90, %r138, %r3;
	@%p90 bra 	BB10_145;

	add.s32 	%r834, %r138, %r14;
	mul.wide.s32 	%rd157, %r834, 8;
	add.s64 	%rd158, %rd6, %rd157;
	ld.global.v2.u32 	{%r835, %r836}, [%rd158];
	and.b32  	%r837, %r835, 131040;
	and.b32  	%r838, %r835, 31;
	xor.b32  	%r839, %r838, 1;
	shr.u32 	%r840, %r837, 3;
	add.s32 	%r842, %r218, %r840;
	mov.u32 	%r843, 1;
	shl.b32 	%r844, %r843, %r839;
	ld.shared.u32 	%r845, [%r842];
	and.b32  	%r846, %r844, %r845;
	setp.eq.s32	%p91, %r846, 0;
	@%p91 bra 	BB10_145;

	bfe.u32 	%r847, %r836, 17, 12;
	mul.wide.u32 	%rd159, %r847, 4;
	add.s64 	%rd160, %rd1, %rd159;
	atom.global.add.u32 	%r848, [%rd160], 1;
	min.s32 	%r849, %r848, %r107;
	mad.lo.s32 	%r850, %r847, %r215, %r849;
	mul.wide.s32 	%rd161, %r850, 8;
	add.s64 	%rd162, %rd4, %rd161;
	st.global.v2.u32 	[%rd162], {%r836, %r835};

BB10_145:
	add.s32 	%r1141, %r1141, 4;
	setp.lt.s32	%p92, %r1141, %r9;
	@%p92 bra 	BB10_133;

BB10_146:
	@%p28 bra 	BB10_175;

	add.s32 	%r143, %r215, -1;
	mov.u32 	%r855, 1;
	max.s32 	%r144, %r11, %r855;
	and.b32  	%r854, %r144, 3;
	mov.u32 	%r1145, 0;
	setp.eq.s32	%p94, %r854, 0;
	@%p94 bra 	BB10_161;

	setp.eq.s32	%p95, %r854, 1;
	@%p95 bra 	BB10_157;

	setp.eq.s32	%p96, %r854, 2;
	@%p96 bra 	BB10_153;

	setp.ge.s32	%p97, %r1, %r4;
	mov.u32 	%r1145, %r855;
	@%p97 bra 	BB10_153;

	add.s32 	%r858, %r1, %r14;
	mul.wide.s32 	%rd163, %r858, 8;
	add.s64 	%rd164, %rd2, %rd163;
	ld.global.v2.u32 	{%r859, %r860}, [%rd164];
	and.b32  	%r861, %r859, 131040;
	and.b32  	%r862, %r859, 31;
	xor.b32  	%r863, %r862, 1;
	shr.u32 	%r864, %r861, 3;
	add.s32 	%r866, %r218, %r864;
	mov.u32 	%r1145, 1;
	shl.b32 	%r867, %r1145, %r863;
	ld.shared.u32 	%r868, [%r866];
	and.b32  	%r869, %r867, %r868;
	setp.eq.s32	%p98, %r869, 0;
	@%p98 bra 	BB10_153;

	bfe.u32 	%r871, %r860, 17, 12;
	mul.wide.u32 	%rd165, %r871, 4;
	add.s64 	%rd166, %rd1, %rd165;
	atom.global.add.u32 	%r872, [%rd166], 1;
	min.s32 	%r873, %r872, %r143;
	mad.lo.s32 	%r874, %r871, %r215, %r873;
	mul.wide.s32 	%rd167, %r874, 8;
	add.s64 	%rd168, %rd4, %rd167;
	st.global.v2.u32 	[%rd168], {%r860, %r859};

BB10_153:
	shl.b32 	%r875, %r1145, 10;
	add.s32 	%r149, %r875, %r1;
	setp.ge.s32	%p99, %r149, %r4;
	@%p99 bra 	BB10_156;

	add.s32 	%r876, %r149, %r14;
	mul.wide.s32 	%rd169, %r876, 8;
	add.s64 	%rd170, %rd2, %rd169;
	ld.global.v2.u32 	{%r877, %r878}, [%rd170];
	and.b32  	%r879, %r877, 131040;
	and.b32  	%r880, %r877, 31;
	xor.b32  	%r881, %r880, 1;
	shr.u32 	%r882, %r879, 3;
	add.s32 	%r884, %r218, %r882;
	mov.u32 	%r885, 1;
	shl.b32 	%r886, %r885, %r881;
	ld.shared.u32 	%r887, [%r884];
	and.b32  	%r888, %r886, %r887;
	setp.eq.s32	%p100, %r888, 0;
	@%p100 bra 	BB10_156;

	bfe.u32 	%r889, %r878, 17, 12;
	mul.wide.u32 	%rd171, %r889, 4;
	add.s64 	%rd172, %rd1, %rd171;
	atom.global.add.u32 	%r890, [%rd172], 1;
	min.s32 	%r891, %r890, %r143;
	mad.lo.s32 	%r892, %r889, %r215, %r891;
	mul.wide.s32 	%rd173, %r892, 8;
	add.s64 	%rd174, %rd4, %rd173;
	st.global.v2.u32 	[%rd174], {%r878, %r877};

BB10_156:
	add.s32 	%r1145, %r1145, 1;

BB10_157:
	shl.b32 	%r893, %r1145, 10;
	add.s32 	%r155, %r893, %r1;
	setp.ge.s32	%p101, %r155, %r4;
	@%p101 bra 	BB10_160;

	add.s32 	%r894, %r155, %r14;
	mul.wide.s32 	%rd175, %r894, 8;
	add.s64 	%rd176, %rd2, %rd175;
	ld.global.v2.u32 	{%r895, %r896}, [%rd176];
	and.b32  	%r897, %r895, 131040;
	and.b32  	%r898, %r895, 31;
	xor.b32  	%r899, %r898, 1;
	shr.u32 	%r900, %r897, 3;
	add.s32 	%r902, %r218, %r900;
	mov.u32 	%r903, 1;
	shl.b32 	%r904, %r903, %r899;
	ld.shared.u32 	%r905, [%r902];
	and.b32  	%r906, %r904, %r905;
	setp.eq.s32	%p102, %r906, 0;
	@%p102 bra 	BB10_160;

	bfe.u32 	%r907, %r896, 17, 12;
	mul.wide.u32 	%rd177, %r907, 4;
	add.s64 	%rd178, %rd1, %rd177;
	atom.global.add.u32 	%r908, [%rd178], 1;
	min.s32 	%r909, %r908, %r143;
	mad.lo.s32 	%r910, %r907, %r215, %r909;
	mul.wide.s32 	%rd179, %r910, 8;
	add.s64 	%rd180, %rd4, %rd179;
	st.global.v2.u32 	[%rd180], {%r896, %r895};

BB10_160:
	add.s32 	%r1145, %r1145, 1;

BB10_161:
	setp.lt.u32	%p103, %r144, 4;
	@%p103 bra 	BB10_175;

BB10_162:
	shl.b32 	%r911, %r1145, 10;
	add.s32 	%r162, %r911, %r1;
	setp.ge.s32	%p104, %r162, %r4;
	@%p104 bra 	BB10_165;

	add.s32 	%r912, %r162, %r14;
	mul.wide.s32 	%rd181, %r912, 8;
	add.s64 	%rd182, %rd2, %rd181;
	ld.global.v2.u32 	{%r913, %r914}, [%rd182];
	and.b32  	%r915, %r913, 131040;
	and.b32  	%r916, %r913, 31;
	xor.b32  	%r917, %r916, 1;
	shr.u32 	%r918, %r915, 3;
	add.s32 	%r920, %r218, %r918;
	mov.u32 	%r921, 1;
	shl.b32 	%r922, %r921, %r917;
	ld.shared.u32 	%r923, [%r920];
	and.b32  	%r924, %r922, %r923;
	setp.eq.s32	%p105, %r924, 0;
	@%p105 bra 	BB10_165;

	bfe.u32 	%r925, %r914, 17, 12;
	mul.wide.u32 	%rd183, %r925, 4;
	add.s64 	%rd184, %rd1, %rd183;
	atom.global.add.u32 	%r926, [%rd184], 1;
	min.s32 	%r927, %r926, %r143;
	mad.lo.s32 	%r928, %r925, %r215, %r927;
	mul.wide.s32 	%rd185, %r928, 8;
	add.s64 	%rd186, %rd4, %rd185;
	st.global.v2.u32 	[%rd186], {%r914, %r913};

BB10_165:
	add.s32 	%r166, %r162, 1024;
	setp.ge.s32	%p106, %r166, %r4;
	@%p106 bra 	BB10_168;

	add.s32 	%r931, %r166, %r14;
	mul.wide.s32 	%rd187, %r931, 8;
	add.s64 	%rd188, %rd2, %rd187;
	ld.global.v2.u32 	{%r932, %r933}, [%rd188];
	and.b32  	%r934, %r932, 131040;
	and.b32  	%r935, %r932, 31;
	xor.b32  	%r936, %r935, 1;
	shr.u32 	%r937, %r934, 3;
	add.s32 	%r939, %r218, %r937;
	mov.u32 	%r940, 1;
	shl.b32 	%r941, %r940, %r936;
	ld.shared.u32 	%r942, [%r939];
	and.b32  	%r943, %r941, %r942;
	setp.eq.s32	%p107, %r943, 0;
	@%p107 bra 	BB10_168;

	bfe.u32 	%r944, %r933, 17, 12;
	mul.wide.u32 	%rd189, %r944, 4;
	add.s64 	%rd190, %rd1, %rd189;
	atom.global.add.u32 	%r945, [%rd190], 1;
	min.s32 	%r946, %r945, %r143;
	mad.lo.s32 	%r947, %r944, %r215, %r946;
	mul.wide.s32 	%rd191, %r947, 8;
	add.s64 	%rd192, %rd4, %rd191;
	st.global.v2.u32 	[%rd192], {%r933, %r932};

BB10_168:
	add.s32 	%r170, %r162, 2048;
	setp.ge.s32	%p108, %r170, %r4;
	@%p108 bra 	BB10_171;

	add.s32 	%r950, %r170, %r14;
	mul.wide.s32 	%rd193, %r950, 8;
	add.s64 	%rd194, %rd2, %rd193;
	ld.global.v2.u32 	{%r951, %r952}, [%rd194];
	and.b32  	%r953, %r951, 131040;
	and.b32  	%r954, %r951, 31;
	xor.b32  	%r955, %r954, 1;
	shr.u32 	%r956, %r953, 3;
	add.s32 	%r958, %r218, %r956;
	mov.u32 	%r959, 1;
	shl.b32 	%r960, %r959, %r955;
	ld.shared.u32 	%r961, [%r958];
	and.b32  	%r962, %r960, %r961;
	setp.eq.s32	%p109, %r962, 0;
	@%p109 bra 	BB10_171;

	bfe.u32 	%r963, %r952, 17, 12;
	mul.wide.u32 	%rd195, %r963, 4;
	add.s64 	%rd196, %rd1, %rd195;
	atom.global.add.u32 	%r964, [%rd196], 1;
	min.s32 	%r965, %r964, %r143;
	mad.lo.s32 	%r966, %r963, %r215, %r965;
	mul.wide.s32 	%rd197, %r966, 8;
	add.s64 	%rd198, %rd4, %rd197;
	st.global.v2.u32 	[%rd198], {%r952, %r951};

BB10_171:
	add.s32 	%r174, %r162, 3072;
	setp.ge.s32	%p110, %r174, %r4;
	@%p110 bra 	BB10_174;

	add.s32 	%r969, %r174, %r14;
	mul.wide.s32 	%rd199, %r969, 8;
	add.s64 	%rd200, %rd2, %rd199;
	ld.global.v2.u32 	{%r970, %r971}, [%rd200];
	and.b32  	%r972, %r970, 131040;
	and.b32  	%r973, %r970, 31;
	xor.b32  	%r974, %r973, 1;
	shr.u32 	%r975, %r972, 3;
	add.s32 	%r977, %r218, %r975;
	mov.u32 	%r978, 1;
	shl.b32 	%r979, %r978, %r974;
	ld.shared.u32 	%r980, [%r977];
	and.b32  	%r981, %r979, %r980;
	setp.eq.s32	%p111, %r981, 0;
	@%p111 bra 	BB10_174;

	bfe.u32 	%r982, %r971, 17, 12;
	mul.wide.u32 	%rd201, %r982, 4;
	add.s64 	%rd202, %rd1, %rd201;
	atom.global.add.u32 	%r983, [%rd202], 1;
	min.s32 	%r984, %r983, %r143;
	mad.lo.s32 	%r985, %r982, %r215, %r984;
	mul.wide.s32 	%rd203, %r985, 8;
	add.s64 	%rd204, %rd4, %rd203;
	st.global.v2.u32 	[%rd204], {%r971, %r970};

BB10_174:
	add.s32 	%r1145, %r1145, 4;
	setp.lt.s32	%p112, %r1145, %r11;
	@%p112 bra 	BB10_162;

BB10_175:
	@%p41 bra 	BB10_204;

	add.s32 	%r179, %r215, -1;
	mov.u32 	%r990, 1;
	max.s32 	%r180, %r13, %r990;
	and.b32  	%r989, %r180, 3;
	setp.eq.s32	%p114, %r989, 0;
	@%p114 bra 	BB10_190;

	setp.eq.s32	%p115, %r989, 1;
	@%p115 bra 	BB10_186;

	setp.eq.s32	%p116, %r989, 2;
	@%p116 bra 	BB10_182;

	setp.ge.s32	%p117, %r1, %r5;
	mov.u32 	%r1149, %r990;
	@%p117 bra 	BB10_182;

	add.s32 	%r993, %r1, %r14;
	mul.wide.s32 	%rd205, %r993, 8;
	add.s64 	%rd206, %rd5, %rd205;
	ld.global.v2.u32 	{%r994, %r995}, [%rd206];
	and.b32  	%r996, %r994, 131040;
	and.b32  	%r997, %r994, 31;
	xor.b32  	%r998, %r997, 1;
	shr.u32 	%r999, %r996, 3;
	add.s32 	%r1001, %r218, %r999;
	mov.u32 	%r1149, 1;
	shl.b32 	%r1002, %r1149, %r998;
	ld.shared.u32 	%r1003, [%r1001];
	and.b32  	%r1004, %r1002, %r1003;
	setp.eq.s32	%p118, %r1004, 0;
	@%p118 bra 	BB10_182;

	bfe.u32 	%r1006, %r995, 17, 12;
	mul.wide.u32 	%rd207, %r1006, 4;
	add.s64 	%rd208, %rd1, %rd207;
	atom.global.add.u32 	%r1007, [%rd208], 1;
	min.s32 	%r1008, %r1007, %r179;
	mad.lo.s32 	%r1009, %r1006, %r215, %r1008;
	mul.wide.s32 	%rd209, %r1009, 8;
	add.s64 	%rd210, %rd4, %rd209;
	st.global.v2.u32 	[%rd210], {%r995, %r994};

BB10_182:
	shl.b32 	%r1010, %r1149, 10;
	add.s32 	%r185, %r1010, %r1;
	setp.ge.s32	%p119, %r185, %r5;
	@%p119 bra 	BB10_185;

	add.s32 	%r1011, %r185, %r14;
	mul.wide.s32 	%rd211, %r1011, 8;
	add.s64 	%rd212, %rd5, %rd211;
	ld.global.v2.u32 	{%r1012, %r1013}, [%rd212];
	and.b32  	%r1014, %r1012, 131040;
	and.b32  	%r1015, %r1012, 31;
	xor.b32  	%r1016, %r1015, 1;
	shr.u32 	%r1017, %r1014, 3;
	add.s32 	%r1019, %r218, %r1017;
	mov.u32 	%r1020, 1;
	shl.b32 	%r1021, %r1020, %r1016;
	ld.shared.u32 	%r1022, [%r1019];
	and.b32  	%r1023, %r1021, %r1022;
	setp.eq.s32	%p120, %r1023, 0;
	@%p120 bra 	BB10_185;

	bfe.u32 	%r1024, %r1013, 17, 12;
	mul.wide.u32 	%rd213, %r1024, 4;
	add.s64 	%rd214, %rd1, %rd213;
	atom.global.add.u32 	%r1025, [%rd214], 1;
	min.s32 	%r1026, %r1025, %r179;
	mad.lo.s32 	%r1027, %r1024, %r215, %r1026;
	mul.wide.s32 	%rd215, %r1027, 8;
	add.s64 	%rd216, %rd4, %rd215;
	st.global.v2.u32 	[%rd216], {%r1013, %r1012};

BB10_185:
	add.s32 	%r1149, %r1149, 1;

BB10_186:
	shl.b32 	%r1028, %r1149, 10;
	add.s32 	%r191, %r1028, %r1;
	setp.ge.s32	%p121, %r191, %r5;
	@%p121 bra 	BB10_189;

	add.s32 	%r1029, %r191, %r14;
	mul.wide.s32 	%rd217, %r1029, 8;
	add.s64 	%rd218, %rd5, %rd217;
	ld.global.v2.u32 	{%r1030, %r1031}, [%rd218];
	and.b32  	%r1032, %r1030, 131040;
	and.b32  	%r1033, %r1030, 31;
	xor.b32  	%r1034, %r1033, 1;
	shr.u32 	%r1035, %r1032, 3;
	add.s32 	%r1037, %r218, %r1035;
	mov.u32 	%r1038, 1;
	shl.b32 	%r1039, %r1038, %r1034;
	ld.shared.u32 	%r1040, [%r1037];
	and.b32  	%r1041, %r1039, %r1040;
	setp.eq.s32	%p122, %r1041, 0;
	@%p122 bra 	BB10_189;

	bfe.u32 	%r1042, %r1031, 17, 12;
	mul.wide.u32 	%rd219, %r1042, 4;
	add.s64 	%rd220, %rd1, %rd219;
	atom.global.add.u32 	%r1043, [%rd220], 1;
	min.s32 	%r1044, %r1043, %r179;
	mad.lo.s32 	%r1045, %r1042, %r215, %r1044;
	mul.wide.s32 	%rd221, %r1045, 8;
	add.s64 	%rd222, %rd4, %rd221;
	st.global.v2.u32 	[%rd222], {%r1031, %r1030};

BB10_189:
	add.s32 	%r1149, %r1149, 1;

BB10_190:
	setp.lt.u32	%p123, %r180, 4;
	@%p123 bra 	BB10_204;

BB10_191:
	shl.b32 	%r1046, %r1149, 10;
	add.s32 	%r198, %r1046, %r1;
	setp.ge.s32	%p124, %r198, %r5;
	@%p124 bra 	BB10_194;

	add.s32 	%r1047, %r198, %r14;
	mul.wide.s32 	%rd223, %r1047, 8;
	add.s64 	%rd224, %rd5, %rd223;
	ld.global.v2.u32 	{%r1048, %r1049}, [%rd224];
	and.b32  	%r1050, %r1048, 131040;
	and.b32  	%r1051, %r1048, 31;
	xor.b32  	%r1052, %r1051, 1;
	shr.u32 	%r1053, %r1050, 3;
	add.s32 	%r1055, %r218, %r1053;
	mov.u32 	%r1056, 1;
	shl.b32 	%r1057, %r1056, %r1052;
	ld.shared.u32 	%r1058, [%r1055];
	and.b32  	%r1059, %r1057, %r1058;
	setp.eq.s32	%p125, %r1059, 0;
	@%p125 bra 	BB10_194;

	bfe.u32 	%r1060, %r1049, 17, 12;
	mul.wide.u32 	%rd225, %r1060, 4;
	add.s64 	%rd226, %rd1, %rd225;
	atom.global.add.u32 	%r1061, [%rd226], 1;
	min.s32 	%r1062, %r1061, %r179;
	mad.lo.s32 	%r1063, %r1060, %r215, %r1062;
	mul.wide.s32 	%rd227, %r1063, 8;
	add.s64 	%rd228, %rd4, %rd227;
	st.global.v2.u32 	[%rd228], {%r1049, %r1048};

BB10_194:
	add.s32 	%r202, %r198, 1024;
	setp.ge.s32	%p126, %r202, %r5;
	@%p126 bra 	BB10_197;

	add.s32 	%r1066, %r202, %r14;
	mul.wide.s32 	%rd229, %r1066, 8;
	add.s64 	%rd230, %rd5, %rd229;
	ld.global.v2.u32 	{%r1067, %r1068}, [%rd230];
	and.b32  	%r1069, %r1067, 131040;
	and.b32  	%r1070, %r1067, 31;
	xor.b32  	%r1071, %r1070, 1;
	shr.u32 	%r1072, %r1069, 3;
	add.s32 	%r1074, %r218, %r1072;
	mov.u32 	%r1075, 1;
	shl.b32 	%r1076, %r1075, %r1071;
	ld.shared.u32 	%r1077, [%r1074];
	and.b32  	%r1078, %r1076, %r1077;
	setp.eq.s32	%p127, %r1078, 0;
	@%p127 bra 	BB10_197;

	bfe.u32 	%r1079, %r1068, 17, 12;
	mul.wide.u32 	%rd231, %r1079, 4;
	add.s64 	%rd232, %rd1, %rd231;
	atom.global.add.u32 	%r1080, [%rd232], 1;
	min.s32 	%r1081, %r1080, %r179;
	mad.lo.s32 	%r1082, %r1079, %r215, %r1081;
	mul.wide.s32 	%rd233, %r1082, 8;
	add.s64 	%rd234, %rd4, %rd233;
	st.global.v2.u32 	[%rd234], {%r1068, %r1067};

BB10_197:
	add.s32 	%r206, %r198, 2048;
	setp.ge.s32	%p128, %r206, %r5;
	@%p128 bra 	BB10_200;

	add.s32 	%r1085, %r206, %r14;
	mul.wide.s32 	%rd235, %r1085, 8;
	add.s64 	%rd236, %rd5, %rd235;
	ld.global.v2.u32 	{%r1086, %r1087}, [%rd236];
	and.b32  	%r1088, %r1086, 131040;
	and.b32  	%r1089, %r1086, 31;
	xor.b32  	%r1090, %r1089, 1;
	shr.u32 	%r1091, %r1088, 3;
	add.s32 	%r1093, %r218, %r1091;
	mov.u32 	%r1094, 1;
	shl.b32 	%r1095, %r1094, %r1090;
	ld.shared.u32 	%r1096, [%r1093];
	and.b32  	%r1097, %r1095, %r1096;
	setp.eq.s32	%p129, %r1097, 0;
	@%p129 bra 	BB10_200;

	bfe.u32 	%r1098, %r1087, 17, 12;
	mul.wide.u32 	%rd237, %r1098, 4;
	add.s64 	%rd238, %rd1, %rd237;
	atom.global.add.u32 	%r1099, [%rd238], 1;
	min.s32 	%r1100, %r1099, %r179;
	mad.lo.s32 	%r1101, %r1098, %r215, %r1100;
	mul.wide.s32 	%rd239, %r1101, 8;
	add.s64 	%rd240, %rd4, %rd239;
	st.global.v2.u32 	[%rd240], {%r1087, %r1086};

BB10_200:
	add.s32 	%r210, %r198, 3072;
	setp.ge.s32	%p130, %r210, %r5;
	@%p130 bra 	BB10_203;

	add.s32 	%r1104, %r210, %r14;
	mul.wide.s32 	%rd241, %r1104, 8;
	add.s64 	%rd242, %rd5, %rd241;
	ld.global.v2.u32 	{%r1105, %r1106}, [%rd242];
	and.b32  	%r1107, %r1105, 131040;
	and.b32  	%r1108, %r1105, 31;
	xor.b32  	%r1109, %r1108, 1;
	shr.u32 	%r1110, %r1107, 3;
	add.s32 	%r1112, %r218, %r1110;
	mov.u32 	%r1113, 1;
	shl.b32 	%r1114, %r1113, %r1109;
	ld.shared.u32 	%r1115, [%r1112];
	and.b32  	%r1116, %r1114, %r1115;
	setp.eq.s32	%p131, %r1116, 0;
	@%p131 bra 	BB10_203;

	bfe.u32 	%r1117, %r1106, 17, 12;
	mul.wide.u32 	%rd243, %r1117, 4;
	add.s64 	%rd244, %rd1, %rd243;
	atom.global.add.u32 	%r1118, [%rd244], 1;
	min.s32 	%r1119, %r1118, %r179;
	mad.lo.s32 	%r1120, %r1117, %r215, %r1119;
	mul.wide.s32 	%rd245, %r1120, 8;
	add.s64 	%rd246, %rd4, %rd245;
	st.global.v2.u32 	[%rd246], {%r1106, %r1105};

BB10_203:
	add.s32 	%r1149, %r1149, 4;
	setp.lt.s32	%p132, %r1149, %r13;
	@%p132 bra 	BB10_191;

BB10_204:
	ret;
}

	// .globl	FluffyTail
.visible .entry FluffyTail(
	.param .u64 FluffyTail_param_0,
	.param .u64 FluffyTail_param_1,
	.param .u64 FluffyTail_param_2,
	.param .u64 FluffyTail_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<20>;
	// demoted variable
	.shared .align 4 .u32 FluffyTail$__cuda_local_var_208417_30_non_const_destIdx;

	ld.param.u64 	%rd2, [FluffyTail_param_0];
	ld.param.u64 	%rd3, [FluffyTail_param_1];
	ld.param.u64 	%rd5, [FluffyTail_param_2];
	ld.param.u64 	%rd4, [FluffyTail_param_3];
	mov.u32 	%r3, %ctaid.x;
	cvt.s64.s32	%rd1, %r3;
	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r1, [%rd8];
	mov.u32 	%r2, %tid.x;
	setp.ne.s32	%p1, %r2, 0;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd9, %rd4;
	atom.global.add.u32 	%r4, [%rd9], %r1;
	st.shared.u32 	[FluffyTail$__cuda_local_var_208417_30_non_const_destIdx], %r4;

BB11_2:
	bar.sync 	0;
	setp.ge.s32	%p2, %r2, %r1;
	@%p2 bra 	BB11_4;

	ld.shared.u32 	%r5, [FluffyTail$__cuda_local_var_208417_30_non_const_destIdx];
	add.s32 	%r6, %r5, %r2;
	cvta.to.global.u64 	%rd10, %rd3;
	mul.wide.s32 	%rd11, %r6, 8;
	add.s64 	%rd12, %rd10, %rd11;
	mul.lo.s64 	%rd13, %rd1, 88064;
	shr.u64 	%rd14, %rd13, 2;
	cvt.s64.s32	%rd15, %r2;
	add.s64 	%rd16, %rd14, %rd15;
	cvta.to.global.u64 	%rd17, %rd2;
	shl.b64 	%rd18, %rd16, 3;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.v2.u32 	{%r7, %r8}, [%rd19];
	st.global.v2.u32 	[%rd12], {%r7, %r8};

BB11_4:
	ret;
}

	// .globl	FluffyRecovery
.visible .entry FluffyRecovery(
	.param .u64 FluffyRecovery_param_0,
	.param .u64 FluffyRecovery_param_1,
	.param .u64 FluffyRecovery_param_2,
	.param .u64 FluffyRecovery_param_3,
	.param .u64 FluffyRecovery_param_4
)
{
	.local .align 16 .b8 	__local_depot12[512];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<51>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<141>;
	.reg .b64 	%rd<159>;
	// demoted variable
	.shared .align 4 .b8 FluffyRecovery$__cuda_local_var_208434_30_non_const_nonces[168];

	mov.u64 	%rd158, __local_depot12;
	cvta.local.u64 	%SP, %rd158;
	add.u64 	%rd29, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd29;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32	%p2, %r1, 41;
	@%p2 bra 	BB12_2;

	shl.b32 	%r17, %r1, 2;
	mov.u32 	%r18, FluffyRecovery$__cuda_local_var_208434_30_non_const_nonces;
	add.s32 	%r19, %r18, %r17;
	mov.u32 	%r20, 0;
	st.shared.u32 	[%r19], %r20;

BB12_2:
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	bar.sync 	0;
	mad.lo.s32 	%r22, %r2, %r3, %r1;
	shl.b32 	%r5, %r22, 10;
	mov.u32 	%r137, 0;

BB12_3:
	ld.param.u64 	%rd155, [FluffyRecovery_param_0];
	ld.param.u64 	%rd154, [FluffyRecovery_param_1];
	ld.param.u64 	%rd153, [FluffyRecovery_param_2];
	ld.param.u64 	%rd152, [FluffyRecovery_param_3];
	add.s32 	%r24, %r5, %r137;
	cvt.s64.s32	%rd151, %r24;
	mov.u32 	%r138, -64;
	mov.u64 	%rd150, %rd1;

BB12_4:
	xor.b64  	%rd30, %rd151, %rd152;
	add.s64 	%rd31, %rd30, %rd153;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r25}, %rd30;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r26,%dummy}, %rd30;
	}
	shf.l.wrap.b32 	%r27, %r26, %r25, 16;
	shf.l.wrap.b32 	%r28, %r25, %r26, 16;
	mov.b64 	%rd32, {%r28, %r27};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r29}, %rd154;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r30,%dummy}, %rd154;
	}
	shf.l.wrap.b32 	%r31, %r30, %r29, 13;
	shf.l.wrap.b32 	%r32, %r29, %r30, 13;
	mov.b64 	%rd33, {%r32, %r31};
	add.s64 	%rd34, %rd154, %rd155;
	xor.b64  	%rd35, %rd33, %rd34;
	xor.b64  	%rd36, %rd32, %rd31;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd34, 32;
	shr.b64 	%rhs, %rd34, 32;
	add.u64 	%rd37, %lhs, %rhs;
	}
	add.s64 	%rd38, %rd35, %rd31;
	add.s64 	%rd39, %rd36, %rd37;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r33}, %rd35;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r34,%dummy}, %rd35;
	}
	shf.l.wrap.b32 	%r35, %r34, %r33, 17;
	shf.l.wrap.b32 	%r36, %r33, %r34, 17;
	mov.b64 	%rd40, {%r36, %r35};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r37}, %rd36;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r38,%dummy}, %rd36;
	}
	shf.l.wrap.b32 	%r39, %r38, %r37, 25;
	shf.l.wrap.b32 	%r40, %r37, %r38, 25;
	mov.b64 	%rd41, {%r40, %r39};
	xor.b64  	%rd42, %rd40, %rd38;
	xor.b64  	%rd43, %rd41, %rd39;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd38, 32;
	shr.b64 	%rhs, %rd38, 32;
	add.u64 	%rd44, %lhs, %rhs;
	}
	add.s64 	%rd45, %rd39, %rd42;
	add.s64 	%rd46, %rd44, %rd43;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r41}, %rd42;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r42,%dummy}, %rd42;
	}
	shf.l.wrap.b32 	%r43, %r42, %r41, 13;
	shf.l.wrap.b32 	%r44, %r41, %r42, 13;
	mov.b64 	%rd47, {%r44, %r43};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r45}, %rd43;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r46,%dummy}, %rd43;
	}
	shf.l.wrap.b32 	%r47, %r46, %r45, 16;
	shf.l.wrap.b32 	%r48, %r45, %r46, 16;
	mov.b64 	%rd48, {%r48, %r47};
	xor.b64  	%rd49, %rd47, %rd45;
	xor.b64  	%rd50, %rd48, %rd46;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd45, 32;
	shr.b64 	%rhs, %rd45, 32;
	add.u64 	%rd51, %lhs, %rhs;
	}
	add.s64 	%rd52, %rd46, %rd49;
	add.s64 	%rd53, %rd51, %rd50;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r49}, %rd49;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r50,%dummy}, %rd49;
	}
	shf.l.wrap.b32 	%r51, %r50, %r49, 17;
	shf.l.wrap.b32 	%r52, %r49, %r50, 17;
	mov.b64 	%rd54, {%r52, %r51};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r53}, %rd50;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r54,%dummy}, %rd50;
	}
	shf.l.wrap.b32 	%r55, %r54, %r53, 25;
	shf.l.wrap.b32 	%r56, %r53, %r54, 25;
	mov.b64 	%rd55, {%r56, %r55};
	xor.b64  	%rd56, %rd54, %rd52;
	xor.b64  	%rd57, %rd55, %rd53;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd52, 32;
	shr.b64 	%rhs, %rd52, 32;
	add.u64 	%rd58, %lhs, %rhs;
	}
	xor.b64  	%rd59, %rd53, %rd151;
	xor.b64  	%rd60, %rd58, 255;
	add.s64 	%rd61, %rd59, %rd56;
	add.s64 	%rd62, %rd60, %rd57;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r57}, %rd56;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r58,%dummy}, %rd56;
	}
	shf.l.wrap.b32 	%r59, %r58, %r57, 13;
	shf.l.wrap.b32 	%r60, %r57, %r58, 13;
	mov.b64 	%rd63, {%r60, %r59};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r61}, %rd57;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r62,%dummy}, %rd57;
	}
	shf.l.wrap.b32 	%r63, %r62, %r61, 16;
	shf.l.wrap.b32 	%r64, %r61, %r62, 16;
	mov.b64 	%rd64, {%r64, %r63};
	xor.b64  	%rd65, %rd61, %rd63;
	xor.b64  	%rd66, %rd62, %rd64;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd61, 32;
	shr.b64 	%rhs, %rd61, 32;
	add.u64 	%rd67, %lhs, %rhs;
	}
	add.s64 	%rd68, %rd62, %rd65;
	add.s64 	%rd69, %rd67, %rd66;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r65}, %rd65;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r66,%dummy}, %rd65;
	}
	shf.l.wrap.b32 	%r67, %r66, %r65, 17;
	shf.l.wrap.b32 	%r68, %r65, %r66, 17;
	mov.b64 	%rd70, {%r68, %r67};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r69}, %rd66;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r70,%dummy}, %rd66;
	}
	shf.l.wrap.b32 	%r71, %r70, %r69, 25;
	shf.l.wrap.b32 	%r72, %r69, %r70, 25;
	mov.b64 	%rd71, {%r72, %r71};
	xor.b64  	%rd72, %rd68, %rd70;
	xor.b64  	%rd73, %rd69, %rd71;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd68, 32;
	shr.b64 	%rhs, %rd68, 32;
	add.u64 	%rd74, %lhs, %rhs;
	}
	add.s64 	%rd75, %rd69, %rd72;
	add.s64 	%rd76, %rd74, %rd73;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r73}, %rd72;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r74,%dummy}, %rd72;
	}
	shf.l.wrap.b32 	%r75, %r74, %r73, 13;
	shf.l.wrap.b32 	%r76, %r73, %r74, 13;
	mov.b64 	%rd77, {%r76, %r75};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r77}, %rd73;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r78,%dummy}, %rd73;
	}
	shf.l.wrap.b32 	%r79, %r78, %r77, 16;
	shf.l.wrap.b32 	%r80, %r77, %r78, 16;
	mov.b64 	%rd78, {%r80, %r79};
	xor.b64  	%rd79, %rd75, %rd77;
	xor.b64  	%rd80, %rd76, %rd78;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd75, 32;
	shr.b64 	%rhs, %rd75, 32;
	add.u64 	%rd81, %lhs, %rhs;
	}
	add.s64 	%rd82, %rd76, %rd79;
	add.s64 	%rd83, %rd81, %rd80;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r81}, %rd79;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r82,%dummy}, %rd79;
	}
	shf.l.wrap.b32 	%r83, %r82, %r81, 17;
	shf.l.wrap.b32 	%r84, %r81, %r82, 17;
	mov.b64 	%rd84, {%r84, %r83};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r85}, %rd80;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r86,%dummy}, %rd80;
	}
	shf.l.wrap.b32 	%r87, %r86, %r85, 25;
	shf.l.wrap.b32 	%r88, %r85, %r86, 25;
	mov.b64 	%rd85, {%r88, %r87};
	xor.b64  	%rd86, %rd82, %rd84;
	xor.b64  	%rd87, %rd83, %rd85;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd82, 32;
	shr.b64 	%rhs, %rd82, 32;
	add.u64 	%rd88, %lhs, %rhs;
	}
	add.s64 	%rd89, %rd83, %rd86;
	add.s64 	%rd90, %rd88, %rd87;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r89}, %rd86;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r90,%dummy}, %rd86;
	}
	shf.l.wrap.b32 	%r91, %r90, %r89, 13;
	shf.l.wrap.b32 	%r92, %r89, %r90, 13;
	mov.b64 	%rd91, {%r92, %r91};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r93}, %rd87;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r94,%dummy}, %rd87;
	}
	shf.l.wrap.b32 	%r95, %r94, %r93, 16;
	shf.l.wrap.b32 	%r96, %r93, %r94, 16;
	mov.b64 	%rd92, {%r96, %r95};
	xor.b64  	%rd93, %rd89, %rd91;
	xor.b64  	%rd94, %rd90, %rd92;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd89, 32;
	shr.b64 	%rhs, %rd89, 32;
	add.u64 	%rd95, %lhs, %rhs;
	}
	add.s64 	%rd96, %rd90, %rd93;
	add.s64 	%rd97, %rd95, %rd94;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r97}, %rd93;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r98,%dummy}, %rd93;
	}
	shf.l.wrap.b32 	%r99, %r98, %r97, 17;
	shf.l.wrap.b32 	%r100, %r97, %r98, 17;
	mov.b64 	%rd98, {%r100, %r99};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r101}, %rd94;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r102,%dummy}, %rd94;
	}
	shf.l.wrap.b32 	%r103, %r102, %r101, 25;
	shf.l.wrap.b32 	%r104, %r101, %r102, 25;
	mov.b64 	%rd99, {%r104, %r103};
	xor.b64  	%rd100, %rd96, %rd98;
	xor.b64  	%rd101, %rd97, %rd99;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd96, 32;
	shr.b64 	%rhs, %rd96, 32;
	add.u64 	%rd102, %lhs, %rhs;
	}
	add.s64 	%rd103, %rd97, %rd100;
	add.s64 	%rd104, %rd102, %rd101;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r105}, %rd100;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r106,%dummy}, %rd100;
	}
	shf.l.wrap.b32 	%r107, %r106, %r105, 13;
	shf.l.wrap.b32 	%r108, %r105, %r106, 13;
	mov.b64 	%rd105, {%r108, %r107};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r109}, %rd101;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r110,%dummy}, %rd101;
	}
	shf.l.wrap.b32 	%r111, %r110, %r109, 16;
	shf.l.wrap.b32 	%r112, %r109, %r110, 16;
	mov.b64 	%rd106, {%r112, %r111};
	xor.b64  	%rd107, %rd103, %rd105;
	xor.b64  	%rd108, %rd104, %rd106;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd103, 32;
	shr.b64 	%rhs, %rd103, 32;
	add.u64 	%rd109, %lhs, %rhs;
	}
	add.s64 	%rd110, %rd104, %rd107;
	add.s64 	%rd155, %rd109, %rd108;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r113}, %rd107;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r114,%dummy}, %rd107;
	}
	shf.l.wrap.b32 	%r115, %r114, %r113, 17;
	shf.l.wrap.b32 	%r116, %r113, %r114, 17;
	mov.b64 	%rd111, {%r116, %r115};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r117}, %rd108;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r118,%dummy}, %rd108;
	}
	shf.l.wrap.b32 	%r119, %r118, %r117, 25;
	shf.l.wrap.b32 	%r120, %r117, %r118, 25;
	mov.b64 	%rd112, {%r120, %r119};
	xor.b64  	%rd154, %rd110, %rd111;
	xor.b64  	%rd152, %rd155, %rd112;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd110, 32;
	shr.b64 	%rhs, %rd110, 32;
	add.u64 	%rd153, %lhs, %rhs;
	}
	xor.b64  	%rd113, %rd154, %rd112;
	xor.b64  	%rd114, %rd113, %rd153;
	st.local.u64 	[%rd150], %rd114;
	add.s64 	%rd151, %rd151, 1;
	add.s64 	%rd150, %rd150, 8;
	add.s32 	%r138, %r138, 1;
	setp.ne.s32	%p3, %r138, 0;
	@%p3 bra 	BB12_4;

	add.s64 	%rd143, %rd1, 504;
	ld.local.u64 	%rd17, [%rd143];
	mov.u16 	%rs4, 63;

BB12_6:
	setp.eq.s16	%p4, %rs4, 63;
	mov.u64 	%rd156, %rd17;
	@%p4 bra 	BB12_8;

	cvt.s32.s16	%r121, %rs4;
	mul.wide.s32 	%rd115, %r121, 8;
	add.s64 	%rd116, %rd1, %rd115;
	ld.local.u64 	%rd117, [%rd116];
	xor.b64  	%rd156, %rd117, %rd17;

BB12_8:
	mov.u32 	%r133, %tid.x;
	mov.u32 	%r132, %ntid.x;
	mov.u32 	%r131, %ctaid.x;
	mad.lo.s32 	%r130, %r131, %r132, %r133;
	shl.b32 	%r129, %r130, 10;
	add.s32 	%r128, %r137, %r129;
	cvt.u64.u16	%rd119, %rs4;
	and.b64  	%rd120, %rd119, 1;
	and.b64  	%rd121, %rd156, 268435455;
	bfi.b64 	%rd122, %rd121, %rd120, 1, 28;
	shr.u64 	%rd123, %rd156, 31;
	and.b64  	%rd124, %rd123, 536870910;
	or.b64  	%rd125, %rd124, %rd120;
	bfi.b64 	%rd20, %rd125, %rd122, 32, 32;
	shl.b64 	%rd126, %rd122, 32;
	or.b64  	%rd21, %rd126, %rd125;
	cvt.s32.s16	%r124, %rs4;
	add.s32 	%r10, %r124, %r128;
	mov.u32 	%r140, -42;
	mov.u32 	%r139, FluffyRecovery$__cuda_local_var_208434_30_non_const_nonces;
	mov.u64 	%rd157, recovery;

BB12_9:
	ld.const.u64 	%rd127, [%rd157];
	setp.eq.s64	%p5, %rd127, %rd20;
	setp.eq.s64	%p6, %rd127, %rd21;
	or.pred  	%p7, %p5, %p6;
	@!%p7 bra 	BB12_11;
	bra.uni 	BB12_10;

BB12_10:
	st.shared.u32 	[%r139], %r10;

BB12_11:
	ld.const.u64 	%rd128, [%rd157+8];
	setp.eq.s64	%p8, %rd128, %rd20;
	setp.eq.s64	%p9, %rd128, %rd21;
	or.pred  	%p10, %p8, %p9;
	@!%p10 bra 	BB12_13;
	bra.uni 	BB12_12;

BB12_12:
	st.shared.u32 	[%r139+4], %r10;

BB12_13:
	ld.const.u64 	%rd129, [%rd157+16];
	setp.eq.s64	%p11, %rd129, %rd20;
	setp.eq.s64	%p12, %rd129, %rd21;
	or.pred  	%p13, %p11, %p12;
	@!%p13 bra 	BB12_15;
	bra.uni 	BB12_14;

BB12_14:
	st.shared.u32 	[%r139+8], %r10;

BB12_15:
	ld.const.u64 	%rd130, [%rd157+24];
	setp.eq.s64	%p14, %rd130, %rd20;
	setp.eq.s64	%p15, %rd130, %rd21;
	or.pred  	%p16, %p14, %p15;
	@!%p16 bra 	BB12_17;
	bra.uni 	BB12_16;

BB12_16:
	st.shared.u32 	[%r139+12], %r10;

BB12_17:
	ld.const.u64 	%rd131, [%rd157+32];
	setp.eq.s64	%p17, %rd131, %rd20;
	setp.eq.s64	%p18, %rd131, %rd21;
	or.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB12_19;
	bra.uni 	BB12_18;

BB12_18:
	st.shared.u32 	[%r139+16], %r10;

BB12_19:
	ld.const.u64 	%rd132, [%rd157+40];
	setp.eq.s64	%p20, %rd132, %rd20;
	setp.eq.s64	%p21, %rd132, %rd21;
	or.pred  	%p22, %p20, %p21;
	@!%p22 bra 	BB12_21;
	bra.uni 	BB12_20;

BB12_20:
	st.shared.u32 	[%r139+20], %r10;

BB12_21:
	ld.const.u64 	%rd133, [%rd157+48];
	setp.eq.s64	%p23, %rd133, %rd20;
	setp.eq.s64	%p24, %rd133, %rd21;
	or.pred  	%p25, %p23, %p24;
	@!%p25 bra 	BB12_23;
	bra.uni 	BB12_22;

BB12_22:
	st.shared.u32 	[%r139+24], %r10;

BB12_23:
	ld.const.u64 	%rd134, [%rd157+56];
	setp.eq.s64	%p26, %rd134, %rd20;
	setp.eq.s64	%p27, %rd134, %rd21;
	or.pred  	%p28, %p26, %p27;
	@!%p28 bra 	BB12_25;
	bra.uni 	BB12_24;

BB12_24:
	st.shared.u32 	[%r139+28], %r10;

BB12_25:
	ld.const.u64 	%rd135, [%rd157+64];
	setp.eq.s64	%p29, %rd135, %rd20;
	setp.eq.s64	%p30, %rd135, %rd21;
	or.pred  	%p31, %p29, %p30;
	@!%p31 bra 	BB12_27;
	bra.uni 	BB12_26;

BB12_26:
	st.shared.u32 	[%r139+32], %r10;

BB12_27:
	ld.const.u64 	%rd136, [%rd157+72];
	setp.eq.s64	%p32, %rd136, %rd20;
	setp.eq.s64	%p33, %rd136, %rd21;
	or.pred  	%p34, %p32, %p33;
	@!%p34 bra 	BB12_29;
	bra.uni 	BB12_28;

BB12_28:
	st.shared.u32 	[%r139+36], %r10;

BB12_29:
	ld.const.u64 	%rd137, [%rd157+80];
	setp.eq.s64	%p35, %rd137, %rd20;
	setp.eq.s64	%p36, %rd137, %rd21;
	or.pred  	%p37, %p35, %p36;
	@!%p37 bra 	BB12_31;
	bra.uni 	BB12_30;

BB12_30:
	st.shared.u32 	[%r139+40], %r10;

BB12_31:
	ld.const.u64 	%rd138, [%rd157+88];
	setp.eq.s64	%p38, %rd138, %rd20;
	setp.eq.s64	%p39, %rd138, %rd21;
	or.pred  	%p40, %p38, %p39;
	@!%p40 bra 	BB12_33;
	bra.uni 	BB12_32;

BB12_32:
	st.shared.u32 	[%r139+44], %r10;

BB12_33:
	ld.const.u64 	%rd139, [%rd157+96];
	setp.eq.s64	%p41, %rd139, %rd20;
	setp.eq.s64	%p42, %rd139, %rd21;
	or.pred  	%p43, %p41, %p42;
	@!%p43 bra 	BB12_35;
	bra.uni 	BB12_34;

BB12_34:
	st.shared.u32 	[%r139+48], %r10;

BB12_35:
	ld.const.u64 	%rd140, [%rd157+104];
	setp.eq.s64	%p44, %rd140, %rd20;
	setp.eq.s64	%p45, %rd140, %rd21;
	or.pred  	%p46, %p44, %p45;
	@!%p46 bra 	BB12_37;
	bra.uni 	BB12_36;

BB12_36:
	st.shared.u32 	[%r139+52], %r10;

BB12_37:
	add.s32 	%r140, %r140, 14;
	add.s32 	%r139, %r139, 56;
	add.s64 	%rd157, %rd157, 112;
	setp.ne.s32	%p47, %r140, 0;
	@%p47 bra 	BB12_9;

	add.s16 	%rs2, %rs4, -1;
	setp.gt.s16	%p48, %rs4, 0;
	mov.u16 	%rs4, %rs2;
	@%p48 bra 	BB12_6;

	add.s32 	%r137, %r137, 64;
	setp.lt.s32	%p49, %r137, 1024;
	@%p49 bra 	BB12_3;

	mov.u32 	%r134, %tid.x;
	setp.lt.s32	%p1, %r134, 42;
	bar.sync 	0;
	@!%p1 bra 	BB12_43;
	bra.uni 	BB12_41;

BB12_41:
	mov.u32 	%r135, %tid.x;
	shl.b32 	%r125, %r135, 2;
	mov.u32 	%r126, FluffyRecovery$__cuda_local_var_208434_30_non_const_nonces;
	add.s32 	%r127, %r126, %r125;
	ld.shared.u32 	%r16, [%r127];
	setp.eq.s32	%p50, %r16, 0;
	@%p50 bra 	BB12_43;

	ld.param.u64 	%rd149, [FluffyRecovery_param_4];
	cvta.to.global.u64 	%rd148, %rd149;
	mov.u32 	%r136, %tid.x;
	mul.wide.s32 	%rd141, %r136, 4;
	add.s64 	%rd142, %rd148, %rd141;
	st.global.u32 	[%rd142], %r16;

BB12_43:
	ret;
}


